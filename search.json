[
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "Parlance",
    "section": "",
    "text": "Website for Parlance"
  },
  {
    "objectID": "education/applications/simon_llm_cli/index.html#chapters",
    "href": "education/applications/simon_llm_cli/index.html#chapters",
    "title": "LLMs on the command line",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nSimon Willison introduces LLM - a command line tool for interacting with large language models.\n01:40 Installing and Using LLM\nSimon demonstrates how to install LLM using pip or homebrew and run prompts against OpenAI’s API. He showcases features like continuing conversations and changing default models.\n10:30 LLM Plugins\nThe LLM tool has a plugin system that allows access to various remote APIs and local models. Simon installs the Claude plugin and discusses why he considers Claude models his current favorites.\n13:14 Local Models with LLM\nSimon explores running local language models using plugins for tools like GPT4All and llama.cpp. He demonstrates the llmchat command for efficient interaction with local models.\n26:16 Writing Bash Scripts with LLM\nA practical example of creating a script to summarize Hacker News threads.\n35:01 Piping and Automating with LLM\nBy piping commands and outputs, Simon shows how to automate tasks like summarizing Hacker News threads or generating Bash commands using LLM and custom scripts.\n37:08 Web Scraping and LLM\nSimon introduces ShotScraper, a tool for browser automation and web scraping. He demonstrates how to pipe scraped data into LLM for retrieval augmented generation (RAG).\n41:13 Embeddings with LLM\nLLM has built-in support for embeddings through various plugins. Simon calculates embeddings for his blog content and performs semantic searches, showcasing how to build RAG workflows using LLM.",
    "crumbs": [
      "Educational Resources",
      "Building Applications",
      "LLMs on the command line"
    ]
  },
  {
    "objectID": "education/applications/simon_llm_cli/index.html#notes",
    "href": "education/applications/simon_llm_cli/index.html#notes",
    "title": "LLMs on the command line",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThese notes were originally published by Simon Willison here\n\n\nNotes for a talk I gave at Mastering LLMs: A Conference For Developers & Data Scientists.\n\nLinks\n\nDatasette\nMy blog\nLLM\n\n\n\nGetting started\nbrew install llm # or pipx or pip\nllm keys set openai\n# paste key here\nllm \"Say hello in Spanish\"\n\n\nInstalling Claude 3\nllm install llm-claude-3\nllm keys set claude\n# Paste key here\nllm -m haiku 'Say hello from Claude Haiku'\n\n\nLocal model with llm-gpt4all\nllm install llm-gpt4all\nllm models\nllm chat -m mistral-7b-instruct-v0\n\n\nBrowsing logs with Datasette\nhttps://datasette.io/\npipx install datasette # or brew or pip\ndatasette \"$(llm logs path)\"\n# Browse at http://127.0.0.1:8001/\n\nTemplates\nllm --system 'You are a sentient cheesecake' -m gpt-4o --save cheesecake\nNow you can chat with a cheesecake:\nllm chat -t cheesecake\nMore plugins: https://llm.datasette.io/en/stable/plugins/directory.html\n\n\nllm-cmd\nHelp with shell commands. Blog entry is here: https://simonwillison.net/2024/Mar/26/llm-cmd/\n\n\nfiles-to-prompt and shot-scraper\nfiles-to-prompt is described here: https://simonwillison.net/2024/Apr/8/files-to-prompt/\nshot-scraper javascript documentation: https://shot-scraper.datasette.io/en/stable/javascript.html\nJSON output for Google search results:\nshot-scraper javascript 'https://www.google.com/search?q=nytimes+slop' '\nArray.from(\n  document.querySelectorAll(\"h3\"),\n  el =&gt; ({href: el.parentNode.href, title: el.innerText})\n)'\nThis version gets the HTML that includes the snippet summaries, then pipes it to LLM to answer a question:\nshot-scraper javascript 'https://www.google.com/search?q=nytimes+slop' '\n() =&gt; {\n    function findParentWithHveid(element) {\n        while (element && !element.hasAttribute(\"data-hveid\")) {\n            element = element.parentElement;\n        }\n        return element;\n    }\n    return Array.from(\n        document.querySelectorAll(\"h3\"),\n        el =&gt; findParentWithHveid(el).innerText\n    );\n}' | llm -s 'describe slop'\n\n\n\nHacker news summary\nhttps://til.simonwillison.net/llms/claude-hacker-news-themes describes my Hacker News summary script in detail.\n\n\nEmbeddings\nFull documentation: https://llm.datasette.io/en/stable/embeddings/index.html\nI ran this:\ncurl -O https://datasette.simonwillison.net/simonwillisonblog.db\nllm embed-multi links \\\n  -d simonwillisonblog.db \\\n  --sql 'select id, link_url, link_title, commentary from blog_blogmark' \\\n  -m 3-small --store\nThen looked for items most similar to a string like this:\nllm similar links \\\n  -d simonwillisonblog.db \\\n  -c 'things that make me angry'\n\n\nMore links\n\nCoping strategies for the serial project hoarder talk about personal productivity on different projects\nFigure out how to serve an AWS Lambda function with a Function URL from a custom subdomain as an example of how I use GitHub Issues",
    "crumbs": [
      "Educational Resources",
      "Building Applications",
      "LLMs on the command line"
    ]
  },
  {
    "objectID": "education/applications/simon_llm_cli/index.html#full-transcript",
    "href": "education/applications/simon_llm_cli/index.html#full-transcript",
    "title": "LLMs on the command line",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Simon Willison: Hey, hey everyone, it’s great to be here. So yeah, the talk today, it’s about command line tools and large language models. And effectively the argument I want to make is that the Unix command line dating back probably 50 years now is it turns out the perfect environment to play around with this new cutting edge technology, because the Unix philosophy has always been about tools that output things that get piped into other tools as input. And that’s really what a language model is, right? [0:27] Simon Willison: An LLM is a, it’s effectively a function that you pipe a prompt to, and then you get a response back out, or you pipe a big chunk of context to, and you get a response that you can do things with. So I realized this last year and also realized that nobody had grabbed the namespace on PyPI, the Python Packaging Index, for the term LLM. So I leapt at that. I was like, okay, this is an opportunity to grab a really cool name for something. And I built this… [0:54] Simon Willison: LittleTool, which originally was just a command line tool for talking to OpenAI. So you could be in your terminal and you could type LLM, say hi in French, and that would fire it through the OpenAI API and get back response and print it to your terminal. That was all it did. And then over time, as other model providers became interesting, and as local models emerged that you could run on your computer, I realized there was an opportunity to have this tool do way more than that. [1:24] Simon Willison: So I started adding plugin support to it so you can install plugins that give you access to flawed and local mistral and all sorts of other models. There’s hundreds of models that you can access through this tool now. I’ll dive into that in a little bit more detail in a moment. First thing you need to know is how to install it. If you are Python people, that’s great. Pip install LLM works. I recommend using pip x to install it because then the dependencies end up packaged away somewhere nice. Or you can install it using homebrew. [1:54] Simon Willison: I think the one I’ve got here, yeah, this one I installed with brew install LLM. I made the mistake of running that command about half an hour ago and of course it’s homebrew so it took half an hour to install everything. So Treat with caution. But it works. And once it’s installed, you have the command. And so when you start using LLM, the default is for it to talk to OpenAI. And of course, you need an OpenAI API key for that. So I’m going to grab my API key. There’s a command you can run. [2:25] Simon Willison: LLM secrets. Is it LLM secrets? [2:30] Hugo Bowne-Anderson: Yes. [2:32] Simon Willison: Yes. No, it’s not. It’s. What is it? LLM keys. That’s it. LLM. So I can type LLM keys, set OpenAI, and then I paste in the key, and I’m done. And having done this, I’ve actually got all sorts of keys in here, but my OpenAI API key has now been made available. And that means I can run prompts. Five great names for a pet pelican. This is my favorite test prompt. And it gives me five great names for a pet pelican. So that’s fun. And that’s running over the API. [3:05] Simon Willison: And because it’s a Unix command line thing, you can do stuff with the app. So you can do things like write that to pelicans.txt. The greater than sign means take the output and run it to a file. Now I’ve got a nice permanent pelicans.txt file with my five distinctive names for a pet pelican. Another thing you can do is you can continue. If you say dash C. which stands for continue, I can say now do walruses. And it will continue that same conversation, say here are five fitting names for a pet walrus. [3:35] Simon Willison: I’m going to say justify those. Oops. And now it says why each of these names are justified for that walrus. That’s super, super, super delightful. [3:48] Hugo Bowne-Anderson: I like Gustav. [3:50] Simon Willison: Gustav, what was, let’s do LLM logs dash. Gustav, a touch of personality and grandeur, strong regal name that suits the impressive size and stature of a walrus, evoking a sense of dignity. It’s good. The justifications are quite good. This is GPT-4-0 I’m running here. You can actually say LLM models default to see what the default model is, and then you can change that as well. So if I want to… be a bit cheaper, I can set it to chat GPT. [4:24] Simon Willison: And now when I do this, oops, these are the GPT 3.5 names, which are slightly less exciting. So that’s all good and fun. But there are a whole bunch of other useful things you can do when you start working with these things in the terminal. So I’m going to… Let’s grab another model. So LLM, as I mentioned, has plugins. If you go to the LLM website, look at list of plugins, there is a plugin directory. This is all of the plugins that are currently available for the tool. And most of these are remote APIs. [5:06] Simon Willison: These are plugins for talking to Claude or Rekha or Perplexity or any scale endpoints, all of these different providers. And then there are also some local model plugins that we’ll jump into in a moment. But let’s grab Claude 3. Claude 3 is my current favorite, my favorite family of models. So I can say LLM install LLM Claude 3, and it will go ahead and install that plugin. If I type LLM plugins, it shows me the plugins that it has installed. Oh, I didn’t mean to install LLM Claude, but never mind. And if I see LLM… [5:39] Hugo Bowne-Anderson: Is Claude 3 currently your favorite? Just out of interest? [5:41] Simon Willison: Two reasons. One, Claude 3 Haiku is incredible, right? Claude 3 Haiku is cheaper than GPT 3.5. I think it’s the cheapest decent model out there. It’s better than 3.5. It has the 100,000 token limit. So you can dump a huge amount of stuff on there. And it can do images. So we’ve got, I think it’s the most exciting model that we have right now if you’re actually building because for the price, you get an enormous array of capabilities. And then Opus, I think Opus was better than GPT-4 Turbo. [6:15] Simon Willison: I think 4.0 is just about caught up for the kind of stuff that I do. But Opus is still like a really interesting model. The other thing I’ll say about Claude is the… There’s this amazing article that just came up about Claude’s personality, which talks about how they gave Claude a personality. And this is one of the most interesting essays I have read about large language models in months. Like, what they did to get Claude to behave the way it does and the way they thought about it is super fascinating. [6:45] Simon Willison: But anyway, so I’ve installed Claude. I’ve now got Claude 3 Opus, Claude 3 Sonnet and Claude 3 Haiku. LLM gives everything long names, so you can say that, say hi in Spanish, and it’ll say hola. If you say it with a flourish, it’ll add an emoji. That’s cute. Hola mi amigo. But you can also say lm-m and just the word haiku because that’s set up as a shorter alias. So if I do that, I’ll get the exact same response. Crucially, you can… [7:22] Simon Willison: This is how I spend most of my time when I’m messing around with models. install plugins for them, or often I’ll write a new plugin because it doesn’t take much effort to write new plugins for this tool. And then I can start mucking around with them in the terminal, trying out different things against them. Crucially, one of the key features of this tool is that it logs absolutely everything that you do with it. It logs that to a SQLite database. [7:47] Simon Willison: So if I type lmat logs path, it will show me the path to the SQLite database that it’s using. And I absolutely adore SQLite databases, partly because my main project, the thing I spend most of my time building, is this thing called Dataset, which is a tool for exploring data in SQLite databases. So I can actually do this. I can say Dataset that, if you put it in double quotes, it makes up that space in the file name. This is taking, this is a good command line trick. [8:18] Simon Willison: It’s taking the path to the log database and passing it to dataset. And now I’ve got a web interface where I can start browsing all of my conversations. So let’s have a look at responses. We sort by ID. Here we go. Say hi in Spanish with a flourish. Hola mi amigo. There it is. It stores the options that we used. It stores the full JSON that came back. It also organizes these things into conversation threads. So earlier we started with, we built, we had that conversation that started five great names for a pet pelican. [8:52] Simon Willison: We replied to it twice and each of those messages was logged under that same conversation idea. So we started five great names for pet pelican. Now do walruses justify those. As a tool for evaluating language models, this is incredibly powerful because I’ve got every experiment I’ve ever run through this tool. I’ve got two and a half thousand responses that I’ve run all in one place, and I can use SQL to analyze those in different ways. [9:18] Simon Willison: If I facet by them, I can see that I’ve spent the most time talking to GPT 3.5 Turbo, Cloud 3 Opus. I’ve done 334 prompts through. I’ve got all of these other ones, Gemini, Gemini Pro. Orca Mini, all of these different things that I’ve been messing around with. And I can actually search these as well. If I search for pelican, these are the six times that I’ve talked to Claude 3 Opus about pelicans. And it’s mostly… Oh, interesting. [9:46] Simon Willison: It’s mostly asking names from Pelicun, but I did at one point ask it to describe an image that was a close-up view of a Pelicun wearing a colorful party hat. The image features aren’t in the main shipped version of the software yet. That’s a feature I need to release quite soon. Basically, it lets you say LLM. like a dash I and then give it the path to an image and that will be sent as part of the prompt. So that’s kind of fun. [10:10] Simon Willison: But yeah, so if you want to be meticulous in tracking your experiments, I think this is a really great way to do that. Like having this database where I can run queries against everything I’ve ever done with them. I can try and compare the different models and the responses they gave to different prompts. That’s super, super useful. Let’s talk a little bit more about plugins. So I mentioned that we’ve got those plugins that add additional models. We also have plugins that add extra features. [10:45] Simon Willison: My favorite of those is this plugin called LLM-CMD, which basically lets you do this. If I say in the LLM-CMD, that’s installing the plugin. That gives me a new command, llmcmd. That command wasn’t there earlier. I can now say llmcmd-help, and it will tell me that this will generate and execute commands in your shell. So as an example, let’s do llm convert the file demos.md to uppercase and spit out the results. Oh, I forgot. So llmcmd. And what it does is it passes that up to, I think, GPT-4.0 is my default model right now. [11:36] Simon Willison: Gets back the command that does this thing. Why is this taking so long? Hang on. Models. Default. Let’s set that to GPT-4.0. Maybe GPT-3.5 isn’t very good at that. Anyway, when this works, it populates my shell with the command that I’m trying to run. And when I hit enter, it will run that command. But crucially, it doesn’t just run the command, because that’s a recipe for complete disaster. It lets you review that command before it goes. And I don’t know why this isn’t working. This is the one live demo I didn’t test beforehand. [12:21] Simon Willison: I will drop, I will make notes available afterwards as well. But this is a right, here we go. Here’s an animated GIF showing me, showing you exactly what happens. This is show the first three lines, show the first three lines of every file in this directory. And it’s spats out head dash N three star. And that does exactly the job. [12:41] Simon Willison: A fun thing about this is that because it’s a command here, it actually, And tab completion works as well, so you can give it file names by tab completing, and when the command is babing itself, that will do the right thing. But that’s kind of fun. It’s kind of neat to be able to build additional features into the tool, which use all of the other features of LLM, so it’ll log things to SQLite and it’ll give you access to all of those different models. [13:07] Simon Willison: But it’s a really fun playground for sort of expanding out the kind of command line features that we might want to use. Let’s talk about local models. So local models that run on your laptop are getting shockingly effective these days. And there are a bunch of LLM plugins that let you run those. One of my favorites of those is called LLM GPT-4-all. It’s a wrapper around the GPT-4-all library that Nomic put out. [13:35] Simon Willison: So Nomic have a desktop application that can run models that is accompanied by a Python library that can run models, which is very neatly designed. And so what I can do is I can install that plugin, LLM install. LLM GPT for all. This will go around fetch that. And now when I run the LLM models command, we’ve got a whole bunch of additional models. This is all of these GPT-4 ones. And you’ll note that some of these say installed. That’s because I previously installed them and they’re sat on my hard drive somewhere. [14:08] Simon Willison: I’m not going to install any new models right now because I don’t want to suck down a four gigabyte file while I’m on a Zoom call. But quite a few of these are installed, including Mistral 7b instruct. So I can grab that and I can say, lm-m Mistral, let’s do. Five great names for a pet seagull. Explanations. And I’m going to fire activity monitor for this right now. Let’s see if we can spot it doing its thing. Right now we’ve just asked a command line tool to load in a 4 gigabyte model file. [14:42] Simon Willison: There we go. It’s loading it in. It’s at 64.3 megabytes. 235 megabytes. It’s spitting out the answer. And then it goes away again. So this was actually a little bit wasteful, right? We just ran a command which loaded four gigabits of memory, of file into memory, and I think onto the GPU, ran a prompt for it, and then threw all of that away again. And it works. You know, we got our responses. And this here ran entirely on my laptop. There was no internet connection needed for this to work. [15:13] Simon Willison: But it’s a little bit annoying to have to load the model each time. So I have another command I wrote, a command I added, llm chat. And llm chat, you can feed it the ID of a model, llm chat dash m mistral 7b. And now it’s giving me a little… [15:33] Simon Willison: chat interface so I can say say hello in Spanish the first time I run this it will load the model again and now now in French and So this is if you’re working with local models This is a better way to do it because you don’t have to pay the cost of loading that model into memory every single time there’s also You can stick in pipe multi and now you can copy and paste a whole block of code into here and it translates And then you type exclamation mark end at the end. [16:08] Simon Willison: And if we’re lucky, this will now give me… Huh. Okay. Well, that didn’t. I may have hit the… I wonder if I’ve hit the context length. Yeah, something went a little bit wrong with that bit. But yeah, being able to hold things in memory is obviously really useful. There are better ways to do this. One of the best ways to do this is using the O-Lama tool, which I imagine some people here have played with already. And O-Lama is an absolutely fantastic tool. [16:50] Simon Willison: It’s a Mac, Linux, and Windows app that you can download that lets you start running local language models. And they do a much better job than I do of curating their collection of models. They have a whole team of people who are making sure that newly available models were available in that tool and work as effectively as possible. But you can use that with LLM as well. If I do LLM install LLM-o-Lama, actually… That will give me a new plugin called LLM-OLAMA. And now I can type LLM models. [17:22] Simon Willison: And now, this time, it’s giving me the OLAMA models that I have available in my machine as well. So in this case, we can do Mixtral. Let’s do LLM-M, LLM-CHAT, LLM-M Mixtral Latest. I’ll write it in Spanish. And this is now running against the Ollama server that’s running on my machine, which I think might be loading Mixtral into memory at the moment, the first time I’m calling it. Once it’s loaded into memory, it should work for following prompts without any additional overhead. Again, I spend a lot of time in an activity monitor these days. [18:00] Simon Willison: There we go. Ollama Runner has four gigabytes in residence, so you’d expect that to be doing the right thing. [18:07] Hugo Bowne-Anderson: So Simon, I just have a quick question. I love how you can use Ollama directly from LLM. I do think one of the other value props about Ollama is the ecosystem of tools built around it. Like you go to Olamas GitHub and there are all types of front ends you can use. And so I love using your LLM client, for example, with like a quick and dirty Gradio app or something like that. But I’m wondering, are there any front ends you recommend or any plugins or anything in the ecosystem to work with? [18:38] Simon Willison: I’ve not explored the Olam ecosystem in much detail. I tend to do everything on the command line and I’m perfectly happy there. But one of the features I most want to add to LLM as a plugin is a web UI. So you can type LLM web, hit enter, it starts a web server for you and gives you a slightly more… slightly more modern interface for messing around with things. And I’ve got a demo that relates to that that I’ll show in a moment. [19:04] Simon Willison: But yeah, so front-end, and actually the other front-end I spend a little bit of time with is LM Studio, which is very nice. That’s a very polished GUI front-end for working with models. There’s a lot of… [19:19] Hugo Bowne-Anderson: It’s quite around with getting two LLMs answering your same question. But there’s a mode where you can… [19:24] Simon Willison: get two or n llms if you have enough processing power to answer the same questions and compare their responses in real time yeah it’s a new feature very cool that is cool i’ve been me i’ve been planning a plugin that will let you do that with llms like llm multi dash m llama dash m something and then give it a prompt um but one of the many ideas on the on the on the backlog at the moment would be super super useful um the other one of course that people should know that if they don’t [19:53] Simon Willison: is llama file I’m going to demonstrate that right now. [19:57] Hugo Bowne-Anderson: Sorry, I’m going to shush now. [20:00] Simon Willison: This is one of the most bizarrely brilliant ways of running language models is there’s this project that’s sponsored by Mozilla called Llama File. And Llama File effectively lets you download a single file, like a single binary that’s like four gigabytes or whatever. And then that file will run, it comes with both the language model and the software that you need to run the language model. And it’s bundled together in a single file and one binary works on Windows, Mac OS, Linux and BST, which is ridiculous. What this thing does is technically impossible. [20:38] Simon Willison: You cannot have a single binary that works unmodified across multiple operating systems. But LamaFile does exactly that. It’s using a technology called Cosmopolitan. which I’ve got here we go I’ve got an article about Cosmopolitan when I dug into it just a while ago to try and figure out how this thing works astonishing project by Justin Tunney But anyway, the great thing about LamaFile is, firstly, you can just download a file and you’ve got everything that you need. And because it’s self-contained, I’m using this as my end-of-the-world backup of human knowledge. [21:16] Simon Willison: I’ve got a hard drive here, which has a bunch of LamaFiles on. If the world ends, provided I can still run any laptop and plug this hard drive in, I will have a GPT 3.5 class. language model that I can just start using. And so the one that I’ve got on there at the moment, I have, I’ve actually got a whole bunch of them, but this is the big one. I’ve got Lama 370B, which is by far, I mean, how big is that thing? That’s a 37 gigabyte file. It’s the four byte quantized version. [21:55] Simon Willison: That’s a genuinely a really, really good model. I actually started this running earlier because it takes about two minutes to load it from over USB-C from drive into memory. So this right here is that. And all I had to do was download that file, shimod7558, and then do.slash metal armor 370B and hit enter. And that’s it. And that then fires up this. In this case, it fires up a web server which loads the model and then starts running on port. Which port is it? [22:32] Simon Willison: So now if I go to localhost 8080, this right here is the default web interface for Llama. It’s all based on Llama.cpp but compiled in a special way. And so I can say, turn names for a pet pelican and hit go. And this will start firing back tokens. Oh, I spelt pelican wrong, which probably won’t matter. [22:58] Hugo Bowne-Anderson: And while this is executing, maybe I’ll also add for those who want to spin up LamaFile now you can do so as Simon has just done. One of the first models in their README they suggest playing around with, which is 4 gigs or 4.7 gigs or something, is the Lava model, which is a really cool multimodal model that you can play around with locally from one file immediately, which is actually mind blowing if you think about it for a second. [23:22] Simon Willison: It really is. Yeah. Do I have that one? Let’s see. Let’s grab. [23:30] Hugo Bowne-Anderson: With your end of the world scenario, now you’ve got me thinking about like post-apocalyptic movies where people have like old LLMs that they use to navigate the new world. [23:40] Simon Willison: Completely. Yeah, that’s exactly how I. [23:43] Hugo Bowne-Anderson: Preppers with LLMs. [23:47] Simon Willison: I’m going to. Wow. Wow. Yeah, I very much broke that one, didn’t I? [23:58] Hugo Bowne-Anderson: We’ve got a couple of questions that may be relevant as we move on. One from Alex Lee is how does LLM compare with tools like Ollama for local models? So I just want to broaden this question. I think it’s a great question. It’s the Python challenge, right? The ecosystem challenge. When somebody wants to start with a tool like this, how do they choose between the plethora? How would you encourage people to make decisions? [24:21] Simon Willison: I would say LLMs… LLM’s unique selling point is the fact that it’s scriptable and it’s a command line tool. You can script it on the command line and it can run access both the local models and the remote models. Like, that I feel is super useful. So you can try something out against a hosted like cloud-free haiku and then you can run the exact same thing against a local Lama 3 or whatever. And that, I think, is the reason to pay attention to that. I just… [24:52] Hugo Bowne-Anderson: I also, I do want to build on that. I’d say if like I’ve been using dataset for some time now, and I love local SQLite databases as well. So the integration of those three with all the dataset plugins as well, make it really, really interesting also. So I think that’s a huge selling point. [25:07] Simon Willison: So what I’ve done here, I closed Llama 370B and I have switched over to that. I switched over to that Lava demo. And if we’re lucky, look at this. Person features a person sitting in a chair with a rooster nearby. She’s a chicken, not a rooster. A white ball filled with eggs. This I think is astoundingly good for a four gigabyte file. This is a four gigabyte file. It can describe images. This is remarkably cool. And then from LLM’s point of view, if I saw LLM Lava file. And then run LM models. [25:50] Simon Willison: I’ve now got a model in here which is Lama file. So I can say LM dash Lama file. Describe chickens. This doesn’t yet. Ooh, what happened there? Error file not found. Not sure what’s going on there. I’ll dig into that one a little bit later. [26:11] Simon Willison: the joys of live demos but yeah um so we’ve got all of this stuff what are some of the things that we can start doing with it well the most exciting opportunity i think is that we can now start um we can now start writing little bash scripts writing little tools on top of this and so if i do This is a script that I’ve been running for quite a while called HN summary, which is a way of summarizing posts on Hacker News or entire conversations on Hacker News. Because Hacker News gets pretty noisy. [26:47] Simon Willison: What’s a good one of these to take a look at? Yeah, I do not have time to read 119 comments, but I’d like to know a rough overview of what’s going on here. So I can say HN-summary. And then paste in that ID. And this is giving me a summary of the themes from the hack news point. So, he’s in theme static versus dynamic linking, package management dependency, Swift cross-platform language. That totally worked. And if we look at the way this worked, it’s just a bash script which does a curl command to get the full. [27:25] Simon Willison: This is from one of the hack news APIs. If you hit this URL here. You will get back JSON of everything that’s going on in that thread as this giant terrifying nested structure. I then pipe it through the JQ command. I use ChatGP to write this because I can never remember how to use JQ. That takes that and turns it into plain text. And actually, I’ll run that right now just to show you what that looks like. There we go. [28:01] Simon Willison: So that essentially strips out all of the JSON stuff and just gives me back the names of the people and what they said. And then we pipe that into LLM-M model. The model defaults to Haiku, but you can change it to other models if you like. And then we feed it this, the dash S option. to LLM, also known as dash dash system, is the way of feeding in a system prompt. [28:26] Simon Willison: So here I’m feeding the output of that curl command, goes straight into LLM as the prompt, and then the system prompt is the thing that tells it what to do. So I’m saying summarize the themes of the opinions expressed here for each theme, output a markdown header. Let’s try that. I’m going to try that one more time, but this time I’ll use GPT-4-0. So we’re running the exact same prompt, but this time through a different model. And here we’re actually getting back quotes. [28:52] Simon Willison: So when it says, you man wizard said this, Jerry Puzzle said this about dynamic and static linking. I really like this as a mechanism of sort of summarizing conversations because the moment you ask for direct prompts, you’re not completely safe from hallucination, but you do at least have a chance of fact checking what the thing said to you. And as a general rule, models are quite good at outputting texts that they’ve just seen. So if you ask for direct quotes from the input, you’ll often get bad results. But this is really good, right? [29:20] Simon Willison: This is a pretty decent, quick way of digesting 128 comments in that giant thread. And it’s all been logged to my SQLite database. I think if I go and look in SQLite, I’ve got hundreds of hack and use threads that I’ve summarized in this way, which if I wanted to do fancy things with later, that’s probably all sorts of fun I could have with them. And again, I will. Here we go. [29:49] Simon Willison: I will share full notes later on, but there’s a TIL that I wrote up with the full notes on how I built up this script. That’s another reason I love Cloud3 Haiku, is that running this kind of thing through Cloud3 Haiku is incredibly inexpensive. It costs… And it may be a couple of cents for a long conversation. And the other model I use for this is Gemini Flash that just came out from Google. It’s also a really good long context model with a very low price per token. But yeah. Where did we go to? [30:28] Hugo Bowne-Anderson: So we have a couple of questions that maybe you want to answer now, maybe we want to leave until later and maybe you cover. And the fact that we saw some sort of formatted output leads to one of these. Is LLM compatible with tools like Instructor, Kevin asks, to generate formatted output? And there are other questions around like piping different. different things together. So I wonder if you can kind of use that as a basis to talk about how you pipe, [30:56] Simon Willison: you know? Yes, I will jump into some quite complex piping in just a moment. LLM does not yet have structured output function calling support. I am so excited about getting that in there. The thing I want to do, there are two features I care about. There’s the feature where you can like get a bunch of unstructured text and feed in a JSON schema and get back JSON. That works incredibly well. A lot of the models are really good at that now. [31:19] Simon Willison: I actually have a tool I built for dataset that uses that feature, but that’s not yet available as a command line tool. And the other thing I want to do is full-blown like tool execution where the tools themselves are plugins. Like imagine if you could install an LLM plugin that added Playwright functions, and now you can run prompts that can execute Playwright automations as part of those prompts, because it’s one of the functions that gets made available to the model. [31:46] Simon Willison: So that, I’m still gelling through exactly how that’s going to work, but I think that’s going to be enormously powerful. [31:53] Hugo Bowne-Anderson: Amazing. And on that point as well, there are some questions around evals. And if you, when using something like this, you can do evals and how that would work. [32:05] Simon Willison: So work in progress. Two months ago, I started hacking on a plugin for LLM for running evals. And it is. Very, very alpha right now. The idea is I want to be able to find my evals as YAML files and then say things like LMEVAL simple dot YML with the 40 model and the chat GPT models. I’m running the same eval against two different models and then get those results back, log them to SQLite, all of that kind of thing. This is a very, very early prototype at the moment. [32:37] Simon Willison: But it’s partly, I just, I’ve got really frustrated with how difficult it is to run evals and how little I understand about them. And when I don’t understand something, I tend to write code as my way of thinking through a problem. So I will not promise that this will turn into a generally useful thing for other people. I hope it will. At the moment, it’s a sort of R&D prototype for me to experiment with some ideas. [32:58] Hugo Bowne-Anderson: I also know the community has generated a bunch of plugins and that type of stuff for Dataset. I’m not certain about LLM, but I am wondering if people here are pretty, you know, pretty sophisticated audience here. So if people wanted to contribute or that type of thing. [33:13] Simon Willison: OK, the number one way to contribute to LLM right now is is by writing plugins for it. And I wrote a very detailed tutorial. The most exciting is the ones that enable new models. So I wrote a very detailed tutorial on exactly. how to write a plugin that exposes new models. A bunch of people have written plugins for API-based models. Those are quite easy. The local models are a little bit harder, but the documentation is here. [33:39] Simon Willison: And I mean, my dream is that someday this tool is widely enough to use that when somebody releases a new model, they build a plugin for that model themselves. That would be the ideal. But in the absence of that, it’s pretty straightforward building new models. I’m halfway through building a plugin for… the MLX Apple framework, which is getting really interesting right now. And I just this morning got to a point where I have a prototype of a plugin that can run MLX models locally, which is great. But yeah, let’s do some commands. [34:17] Simon Willison: Okay, I’ll show you a really cute thing you can do first. LLM has support for templates. So you can say things like LLM dash dash system, you are a sentient cheesecake, tell it the model, and you can save that as a template called cheesecake. Now I can say LLM chat dash T cheesecake, tell me about yourself. And it’ll say, I’m a sentient cheesecake, a delightful fusion of creamy textures. So this is, I have to admit, I built this feature. I haven’t used it as much as I expected it I would. [34:47] Simon Willison: It’s effectively LLM’s equivalent of GPT’s, of chat GPT’s. I actually got this working before GPT’s came along. And it’s kind of fun, but it’s, yeah, like I said, I don’t use it on a daily basis. And I thought I would when I built it. Let’s do some really fun stuff with piping. So I’ve got a couple of the, one of the most powerful features of LLM is that you can pipe things into it with a system prompt to have it, to then process those things further. [35:17] Simon Willison: And so you can do that by just like catting files to it. So I can say cat demos.md pipe LLM dash S summary short. And this will give me a short summary of that document that I just piped into it, which works really well. That’s really nice. Cool. A little bit longer than I wanted it to be. Of course, the joy of this is that once this is done, I can then say lm-c, no, much, much, much shorter and in haikus. [35:50] Simon Willison: And now it will write me some haikus that represent the demo that I’m giving you right now. These are sentient cheesecake, templates to save brilliant minds, cheesecake chats with us. That’s lovely. So being able to pipe things in is really powerful. I built another command called files to prompt, where the idea of this one is if you’ve got a project with multiple files in it, running files to prompt will turn those into a single prompt. [36:14] Simon Willison: And the way it does that is it outputs the name of the file and then the contents of the file and then name of the next file, contents of the file, et cetera, et cetera, et cetera. But because of this, I can now do things like suggest tests to add to this project. Oh. I’m sorry. I’m sorry. I forgot the LLM-S. Here we go. And this is now, here we go, reading all of that code and suggesting, okay, you should have tests that, oh, wow, it actually, it’s writing me sample tests. [36:46] Simon Willison: This is very, this is a very nice result. So I use this all the time. When I’m hacking on stuff on my machine, I will very frequently just. cat a whole directory of files into a prompt in one go, and use the system prompt to say, what tests should I add? Or write me some tests, or explain what this is doing, or figure out this bug. Very, very powerful way of working with the tool. But way more fun than that is another tool I built called ShotScraper. [37:14] Simon Willison: So ShotScraper is a browser automation tool which started out as a way of taking screenshots. So once you’ve got it installed, you can do ShotScraper and then the URL to a web page, and it will generate a PNG file with a screenshot of that web page. That’s great. I use that to automate screenshots in my documentation using this. But then I realized that you can do really fun things by running JavaScript from the command line. [37:38] Simon Willison: So a very simple example, if I say, shot scraper JavaScript, give it the URL to a website and then give it that string, document.title, it will load that website up in a hidden browser. It’ll execute that piece of JavaScript and it will return the result of that JavaScript directly to my terminal. So I’ve now got the title of this webpage and that’s kind of fun. Where that gets super, super fun is when you start doing much more complicated and interesting things with it. So let’s scrape Google. Google hate being scraped. We’ll do it anyway. [38:12] Simon Willison: Here is a Google search for NY Times plus slop. There’s an article in the New York Times today with a quote for me about the concept of slop in AI, which I’m quite happy about. And then so you can open that up and start looking in the. If you start looking at the HTML, you’ll see that there’s H3s for each results, and the H3s are wrapped by a link that links to that page. [38:37] Simon Willison: So what I can do is I can write a little bit of JavaScript here that finds all of the H3s on the page, and for each H3, it finds the parent link and its href, and it finds the title, and it outputs those in an array. And if I do this… This should fire up that browser. That just gave me a JSON array of links to search results on the New York Times. Now I could pipe that to LLM. So I’m gonna do pipe LLM. [39:06] Simon Willison: Actually, no, I’m gonna do a slightly more sophisticated version of this. This one goes a little bit further. It tries to get the entire… It tries to get the snippet as well, because the snippet gives you that little bit of extra context. So if I take that, and I’m just going to say dash S describe slot. And what we have just done. is we have done RAG, right? This is retrieval augmented generation against Google search results using their snippets to answer a question done as a bash one-liner effectively. [39:42] Simon Willison: Like we’re using ShotScraper to load up that web page. We’re scraping some stuff out with JavaScript. We’re piping the results into LLM, which in this case is sending it up to GPT-4.0, but I could equally tell it to send it to Claude or to run it against a local model or any of those things. And it’s a full RAG pipeline. I think that’s really fun. I do a lot of my experiments around the concept of RAG, just as these little shell scripts here. [40:09] Simon Willison: You could consider the hack and use example earlier was almost an example of RAG, but this one, because we’ve got an actual search term and a question that we’re answering, I feel like this is it. This is a very quick way to start prototyping different forms of retrieval augmented generation. [40:27] Hugo Bowne-Anderson: Let me ask though, does it use? I may have missed, does it use embeddings? [40:32] Simon Willison: Not yet, no, but I’ll get into embeddings in just a second. [40:35] Hugo Bowne-Anderson: And that’s something we decided not to talk too much about today, but it’d be sad if people didn’t find out about your embeddings. [40:42] Simon Willison: I have a closing demo I can do with embeddings. Yeah, this right here, effectively, we’re just copying and pasting these search results from the browser into the model and answering a question, but we’re doing it entirely on the command line, which means that we can hook up our own bash scripts that… automate that and pull that all together. There’s all sorts of fun bits and pieces we can do with that. But yeah, let’s… The ShotScraper JavaScript thing I’ll share later. Let’s jump into the last… Let’s jump into embedding stuff. So… [41:16] Simon Willison: If you run llm dash help, it’ll show you all of the commands that are available in the LLM family. The default command is prompt. That’s for running prompts. There are also these collections for dealing with embeddings. I would hope everyone in this course is familiar enough with embeddings now that I don’t need to dig into them in too much detail. But it’s exactly the same pattern as the language models. Embeddings are provided by plugins. There are API-based embeddings. There are local embedding models. It all works exactly the same way. [41:46] Simon Willison: So if I type LLM embed models, that will show me the models that I have installed right now. And actually, these are the open AI ones, the three small, three large, and so on. If I were to install additional plugins, is the embeddings documentation. There’s a section in the plugin directory for embedding models. So you can install sentence transformers, you can get clip running, and various other bits and pieces like that. But let’s embed something. So if I say lm embed, let’s use the OpenAI 3 small model and give it some text. [42:28] Simon Willison: It will embed that text and it will return an array of, I think, 6,000. How many is that? Like JQ length. An array of 1,536 floating point numbers. This is admittedly not very interesting or useful. There’s not a lot that we can do with that JSON array of floating point numbers right here. You can get it back in different shapes and things. You can ask for it in, I think I can say, dash, dash, X. I can say dash f hex and get back a hexadecimal blob of those. Again, not particularly useful. [43:04] Simon Willison: Where embeddings get interesting is when you calculate embeddings across a larger amount of text and then start storing them for comparison. And so we can do that in a whole bunch of different ways. There is a command called, where is it? Embed multi. Where’s my embed multi documentation gone? Here we go. The embed multi command lets you embed multiple strings in one go, and it lets you store the results of those embeddings in a SQLite database because I use SQLite databases for everything. [43:40] Simon Willison: So when I have here a SQLite database, I’m going to open it up actually using Dataset Desktop, which is my Mac OS Electron app version of Dataset. How big is that file? That’s a 129 megabyte file. Wow. Does this have embeddings in already? It does not. Okay, so this right here is a database of all of the content on my blog. And one of the things I have on my blog is I have a link blog, this thing down the side, which has 7,000 links in it. [44:16] Simon Willison: And each of those links is a title and a description and a URL, effectively. So I’ve got those here in a SQLite database, and I’m going to create embeddings for every single one of those 7,168 bookmarks. And the way I can do that is with a, well, firstly, I need to figure out a SQL query that will get me back the data I want to embed. That’s going to be select ID, link URL, link title, commentary from blog, blogmark. [44:44] Simon Willison: The way LLM works is when you give it a query like this, it treats the ID there as the unique identifier for that document, and then everything else gets piped into the embedding model. So once I’ve got that in place, I can run this command. I can say, LLM embed multi. I’m going to create a collection of embeddings called links. I’m going to do it against that Simon Wilson blog SQLite database. I’m going to run this SQL query, and I’m using that three small model. [45:11] Simon Willison: And then dash dash store causes it to store the text in the SQLite database as well. Without that, it’ll just store the IDs. So I’ve set that running, and it’s doing its thing. It’s got 7,000 items. Each of those has to be sent to the OpenAI API in this case, or if it was a local model, it would run it locally. And while that’s running, we can actually see what it’s doing by taking a look at the embeddings table. Here we go. [45:38] Simon Willison: So this table right here is being populated with the being populated by that script. We’re at one thousand two hundred rows now. I hit refresh. We’re at two thousand rows. And you can see for each one, we’ve got the content which was glued together. And then we’ve got the embedding itself, which is a big binary blob of it’s a. binary encoded version of that array of 1,500 floating point numbers. But now that we’ve got those stored, we can start doing fun things with them. I’m going to open up another. There we go. [46:20] Simon Willison: So I’ve opened up another window here so that I can say LLM similar. I’m going to look for similar items in the links collection to the text, things that make me angry. Oh, why doesn’t the, oh, because I’ve got to add the dash D. Here we go. So this right here is taking the phrase, things that make me angry, it’s embedding it, and it’s finding the most similar items in my database to that. And there’s an absolutely storming rant from somebody. There’s death threats against bloggers. There’s a bunch of things that might make me angry. [46:56] Simon Willison: This is the classic sort of embedding semantic search right here. And this is kind of cool. I now have embedding search against my blog. Let’s try something a little bit more useful. I’m going to say. Let’s do dataset plugins. So we’ll get back everything that looks like it’s a dataset plugin. There we go. And I can now pipe that into LLM itself. So I can pipe it to LLM and I’m gonna say system prompt, most interesting plugins. And here we are. Again, this is effectively another version of command line rag. [47:39] Simon Willison: I have got an embeddings database in this case. I can search it for things that are similar to things. I like this example because we’re running LLM twice. We’re doing the LLM similar command to get things out of that vector database. And then we’re biking to the LLM prompt command to summarize that data and turn it into something interesting. And so you can build a full rag-based system again as a… little command line script. I think I’ve got one of those. Log answer. Yes, there we go. [48:13] Simon Willison: This one I don’t think is working at the moment, but this is an example of what it would take. What it would take to… take a question, run a embedding search against, in this case, it’s every paragraph in my blog. I’ve got a little bit of JQ to clean that up. And then I’m piping it into… In this case, the local Lama file, but I’ve typed into other models as well to answer questions. [48:38] Simon Willison: So you can build a full RAG Q&A workflow as a bash script that runs against this local SQLite database and does everything that way. It’s worth noting that this is not a fancy vector database at all. This is a SQLite database with those embedding vectors as binary blobs. Anytime you run a search against this, it’s doing effectively it’s a… Effectively, it’s doing a brute force. It’s calculating the vector similarity difference between your input and every one of those things, and then it’s sorting by those records. [49:13] Simon Willison: I find for less than 100,000 records, it’s so fast it doesn’t matter. If you were using millions and millions of records, that’s the point where the brute force approach doesn’t work anymore, and you’ll want to use some kind of specialized vector index. There are SQLite vector indexing tools that I haven’t integrated with yet, but they’re looking really promising. You can use pinecone and things like that as well. One of my future goals for LLM is to teach it how to work with external vector indexes. [49:40] Simon Willison: Cause I feel like once you’ve got those embeddings stored, having a command that like synchronizes up your pinecone to run searches, that feels like that would be a reasonable thing to do. I realize we’re running a little bit short on time. So I’m gonna switch to questions for the rest of the section. I think I went through all of the demos that I wanted to provide. [49:59] Hugo Bowne-Anderson: Awesome. Well, thank you so much, Simon. That was illuminating as always. And there are a lot more things I want to try now. And I hope for those who have played around with LLM and these client utilities that I’ve got a lot more ideas about how to do so. And for those who haven’t, please jump in and let us know on Discord or whatever, like what type of stuff you, what type of fun you get to have. [50:20] Hugo Bowne-Anderson: Question wise, I haven’t been able to rank all of them some reason with respect to upvotes, unfortunately, this time. There was one, and I don’t know if you mentioned this, there was one quickly about Hugging Face hub models. Are there plugins? [50:37] Simon Willison: No, there is not. And that’s because I am GPU poor. I’m running on a Mac. Most of the Hugging Face models appear to need an NVIDIA GPU. If you have an NVIDIA GPU and want to write the LLM Hugging Face plugin, I think it would be quite a straightforward plugin to write, and it would be enormously useful. So yeah, that’s open right now for somebody to do that. Same thing with, is it VLLX or something? [51:05] Simon Willison: There’s a few different serving technologies that I haven’t dug into because I don’t have an NVIDIA GPU to play with on a daily basis. But yeah, the Hugging Face Models thing would be fantastic. [51:15] Hugo Bowne-Anderson: Awesome. And how about some of the serverless inference stuff? Is there a way we can use LLM to ping those in? [51:23] Simon Willison: Do you mean like the Cloudflare ones and so on? [51:27] Hugo Bowne-Anderson: I’m thinking like, let me… If you go to any given model, there is some sort of serverless inference you can… [51:36] Simon Willison: you can just do to ping the apis that they’ve already got set up there oh interesting i mean so as you can see we’ve got like 20 different plugins for any scale endpoints is a very good one fireworks um open router so if it’s available via an api you can build a plugin for it the other thing is if it’s an open air compatible if it’s open ai compatible as the api you don’t have to build anything at all you can actually configure llm you can teach it about additional Yeah, you can teach about additional [52:08] Simon Willison: OpenAI compatible models by just dropping some lines into a YAML file. So if it already speaks OpenAI, without writing additional plugins, you can still talk to it. [52:20] Hugo Bowne-Anderson: Amazing. And just check out the link I shared with you. If you want to open that one, it should be in the chat. [52:27] Simon Willison: Is it the… [52:29] Hugo Bowne-Anderson: It’s the hugging face. API. [52:34] Simon Willison: No, I’ve not built something against this yet. [52:38] Hugo Bowne-Anderson: This could actually be really exciting. Yeah. Because I’ve got a lot of pretty heavy-duty models that you can just ping as part of their serverless. [52:49] Simon Willison: I don’t think anyone’s built that yet, but that would be a really good one to get going, absolutely. [52:55] Hugo Bowne-Anderson: So if anyone’s interested in that, definitely jump in there. We do have questions around using your client utility for agentic workflows. [53:07] Simon Willison: Yeah, not yet, because I haven’t done the function calling piece. Once the function calling piece is in, I think that’s going to get really interesting. And that’s also the kind of thing where I’d like to, I feel like you could explore that really by writing additional plugins, like an LLM agents plugin or something. So yeah, there is the other side of LLM which isn’t as mature is there is a Python API. So you can pip install LLM and use it from Python code. [53:35] Simon Willison: I’m not I’m not completely happy with the interface of this yet, so I don’t tend to push people towards it. Once I’ve got that stable, once I have a 1.0 release, I think this will be a very nice sort of abstraction layer over a hundred different models, because any model that’s available through a plugin to the command line tool will be available as a plugin that you can use from Python directly. So that’s going to get super fun as well, especially in Jupyter notebooks and such like. [53:58] Hugo Bowne-Anderson: Awesome. We actually have some questions around hardware. Would you mind sharing the system info of your Mac? Is it powerful? Is it with all the commands you demoed? Wonder if I need a new Mac? I can tell people that I’ve got an M1 from 2021. So my MacBook’s got a GPU, but it’s like three, four years old or whatever. And it runs this stuff wonderfully. [54:20] Simon Willison: So yeah, I’m an M2 Mac, 64 gig. I wish I’d got more RAM. At the same time, the local models, the Mistral 7Bs and such like, run flawlessly. PHY3, absolutely fantastic model, that runs flawlessly. They don’t even gobble up that much RAM as well. The largest model I’ve run so far is Lama 370B, which takes about 40 gigabytes of RAM. And it’s definitely the most GPT 3.5-like local model that I’ve ever run. [54:53] Simon Willison: I have a hunch that within a few months the Mac will be an incredible platform for running models because Apple are finally like all speed ahead on local model stuff. Their MLX library is really, really good. So it might be that in six months time, an M4 MacBook Pro with 192 gigabytes of RAM is the best machine out there. But I wouldn’t spend any money now based on future potential. [55:18] Hugo Bowne-Anderson: Right. And I’m actually very bullish on Apple and excited about what happens in the space as well. Also, we haven’t talked about this at all, but the ability to run all this cool stuff on your cell phone is people are complaining about all types of stuff at the moment and Apple hasn’t done this. But this is wild. This is absolutely like cosmic stuff. [55:40] Simon Willison: There is an app that absolutely everyone with a modern iPhone should try out called MLC Chat. Yeah. It straight up runs Mistral on the phone. It just works. And it’s worked for like six months. It’s absolutely incredible. I can run Mistral 7B Instruct Quantized on my iPhone. Yeah. And it’s good. I’ve used this on flights to look up Python commands and things. Yeah. That’s incredible. And yeah, Apple stuff. [56:09] Simon Willison: it’s interesting that none of the stuff they announced the other day was actually a chatbot you know that they’re building language model powered features that summarize and that help with copy editing and stuff they’re not giving us a chat thing which means that they don’t they’re not responsible for hallucinations and all of the other weird stuff that can happen which is a really interesting design choice i feel like apple did such a good job of avoiding most of the traps and pitfalls and weirdnesses in in the in the products that they announced yesterday [56:40] Hugo Bowne-Anderson: Totally agree. And so two more questions, we should wrap up in a second. I wonder, people have said to me, and I don’t… My answer to this, people like, hey, why do you run LLMs locally when there are so many ways to access bigger models? And one of my answers is just, like you mentioned being on a plane or in the apocalypse or that type of thing. But it’s also just for exploration to be able to do something when I’m at home to use my local stuff. [57:14] Simon Willison: The vast majority of my real work that I would do LLMs, I use Clod 3 Opus, I use GPT-4O, I occasionally use Clod 3 Haiku. But the local models as a way of exploring the space are so fascinating. And it’s also, I feel like if you want to learn how language models work, the best way to do that is to work with the really bad ones. [57:35] Simon Willison: Like the working, spending time with a crap local model that hallucinates constantly is such a good way of getting your sort of mental model of what these things are and how they work. Because when you do that, then you start saying, oh, okay, I get it. ChatGPT 3.5 is like Mistral 7b, but it’s a bit better. So it makes less mistakes and all of those kinds of things. But yeah, and I mean, there are plenty of very valid privacy concerns around this as well. I’ve kind of skipped those. [58:03] Simon Willison: Most of the stuff I say to models is me working on open source code, where if there’s a leak, it doesn’t affect me at all. But yeah, I feel like… If you’re interested in understanding the world of language models, running local models is such a great way to explore them. [58:20] Hugo Bowne-Anderson: Totally. I do have a question also around, you mentioned the eval tool that you’re slowly working on. Does it incorporate data set as well? Because I couldn’t, when, so when I want to do like at least my first round of evaluations, I’ll do it in a notebook or spin up a basic streamlet out where I can tag things as right or wrong and then filter by those. So these are the types of that could make sense in data. [58:41] Simon Willison: Where I want to go. So the idea with the evals tools, and it doesn’t do this yet. It should be recording the results to SQLite so that you can have. like a custom data interface to help you evaluate them. I want to do one of those, the LMSIS arena style interfaces where you can see two different prompted, two different responses from prompts that you’ve run evals against and click on the one that’s better and that gets recorded in the database as well. [59:05] Simon Willison: Like there’s so much that I could do with that because fundamentally SQLite is such a great sort of substrate to build tools on top of. Like it’s incredibly fast it’s free everyone’s got it you can use it as a file format for passing things around like imagine running a bunch of evals and then schlepping a like five megabytes sqlite file to your co-worker to have a look at what you’ve done that stuff all becomes possible as well But yeah, so that’s the ambition there. I don’t know when I’ll get to invest the time in it. [59:35] Hugo Bowne-Anderson: Well, once again, like if people here are interested in helping out or chatting about this stuff, please do get involved. I do. I am also interested, speaking about the SQLite database and then dataset. So one thing that’s also nice about LM Studio is that it’ll tell you, like it does have some serious product stuff. When you run something, it’ll like give you in your GUI. the latency and number of tokens and stuff like that. We log that stuff to SQLite and have that in. And then like serious, you know, benchmarking of different models. [1:00:10] Simon Willison: Yep. I’ve been meaning to file this ticket for a while. Awesome. That needs to happen. Yep. I guess it’s tokens per second and total duration. Yeah, exactly. It’s going to be interesting figuring out how to best do that for models where I don’t have a good token count from them, but I can fudge it. Just the duration on its own would be useful things to start recording. [1:00:41] Hugo Bowne-Anderson: Absolutely. And so there’s kind of a through line in some of these questions. Firstly, a lot of people are like, wow, this is amazing. Thank you. Misha has said Simon is a hero. Thank you. has said this is brilliant. I can’t believe you’ve done this. So that’s all super cool. I want to build on this question. Eyal says a little off topic, but how are you able to build so many amazing things? I just want to- [1:01:07] Simon Willison: I have a blog post about that. [1:01:09] Hugo Bowne-Anderson: Raise that as an issue on a GitHub repository? Well, [1:01:12] Simon Willison: yeah. Here we go. I have a blog- It’s not the building, [1:01:15] Hugo Bowne-Anderson: it’s the writing as well. So yeah, what structures do you put in your own life in order to- I have a great story. [1:01:24] Simon Willison: Basically, so this talk here is about my approach to personal projects and effectively, and really the argument I make here is that you need to write unit tests and documentation because then you can do more projects. Because if you haven’t done that, you’ll come across a project like my LLM evals project I haven’t touched in two months, but because I’ve got a decent set of issues and sort of notes tucked away in there, I’m going to be able to pick up on that really easily. [1:01:48] Simon Willison: And then the other trick is I only work on projects that I already know I can do quickly. I don’t have time to take on a six-month mega project, but when I look at things like LLM, I already had the expertise of working with SQLite from the dataset stuff. I knew how to write Python command line applications. I knew how to build plugin infrastructures because I’d done that for dataset. [1:02:09] Simon Willison: So I was probably the person on earth most well-equipped to build a command line tool in Python that has plugins and does language model stuff and logs to SQLite. And so really that’s my sort of main trick is I’ve got a bunch of things that I know how to do, and I’m really good at spotting opportunities to combine them in a way that lets me build something really cool, but quite quickly, because I’ve got so many other things going on. Amazing. That’s the trick. It’s being selective in your projects. [1:02:38] Hugo Bowne-Anderson: And also there are small things you do like your, well, it’s not small anymore, right? But your Today I Learn blog, and what a wonderful way to, you know, it doesn’t necessarily need to be novel stuff, right? But because it’s the Today I Learn, you just quickly write something you learn. [1:02:52] Simon Willison: I will tell you the trick for that is every single thing that I do, I do in GitHub issues. So if I’m working on anything at all, I will fire up a GitHub issue thread in a private repo or in a public repo, and I will write notes as I figure it out. And one of my favorite examples, this is when I wanted to serve an AWS Lambda function with a function URL from a custom subdomain, which took me 77 comments all from me to figure out because, oh my God, AWS is a nightmare. [1:03:21] Simon Willison: And in those comments, I will drop in links to things I found and screenshots of the horrifying web interfaces I have to use and all of that kind of thing. And then when I went to write up a TIL, I just copy and paste the markdown to the issue. So most of my TALs take like 10 minutes to put together because they’re basically just the sort of semi-structured notes I had already copied and pasted and cleaned up a little bit. But this is in that productivity presentation I gave. This works so well. [1:03:49] Simon Willison: It’s almost like a scientist’s notebook kind of approach where anything you’re doing, you write very comprehensive notes on what do I need to do next? What did I just try? What worked? What didn’t work? And you get them all in that sequence. And it means that I can. [1:04:03] Simon Willison: I don’t remember a single thing about AWS Lambda now, but next time I want to solve this problem, I can come back and I can read through this and it’ll sort of reboot my brain to the point that I can take out the project from where I got to. [1:04:16] Hugo Bowne-Anderson: Awesome. I know we’re over time. There are a couple of very more interesting questions. So if you’ve got a couple more minutes. [1:04:22] Simon Willison: Yes, absolutely. [1:04:23] Hugo Bowne-Anderson: There’s one around, have you thought about using textual or rich in order to make pretty output? [1:04:32] Simon Willison: I think. Does that work? Like what’s a LM logs? What’s this? LM logs pipe hyphen dash m rich. What’s the thing? Is it rich dot markdown you can do? [1:04:47] Hugo Bowne-Anderson: I think so, but… [1:04:50] Simon Willison: Maybe I do. Look at that! There we go, Rich just pretty printed my markdown. So yeah, so I haven’t added Rich as a dependency because I’m very, very protective of my dependencies. I try and keep them as minimal as possible. I should do a plugin. It would be really cool if there was a… I’d need a new plugin hook, but if there was a plugin where you could install LLM Rich and now LLM outputs things like that, that would be super fun. So yeah, I should do that. [1:05:18] Hugo Bowne-Anderson: That would be super cool. And just for full transparency, occasionally when I want to have fun playing around with the command line, I muck around with tools like CowSay. So piping LLM to CowSay has been fun as well to get cows to. Final question is. Misha has a few GPU rigs and they’re wondering if there’s any idea how to run LLM with multiple models on different machines but on the same LAN. [1:05:53] Simon Willison: I would solve that with more duct tape. I’d take advantage of existing tools that let you run the same command on multiple machines. Ansible, things like that. I think that would be the way to do that. And that’s the joy of Unix is so many of these problems, if you’ve got a little Unix command, you can wire it together with extra bash script and Ansible and Kubernetes and Lord only knows what else. I run LLM occasionally inside of GitHub Actions, and that works because it’s just a command line. [1:06:22] Simon Willison: So yeah, for that, I’d look at existing tools that let you run commands in parallel on multiple machines. [1:06:30] Hugo Bowne-Anderson: Amazing. So everyone, next step, pip install LLM or pipx install LLM and let us know on Discord how you go. I just want to thank everyone for joining once again. And thank you, Simon, for all of your expertise and wisdom. And it’s always fun to chat. [1:06:46] Simon Willison: Thanks for having me. And I will drop a marked down document with all of the links and demos and things in Discord at some point in the next six hours. So I’ll drop that into the Discord channel. [1:06:58] Hugo Bowne-Anderson: Fantastic. All right. See you all on Discord. And thanks once again, Simon.",
    "crumbs": [
      "Educational Resources",
      "Building Applications",
      "LLMs on the command line"
    ]
  },
  {
    "objectID": "education/rag/ben.html",
    "href": "education/rag/ben.html",
    "title": "Beyond the Basics of RAG",
    "section": "",
    "text": "This talk was given by Ben Clavié at the Mastering LLMs Conference.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Beyond the Basics of RAG"
    ]
  },
  {
    "objectID": "education/rag/ben.html#chapters",
    "href": "education/rag/ben.html#chapters",
    "title": "Beyond the Basics of RAG",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nHamel introduces Ben Clavier, a researcher at Answer.ai with a strong background in information retrieval and the creator of the RAGatouille library.\n00:48 Ben’s Background\nBen shares his journey into AI and information retrieval, his work at Answer.ai, and the open-source libraries he maintains, including ReRankers.\n02:20 Agenda\nBen defines Retrieval-Augmented Generation (RAG), clarifies common misconceptions, and explains that RAG is not a silver bullet or an end-to-end system.\n05:01 RAG Basics and Limitations\nBen explains the basic mechanics of RAG, emphasizing that it is simply the process of stitching retrieval and generation together, and discusses common failure points.\n06:29 RAG MVP Pipeline\nBen breaks down the simple RAG pipeline, including model loading, data encoding, cosine similarity search, and obtaining relevant documents.\n07:54 Vector Databases\nBen explains the role of vector databases in handling large-scale document retrieval efficiently and their place in the RAG pipeline.\n08:46 Bi-Encoders\nBen describes bi-encoders, their efficiency in pre-computing document representations, and their role in quick query encoding and retrieval.\n11:24 Cross-Encoders and Re-Ranking\nBen introduces cross-encoders, their computational expense, and their ability to provide more accurate relevance scores by encoding query-document pairs together.\n14:38 Importance of Keyword Search\nBen highlights the enduring relevance of keyword search methods like BM25 and their role in handling specific terms and acronyms effectively.\n15:24 Integration of Full-Text Search\nBen discusses the integration of full-text search (TF-IDF) with vector search to handle detailed and specific queries better, especially in technical domains.\n16:34 TF-IDF and BM25\nBen explains TF-IDF, BM25, and their implementation in modern retrieval systems, emphasizing their effectiveness despite being older techniques.\n19:33 Combined Retrieval Approach\nBen illustrates a combined retrieval approach using both embeddings and keyword search, recommending a balanced weighting of scores.\n19:22 Metadata Filtering\nBen emphasizes the importance of metadata in filtering documents, providing examples and explaining how metadata can significantly improve retrieval relevance.\n22:37 Full Pipeline Overview\nBen presents a comprehensive RAG pipeline incorporating bi-encoders, cross-encoders, full-text search, and metadata filtering, showing how to implement these steps in code.\n26:05 Q&A Session Introduction\n26:14 Fine-Tuning Bi-Encoder and Cross-Encoder Models\nBen discusses the importance of fine-tuning bi-encoder and cross-encoder models for improved retrieval accuracy, emphasizing the need to make the bi-encoder more loose and the cross-encoder more precise.\n26:59 Combining Scores from Different Retrieval Methods\nA participant asks about combining scores from different retrieval methods. Ben explains the pros and cons of weighted averages versus taking top candidates from multiple rankers, emphasizing the importance of context and data specifics.\n29:01 The Importance of RAG as Context Lengths Get Longer\nBen reflects on how RAG may evolve or change as context lengths of LLMs get larger, but emphasizing that long context lengths are not a silver bullet.\n30:06 Chunking Strategies for Long Documents\nBen discusses effective chunking strategies for long documents, including overlapping chunks and ensuring chunks do not cut off sentences, while considering the importance of latency tolerance in production systems.\n30:56 Fine-Tuning Encoders and Advanced Retrieval with ColBERT\nBen also discusses when to fine-tune your encoders, and explains ColBERT for advanced retrieval.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Beyond the Basics of RAG"
    ]
  },
  {
    "objectID": "education/rag/ben.html#slides",
    "href": "education/rag/ben.html#slides",
    "title": "Beyond the Basics of RAG",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Beyond the Basics of RAG"
    ]
  },
  {
    "objectID": "education/rag/ben.html#additional-resources",
    "href": "education/rag/ben.html#additional-resources",
    "title": "Beyond the Basics of RAG",
    "section": "Additional Resources",
    "text": "Additional Resources\nThe following resources were mentioned during the talk:\n\nEasily use and train state of the art late-interaction retrieval methods (ColBERT) in any RAG pipeline. https://github.com/bclavie/RAGatouille\nA lightweight unified API for various reranking models: https://github.com/AnswerDotAI/rerankers\nA Hackers’ Guide to Language Models: https://www.youtube.com/watch?v=jkrNMKz9pWU\nGLiNER: Generalist Model for Named Entity Recognition using Bidirectional - Transformer: https://arxiv.org/abs/2311.08526\nFine-Tuning with Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/training_overview.html\nElastic, Dense vector field type: https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Beyond the Basics of RAG"
    ]
  },
  {
    "objectID": "education/rag/ben.html#full-transcript",
    "href": "education/rag/ben.html#full-transcript",
    "title": "Beyond the Basics of RAG",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:01] Hamel: Ben Clavier is one of the cracked researchers who work at Answer.ai. You’ve heard from several researchers from Answer.ai already in this conference. Ben has a background in information retrieval, amongst other things, and he has an open source package called Ragatouille, which you should check out. He also comes from a deep background in information retrieval. and brings that to RAG. And he’s also one of the clearest thinkers on the topic. But yeah, I’ll hand it over to you, Ben, to kind of give more color to your background, anything that I missed. [0:45] Hamel: And yeah, we can just jump into it. [0:48] Ben: Okay, let’s go. So I think that’s pretty much the key aspect of my background. You pretty much read this slide out. So I do R&D at Ansoya with Jeremy. You’ve seen Jono in this course and there’s a lot of other awesome people. We’re a distributed R&D lab, so we do AI research and we try to be as open source as possible because we want people to use what we build. Prior to joining ANSR, I did a lot of NLP and kind of stumbled upon information retrieval because it’s very, very useful and everybody wants information retrieval. [1:20] Ben: It’s more for clarifying what information retrieval is, which I hope today’s talk will help. And yeah, so my claim to fame or claim to moderate fame at least is the Ragatool library, which makes it much easier to use a family of models called Colbert. which we will very briefly mention today, but won’t have time to go into detail. But hopefully, like, if you want to know more about that, like, do feel free to ping me on Discord. I’m generally either very responsive or you need to ping me again. Pretty much how I work. [1:50] Ben: And I also maintain the ReRankers library, which we’ll discuss in one of the later slides. And yeah, if you know me, I want to follow me. I want to hear more. But what I do is pretty much all on Twitter. I’m not on LinkedIn at all. I’m just everything go through Twitter. A lot of memes and shitposts, but some very informative stuff once in a while. So. So yeah, and let’s get started with what we’re going to talk about today. So it’s only half an hour, so we’re not going to talk about a lot. [2:20] Ben: I’m going to talk about why I think I’ll do like call retrieval basics as they should exist in your pipelines, because RAG is a very nebulous term and that will be the first slide and Hamel will be very happy about that slide, I think. But RAG is not a silver bullet. RAG is not a new thing from December 2022. RAG is not even an end-to-end system. We’ll cover that, but I think it’s very important to like ground it a bit when we talk about RAG because it means a lot of different things to different people. [2:47] Ben: Then we will cover what we call the compact MVP, which is what most people do when they are starting out with RAG. It’s actually an example from Jeremy. It’s like the simplest possible implementation of RAG, as in just using a vector search. And then the other topics are basically things that I think you should have in your rack pipeline as part of your MVP. And I’ll show that like there’s a lot of scary concepts because they’re all big walls like by encoder, cross encoder, TFIDF, BM25, filtering. [3:14] Ben: That sounds like a lot, but then I’m going to try and show it that they’re very simple concepts and you can have pretty much the same MVP by adding just 10 lines of code, by choosing like by the state of the art retrieval components in every bit. And the bonus, which I don’t think we’ll have time to cover when I try this again, was talking about Colbert because I like talking about Colbert. So I might do it at the end if we have some time, but I might not. And yeah, that’s it for the agenda. [3:40] Ben: And then I also think it’s important to have the counter agenda, which is what we won’t be talking about today, because those are just as important for RAG. But they are not what we put in the very basics. And here we’re very much about the basics. So one of them is. How to monitor and improve RAC systems because RACs are systems and they’re living systems and they’re very much things you should monitor and continuously improve on. I think Jaydon covered that quite well in his talk yesterday or last week. Yeah, last week. [4:07] Ben: So I would invite you to watch that and watch Jaydon and Dan’s upcoming course if it does materialize. Evaluations, they’re also extremely important, but we won’t talk about them at all today. I know that Joe will talk about them at length in his talk. Benchmarks and paper references. So I’ll make a lot of claims that you will just have to trust me on because I don’t want to have too many references or too many academic looking tables and this trying to keep it quite lively and airy. [4:33] Ben: I won’t give you a rundown of all the best performing models and why you should use them. I won’t talk about training, data augmentation, et cetera. And I won’t talk about all the other cool approaches like Splayed, Colbert, and details because they go beyond the basics. But those are all very important topics, so if you’re interested, do look up, there’s a lot of good resources out there. Do feel free to ask me. And with that, let’s get started with the rant, which is my favorite part. [5:01] Ben: This is a thing that Hamel has been doing on Twitter recently as part of his flame posting campaign, I’ll say, which is basically, there’s so much in AI, so much in especially the LLM world that uses worlds that are like a lot scarier than they need to be. And RUG’s probably that because to me when I hear retrieval of matter generation or RUG, it sounds like that’s an end-to-end system, that’s a very definite set of components, that’s a thing that works on its own. [5:27] Ben: And it’s not, it’s literally just doing retrieval to put stuff into your prompt context, like before your prompt or after your prompt, you want to get some context, so you’re doing retrieval. But that means that’s not an end-to-end system, despite what Jason will have you believe on his Twitter, he’s not created it, but it does make a lot of money from it. And it’s basically just the act of stitching together retrieval, so the R part of RAG and generation, so the G part of RAG. like to ground the later. [5:53] Ben: So you want your generation to be grounded to use some context. So you’re doing retrieval on the wire of documents you have and pass it to your LLM. But there’s no magic going on. It’s very much like a pipeline that take the output of model A and gives it to model B. The generation part is what’s handled by large language models and good rags and actually three different components. It’s your good retrieval pipeline. It’s a good generative model and it’s a good way of linking them up. So it can be formatting your prompt or whatever. [6:20] Ben: And it’s very important to think about it when you’re saying my rack doesn’t work. You need to be more specific like my rack doesn’t work is the same as saying my car doesn’t work. It’s like yeah, but something specific is broken. You need to figure out what is the retrieval part is the LLM struggling to make use of the context, etc. There’s a lot of failure cases there. And with that being said, let’s look at what the compact MVP is. [6:44] Ben: So that is basically what you will see, I think, if you’ve read any Medium blog post about the advent of Frag in early 2023. That’s the pipeline that everyone used. And that’s also because the easiest pipeline to put into production is very simple. You have a query. You have an embedding model. You have documents. The documents get embedded and pulled into a single vector. Then you do cosine similarity search between the vectors for your query and for the documents. And that gets you a result. That gets you a score. [7:10] Ben: And this is a bit of a teaser for an upcoming slide when I say this is called the Bayan Khodor approach, but just so you get the term in mind and I’ll define it because that’s one of those things that is like a scary term. That’s actually very, very simple when you break it down. But first, let’s look at what this actually means in code, this whole pipeline. So the first thing you want to do is load your model. [7:29] Ben: Then you get your data, you encode it, you store your vectors, and you get your query, you encode it. And then here we use NumPy, you do a cosine similarity search, eg a dot product between normalized vectors to get the most similar documents. And the documents whose embedding are similar to your query embedding is what you would consider as your relevant documents. And that’s pretty much it. Thanks. modified from something that Jeremy did to showcase how simple RAG actually is in his Hackers Guide to LLMs. [8:02] Ben: But that’s what you want to do to retrieve context in the simplest possible way. And you will have noticed that there’s no vector DB in this. This is all numpy arrays. And this is all numpy arrays because when you use vector DBs, the huge point of using a vector DB is to allow you to efficiently search through a lot of documents because what a vector DB does generally, not all of them, but most of them, wrap stuff like HNSW, IVFPQ, which are indexing types. [8:31] Ben: That allows you to do is to find and retrieve relevant documents without having to compute cosine similarity against every single document. It tries to do an approximate search of an exact search. This is not something that you need if you’re embedding 500 documents. Your CPU can do that in milliseconds. You don’t actually need a vector DB if you’re trying to go to the simplest possible stage. But if you wanted one, it would go right here on the graph, right after you embed your documents, you would put them in the vector DB. [9:00] Ben: And the second thing I think to discuss about is like this tiny graph is why am I calling embeddings by encoders? Because that step that I call by encoder, you will have seen a lot of times, but you will always see it generally called embeddings or model. And by encoder is the term that the IR literature uses to refer to that. And it’s simply because you encode things separately, like you do two encoding stages. So it’s a by encoding. And that’s used to create single vector representations where you pre-compute all your documentary presentations. [9:30] Ben: So when you’re using by encoders, you encode your documents whenever you want. Like when you’re creating your database, when you’re adding documents, those get encoded at a time that’s completely separate from intrants. And then only at intrants will you, like in the second aspect of this column, will you embed your query to compare to your pre-computed documentary presentations. So that’s really computationally efficient because at inference you’re only ever encoding one thing which is the query and everything else has been done before. And so that is part of why it’s done that quickly. [10:02] Ben: And I did want to take a slight break because I can see there are questions, but they’re not showing up on my screen. So if there are any on this quick MVP, then. [10:11] Participant 3: Yeah, let me look through some of the questions. I’m going to give you a few of them and you can decide whether you want to take them now or later. So we got one. It’s a 7,000 query, a 7,000 question and answer data set. Can I optimize RAG to accurately retrieve and quote exact answers? We’ll also effectively hand in queries that are slightly different from the original data. I think there’s actually two parts to that. So one is to quote the exact answer, which is something about the information retrieval part. [10:45] Participant 3: But it’s rather just like, what do you tell the LLM to do? But the information retrieval part is probably well. [10:56] Ben: I will actually cover how to better deal with out of context things in the upcoming slide. [11:04] Participant 3: Why do you keep going? [11:08] Ben: None of these questions can be saved right now. Perfect. The next one is if that’s very computationally efficient, there is an obvious trade-off here. And that is your documents are entirely unaware of your query and your queries are entirely unaware of your documents. [11:24] Ben: which means that you’re very very like subject to how it was trained is basically if your queries look a bit different from your training data or if like if there’s very very specific information that will be in certain documents and not other sometimes you want to know what how the query is phrased you want to know what the query is looking for when you’re encoding your document so that it can like kind of paint that representation and represent it more towards information that you’re interested in And that’s done with what we call re-ranking. [11:53] Ben: So re-ranking is another one of those scary stages that we’ll see in your pipeline. And the most common way to do re-ranking is using something that we call cross-encoder. And cross-encoder is another one of those scary words, like by encoder that you feel should be like a very advanced concept, but it’s actually very simple. This graph here represents the whole difference between them. The bi-encoder is basically this two column system that we described where documents get encoded in their corner, queries get encoded in their own corner, and they only meet very, very late. [12:20] Ben: Like you only do cosine similarity between vectors, but the documents never seen the query and vice versa. The cross-encoder is different. The cross-encoder is a model that will take your document and your query together. So you’re going to give it both your document or like a series of documents, depending on the type of model, but to keep it simple, we do it one by one. So you always give it a query document pair. And you put it through this cross-encoder model, which is effectively a classifier with a single label. [12:46] Ben: And the probability of the label being positive is what your model considers as how similar the documents are or how relevant it is. This is extremely powerful because it means that the model knows everything about what you’re looking for when it’s encoding the document. It can give you a very accurate score or at least a more accurate score. The problem is that you can see how that wouldn’t scale because it’s not very computationally realistic to compute this query document score for every single query document pair every time you want to retrieve a document. [13:15] Ben: Say you’ve got Wikipedia embedded, you’ve got, I don’t know, like 10 million paragraphs. You’re not going to compute 10 million scores. through a model for like using 300 million parameters for every single document for you. You would eventually return something and it would be a very, very relevant document, but it will also take 15 minutes, which is probably not what you want in production. [13:37] Ben: So you probably also have heard, or you might also have heard if you’re really into retrieval, or not heard at all if you’re not into retrieval of other re-ranking approaches like RankGPT or RankLLM using LLMs to rank documents has been a big thing lately. For people really into retrieval, you will know of MonoT5, et cetera. So those are not cross-encoders, but that’s not really relevant to us because the core idea is the same, and that’s basically what we always do with re-ranking in the pipeline. [14:04] Ben: you use a powerful model that is computationally expensive to score only a subset of your documents. And that’s why it’s re-ranking and not ranking, because this can only work if you give it like, I don’t know, 10, 50, not more than that document. So you always have a first stage retrieval, which here is our vector search. And then the re-ranker does the ranking for you, so it creates an ordered list. There’s a lot of ways to try those models out. [14:28] Ben: Some of them have an API base, so it’s just an API called to cohere or Jena. Some of them you run your machine. If you want to try them out, and this is basically the self-promotion moment, I do maintain at answer.ai library just called rerankers with the QR code here, where it’s basically a unified API so you can test any ranking method in your pipeline and swap them out freely. And that’s what your pipeline looks like now. [14:52] Ben: It’s the same with just that one extra step at the end where you re-rank things before getting your results. So we’ve added re-ranking, but there’s something else that’s missing here. And that’s something actually addresses the first question, at least partially, is that the semantic search via embeddings is powerful and I’m not saying don’t choose vectors. Vectors are cool, like models are cool, deep learning is cool. But it’s very, very hard if you think about it, because you’re asking your model to take, I don’t know, 512 tokens, even more if you’re doing long context. [15:24] Ben: And you’re like, okay, put all of this into this one vector. We are just using a single vector. You’ve got like, I don’t know, 384, 1024 at most floats, and that must represent all the information in this document. That’s naturally lossy. There’s no way you’re going to keep all of the information here. And what you do when you’re training on embedding is that you’re teaching the embedding to represent information that is useful in their training. [15:49] Ben: So the model doesn’t learn to represent all of the document’s information because that’s pretty much impossible since embeddings are essentially a form of compression. What the model actually learned is to replant the information that is useful to the training queries. So your training data is very, very important here. It’s like replanting the documents in a way that will help you use the queries in the way that phrase in your training data to retrieve a given document. [16:16] Ben: So when you use that on your own data, it’s likely that you’re going to be missing some information, or when you go slightly out of distribution. There’s another thing which is humans love to use keywords, especially if you’re going into the legal domain, the biomedical domain, anything specific. We have a lot of acronyms that might not even be in the training data, but we use a lot of acronyms. We use a lot of very advanced medical words. People love jargon. People love to use technical words because they’re very, very useful. [16:44] Ben: And that’s why you should, and I know it sounds like I’m talking from the 70s, because that’s actually a method from the 70s, but you should always have keyword search in your pipeline. You should always also have full text search on top of like anything that you do with vectors. And keyword search, which you can call full text search or like tfidifbm25, it’s powered by what we call tfidif. [17:06] Ben: which is a very basic NLP concept that essentially stands for term frequency, inverse document frequency, and it assigns every single word in a document or a group of words because sometimes we do them two by two, or three by three even. It gives them a weight based on how rare they are. So a word that appears everywhere like V or A has a very, very small weight and a word that’s highly specific to certain documents has a very high weight. [17:32] Ben: And the main method to use TF-IDF for retrieval is called BM25, which stands for Best Matching 25. It was invented in the 70s. It’s been updated since then, but it’s basically just been iterations of it. And you’ll often hear IR researchers say that the reason that the field’s not taken off like NLP has or Computer Vision has is because the baseline is just too good. We’re still competing with BM25, although it’s been 50 years now. Oh my god, it’s been 50 years. [18:00] Ben: Yeah, so the M25 existed for like, basically my entire lifetime before my birth, and it’s still used in production pipeline today. That’s how good it is. And the good thing is it’s just word counting with a match with like a waiting formula. So the compute time is virtually unnoticeable. Like you can add that to your pipeline, you will absolutely never fail it. [18:20] Ben: And I said I wouldn’t add anything from papers, but I feel like because I’m making a very strong claim that this method from 70 is strong, I should add a table and add the table from the bare paper, which is the retrieval part of MTEB, which is basically the main embeddings benchmark. And they compared it to a lot of models that were very popular for retrieval, like DPR and very strong vector retrievers. [18:45] Ben: And basically, you can see that unless you go into very over-trained embeddings like E5, BGE, BM25 is competitive with virtually all deep learning-based approaches, at least at the time of the paper, which was only just three years ago. We now have embeddings that are better, but we don’t have any embeddings that are better to the point where they’re not made better by being used in conjunction with BM25. [19:10] Ben: Knowing that this is how you want your pipeline to look, you’ll notice that there’s now a whole new pathway for both the query and the documents, who are on top of being encoded by the embedder, they’re also encoded by TF-IDF to get full text search, and that will help you retrieve keywords, etc. Humans use keywords in queries all the time, it’s something you should do. At the end, you will combine the scores. You can do that in a lot of ways. [19:33] Ben: I won’t go into too much details, but what a lot of people do is give a weight of 0.7 to the cosine similarity score and 0.3 to the full text hash. But I’m pretty sure we could do a whole talk for an hour on different methods of combining that. Okay. I do have five more minutes. [19:50] Ben: So the last one that you want to add to a simple pipeline, the thing that I think really completes your MVP plus plus is using metadata and using metadata filtering because academic benchmarks don’t because in academic benchmarks documents exist mostly in a vacuum like they don’t exist in the real world they’re not tied to a specific company etc when you’re using rag in production it’s very, very rare that someone comes to you and says, these documents came to me in a dream and caught them. Like they came from somewhere. They’ve been generated by a department. [20:21] Ben: They’ve been generated for a reason. They might be old Excel sheets or whatever, but they have business sense or they have in context sense. And the metadata is actually sometimes a lot more informative than the document content, especially in RAG contexts. So if you take the query here, which is, can you get me the Cruise Division financial report for Q422? There’s a lot of ways in which this can go wrong if you’re just looking at it from the semantic or even using keywords aspect. [20:52] Ben: When you say, when you see like this, the model must capture the financial report. So you, the model must figure out you want the financial report, but also cruise division Q4 and 2022 and embedding models are bad at numbers. So you might get a financial report, but maybe for another division, or maybe for the cruise division of 1998, it’s very hard to just hope that your vector will capture all of this. [21:15] Ben: But there’s another failure case, which will happen, especially with weaker LLMs, is that if you just have like top Ks and top five documents, and you retrieve the top five documents for your query. Even if your model is very good, if you just let it retrieve the top five documents, no matter what, you will end up with financial reports, at least five of them, and there’s most likely only one for Q4 22. [21:37] Ben: So at that point, you’re just passing all five to the model and being like, good luck, use the right one, which might confuse it, especially because tables can be hard, et cetera. And I’m not saying that your vector search will fail, but statistically it will. In most cases, it will fail. If it don’t fail for this query, it will fail for a similar one. But that’s actually very, very easy to mitigate. You just have to think outside of the vector and just use more traditional methods. You can use entity detection models. [22:07] Ben: One that’s very good for this is Gleaner, which is a very recent model that does basically zero-shot entity detection. You give it arbitrary entity types, so document type, time period, and the department. And this is like a live thing of Glenore. You can run the demo on the bottom, but here we just extract financial report, time period, and department. And when you generate your database for RAG, all you need to do is basically specify the time period. [22:32] Ben: So when you get an Excel sheet, you will just pass the name for it or pass the date in it and give metadata 2024 Q2, and Q4, sorry, 2022 Q2, the Q4. Okay, mixed up there. Then you just need to ensure that this is stored alongside your document. At query time, you can always pre-filter your document set to only query things that make sense. You will only query documents for this relevant time period. [22:56] Ben: You ensure that even if you give your model the wrong thing, it will at least be the right time frame so it can maybe try and make sense of it. And with this final component, this is what your pipeline looks like. You can see the new component here, which is metadata filtering, which doesn’t apply to queries. Queries go right through it. The documents get filtered by that, and we won’t perform search on documents that will not meet the metadata that we want. [23:20] Ben: And okay, I do agree that this looks a lot scarier than the friendly one at the start, which just had your embedder and then cosine similarity search and the results. It is actually not very scary. This is your full pipeline. This implements everything we’ve just talked about. It’s about 25 lines of code if you remove the commands. It does look a bit more unfriendly because there’s a lot more moving parts, I think. There’s a lot more steps, but if you want, we can just break it down a bit further. We use LensDB for this. [23:50] Ben: This is not necessarily an endorsement of LensDB as a vector DB, although I do like LensDB because it makes all of these components, which are very important, very easy to use. But I try not to take side in the vector DB wars because I’ve used WeaveYard, I’ve used Chroma, I’ve used LensDB, I’ve used Pencode, they all have their place. But I think LensDB, if you’re trying to build an MVP, is the one you should always use for MVPs right now because it has those components built in. [24:14] Ben: And here you can see just how easy it actually is. So we still load the By Encoder, just in a slightly different way, same as earlier. We define our document metadata. Here is just a string category, but it could be a timestamp, it could be just about anything. Then we encode a lot of documents just like we did previously. Here we’ve created, so it’s not an index, this is still a hard search, this is not an approximate search. Then we create a full text search index, which is generating those TF-IDF. [24:40] Ben: Why I mentioned before, we give a way to every single term in the documents. Then we load the reranker. Here we’re using the query ranker because it’s simple to use an API. And at the very end, you’ve just got your query and your search where we restrict it to the category equals films. So we will only ever search into the document that’s about a film, not about an author, not about a director. We get the top 10 results and we just have a quick ranking step. And that’s pretty much it. [25:06] Ben: We’ve taken the pipeline at the start, which only had the biancoder component to a pipeline that now has the biancoder component, metadata filtering, full text search, and a reranker at the end. So we’ve added like basically the four most important components of RetriVault into a single pipeline. And it really don’t take much more space in your code. And Yeah, that is pretty much the end of this talk. So there’s a lot more to cover in RAC. This is definitely not the full cover of RAC, but this is the most important thing. [25:36] Ben: This is what you need to know about how to make a good pipeline very quickly. All the other improvements are very, very valuable, but they have a decreasing cost effort ratio. This takes virtually no effort to put in place. Definitely worth learning about sparse methods, multi-vector methods, because they are very adapted to a lot of situations. Colbert, for instance, is very strong out of domain. Sparse is very strong in domain. [25:58] Ben: You should watch Jason’s talk about rack systems and Joe’s upcoming talk about retrieval evaluations because those are by a clear trifecta of the most important things. And yeah, any questions now? [26:12] Participant 3: Hamel and I were just messaging saying… We love this talk. Everything is presented so clearly. We’ve also got quite a few questions. [26:30] Hamel: My favorite talk so far. Not big favorites, but yeah. [26:36] Ben: Thank you. [26:37] Participant 3: Go ahead. [26:41] Hamel: Okay, questions. Did you have one that you were looking at already, Dan? I can tell it. [26:47] Participant 3: Yeah, we’ve got one that I quite like. Can the way that you fine-tune your bi-encoder model affect how you should approach fine-tuning for your cross-encoder and vice versa? [26:58] Ben: Yes. I don’t think I can give a really comprehensive answer because it will really depend on your domain, but you generally want them to be complementary. So if you’re in a situation where you’ve got the compute and the data to fine-tune both, you always want to… by encoder to be a bit more loose. Like you want it to retrieve potential candidates and then you want to trust your reranker, like your cross-encoder to actually do the filtering. [27:21] Ben: So if you’re going to use both and have full control over both, you might want to fine tune it in a way that will basically make sure that your top K candidates can be a bit more representative and trust the reranker. [27:35] Participant 3: Let me ask, this wasn’t an audience question, but a related question. You showed us where the, when you choose questions to feed into the re-ranker, that’s sort of a weighted average of what you get from the TF-IDF or BM-25 with what you get from the just simple vector search. What do you think of as the advantage or disadvantage of that over saying we’re going to take the top X from one cat from one of the rankers and the top X from the others? [28:14] Participant 3: And that way, if you think one of these is, for some questions, especially bad, you have a way of short-circuiting its influence on what gets sent to the re-ranker. [28:27] Ben: Yeah, I think that also makes complete sense. And that’s another, that’s a cop-out answer I use a lot, but that also depends a lot on your data. Like a lot of the time you want to look at what’s your actual context and how it’s actually being used. Because in some situations that actually works better, like especially if you work with biomedical data, because there’s so much like specific documents, it’s quite often the embedding won’t be that amazing on some questions. [28:52] Ben: So you just want to take the top five from both and get the re-ranker to do it, because the re-ranker is quite aware. So it’s a perfectly valid approach to combine them that way. [29:04] Participant 3: You want to pick a question, Hamel? [29:10] Hamel: Yeah, I’ve been looking through them. You guys have been… Okay, Jeremy’s asking, can we get a link to the code example? Yeah, sure. Your slides in Maven. We can also, can I share your slides in Discord as well, Ben? [29:25] Ben: Yes, please. [29:26] Hamel: Yeah. I’ll go ahead and share the slides in [29:28] Ben: Discord. And I’ll share the GitHub gist for the code examples I thought of. [29:34] Participant 3: And I’ll embed the link to the slides in Maven for people who want to talk some point deep into the future and might lose track of it in Discord. There’s a question somewhere in here I’ll find in a moment, but we got this question for Jason, the speed and then the speaker just before you, Paige Bailey said. RAG, you know, in the world of million token context lengths is not going to be as important. What’s your take on the relative importance of RAG in the future? [30:20] Ben: So I’m still very hopeful about RAG in the future. And I think I see it as some sort of like, so your LLM to me is like your CPU and your context window will be your RAM. And so like, even if you’ve got 32 gigs of RAM, nobody’s ever said, yeah, throw away your hard drive. You don’t need that. Like in a lot of contexts, you will still want to have like some sort of storage where you can retrieve the relevant documents. [30:42] Ben: Having to use a long context window is never going to be a silver bullet. Just like RAG is never a silver bullet. But I’m actually really happy because it just means I can retrieve much longer documents and get more efficient rack systems. Because to me, it’s a bit of a trade off where if you’ve got a longer context, it just means you’ve got a lot more freedom with how quick your retrieval system can be. Because if you need to use top 10 or top 15, that’s fine. You can fit them in. [31:06] Ben: Whereas when you can only fit the top three documents, you need your retrieval system to be really good, which might mean really slow. Yeah. [31:12] Participant 3: So, yeah. [31:13] Ben: So, yeah. [31:26] Participant 3: We had a question from Wade Gilliam. What are your thoughts on different chunking strategies? [31:36] Ben: I probably don’t think about chunking as much as I should. I am very hopeful for future avenues using LLMs to pre-chunk. I don’t think those work very well right now, but in my test I’ve never been impressed. Also, I do tend to use Colbert more often than Bancoders, and Colbert is a lot more resistant to chunking, so it’s something that I don’t care about as much. But generally I would try to… [32:01] Ben: So my go-to is always to chunk based on like around 300 tokens per chunk, and try to do it in a way where you never cut off a sentence in the middle, and always keep like the last 50 tokens and the next 50 tokens of the previous and next chunk. Because information overlap is very useful to give content, like please don’t be afraid to duplicate information in your chunks. [32:22] Hamel: I have a question about the buy encoder. Do you ever try to fine tune that using some kind of like label data to get that to be really good? Or do you usually kind of use that off the shelf and then use a re-ranker? And how do you usually go about it or how do you make the trade off? [32:43] Ben: So again, context dependent, but if you have data, you should always fine-tune all your encoders, be it the bi-encoder, the cross-encoder. I think Colbert, because it’s single vector, you can get away with not fine-tuning for a bit longer because it’s multi-vector, so you can get away with not fine-tuning for a bit longer. But if you have data, it’s all about like basically the resources you have. So in this talk, we’re doing an MVP, this is something you can put together in an afternoon. If your company says you have $500. [33:10] Ben: Spend 480 of that on OpenAI to generate synthetic questions and find your encoders that will always get you better results. Like always find your encoders if you can. And so, yes, so a couple of questions about fitting Colbert in and I’m using this entire executive decision to answer those. So Colbert in this pipeline, some people use it as a re-ranker, but then that’s not optimal. That’s very much when you don’t want to have to change your existing pipeline. [33:50] Ben: If you were to design a pipeline from scratch and wanted to use Colbert, you would have it instead of the BI encoder and it would perform basically the same role as the BI encoder, which is first-edge retrieval. And if you wanted to use Colbert, and especially if you don’t have the budget to fine-tune and need a re-ranking step, sometimes it can actually be better to use Colbert as a re-ranker still. Because the multi-vector approach can be better at capturing keywords, etc. But that’s very context-dependent. So ideally, you would have it as ShowByEncoder. [34:22] Participant 3: For a lot of people here who probably aren’t familiar with Colbert, Colbert, can you give the… Quick summary of it? [34:32] Ben: Yeah, sorry, I got carried away because I saw the question. So Colbert is an approach which is effectively a biancoder, but instead of cramming everything into a single vector, you represent each document as a bag of embeddings. So like, if you’ve got 100 tokens, instead of having one big 124 vector, you will have a lot of small 128 vectors, one for each token. And then you will score that at the end. You will do the same for the query. So if your query is 32 tokens, you will have 32 query token. [35:02] Ben: And for each query token, you will compare it to every token in the document and keep the highest score. And then you will sum up those highest scores and that will be the score for that given document. That’s called max similarity. And the reason that’s so powerful is not because it does very well on data it’s been trained on. You can beat it with a normal Bayer encoder, but it does very well at extrapolating to out of domain because you just give the model so much more room to replant each token in its context. [35:29] Ben: So it’s much easier if you’re in a non-familiar setting, you’ve not compressed as much information. And I do have self promotion. I do have a pretty cool Colbert thing coming out later this week to compress the Colbert space by reducing the tokens that actually needs to save by about 50 to 60% without losing any performance. So that’s a bit of a teaser, but look forward to the blog post if you’re interested. [35:57] Participant 3: And to find the blog post, you suggest people follow you on Twitter or? [36:02] Ben: Yeah, definitely follow me on Twitter. Because it was pretty much the only place where you can reliably reach me. [36:14] Hamel: Someone’s asking what are some good tools to fine tune embeddings for retrieval? Would you recommend Ragatouille or anything else? Like what’s your… [36:24] Ben: I’d recommend sentence transformers, especially with the 3.0 release recently. It’s now much, much funnier to use. It’s basically, there’s no need to reinvent the wheel. They’ve got all the basics implemented very well there, so sentence transformers. [36:44] Participant 3: Question from Divya. Can you give any pointers on how one fine-tunes their embedding model? [36:53] Ben: Sorry, can you repeat that? I could have said it. [36:55] Participant 3: Yeah. The question is, can you give any pointers or describe the flow for when you fine tune your embedding model? [37:04] Ben: Okay. So that’s probably a bit more involved than this talk, but essentially when you fine tune your embedding model, what you’ll want is queries. You need to have queries and you need your documents and you’re going to tell the model. For this given query, this document is relevant. And for this given query, this document is not relevant because sometimes there’s a triplet loss. And a triplet loss is what you will do when you have one positive document and one negative document. And you’ll kind of be teaching the model, this is useful, this is not useful. [37:32] Ben: And I’m not going to go down too much because this rabbit hole can take you quite far. But sometimes when you have triplets, you also want to use what we call hard negatives. which is you want to actually use retrieval to generate your negative examples because you want them to be quite close to what the positive example is, but not quite the right thing. Because that’s why you teach the model more, but was actually useful to answer your query. [37:57] Ben: So the workflow is probably, as always, look at your data, figure out what kind of queries your user will actually be doing. If you don’t have user queries. Go into production, write some, write some queries yourself and give that to an LLM, generate more queries and you can have a pretty solid ritual pipeline like that. [38:16] Hamel: Someone’s asking in the Discord, and I get this question all the time, is please share your thoughts on graph rag. [38:25] Ben: I have never actually done graph rag. I see this mentioned all the time, but it’s not something that has come up for me at all. So I don’t have strong thoughts about. I think it’s cool, but that’s pretty much the full extent of my knowledge. [38:49] Hamel: Someone’s asking, okay, when you have long context windows, does that allow you to do something different with RAG, like retrieve longer documents or do any other different kinds of strategies than you were able to before? Does it change anything? How you go about this? [39:09] Ben: Yeah, I think it’s a bit what I mentioned before. To me it changes two main things. One is I can use longer documents, which means I can use longer models, or I can stitch chunks together. Because sometimes if your retrieval model isn’t very good at retrieving long documents, which is often the case, you might just want, if I get a chunk from this document, give the model the full document. Like if I just get a chunk from it past the full context and you just hope the model is able to read it. [39:34] Ben: And if you’ve got a good long context model, it can. So it changes how you decide to feed the information into the model. And then the other aspect is, like I said, it changes the retrieval overhead because if you need to be very good, like I was saying, if you need only the top three documents to be relevant, you’re going to spend a lot of time and money on retrieval pipeline. If you’re like, oh, as long as my recall at 10 or my recall at 15 is good, that’s fine. [39:56] Ben: You can afford to have much lighter models and spend a lot less time and resources on retrieval. There’s a lot of diminishing returns in retrieval when getting a good recall at 10. So recall at 10 is how likely you are to retrieve the relevant document in the first 10 results. is generally very easy. Recall at 100 is very, very easy. And then recall at 5 is getting harder. And recall at 3 and recall at 1 are like the really tough ones because a lot of the training data is noisy. [40:23] Ben: So it can even be hard to know what a good recall at 1 is. So longer context makes that irrelevant. And that’s why it’s great for RUG. [40:49] Hamel: Someone’s asking, and I don’t even know what this means, what’s your view on PIDE versus React versus StepBack? [40:58] Ben: I’ve only used React out of those. And so those are like adjunct systems of function coding. It’s like to give your LLM the ability to call tools, at least React is. I don’t have strong thoughts on those in the context of retrieval, so I can’t really answer the question. Yeah, I think. I would occasionally use React from the model to be able to trigger a search itself, but I think that’s still an open area of research. And I think Griffin from AnswerIA is also in the chat and he’s very interested in that. [41:31] Ben: It’s basically how do you get a model to tell you that it doesn’t know? Because sometimes you don’t need retrieval, the model already knows. Sometimes you do need retrieval, but that’s still a very open question. Like how do you decide when to search? So no strong thoughts there yet. [41:50] Participant 3: You may or may not have a good answer for this one. Is there an end-to-end project, open-source project, that someone could look at as a way to see or evaluate the difference in result quality when they do result from just buying code or MVP and compare that to the final compact MVP++ that you showed? [42:14] Ben: No, actually, that’s a very good point. I don’t think there is one that systematically goes through every step. And that’s probably something that I would like to build at some point or find one because just like most things in retrieval, everything is kind of conventional wisdom. Like you’ve seen it piece and pieces in a lot of projects and you just know that that’s how it is. But unless you dig deep into the papers or like do it yourself, it’s quite rare to find very good resources showing that. [42:54] Participant 3: A related question, do you have a tutorial that you typically point people to on fine-tuning their encoder? [43:08] Ben: That would be the sentence transformers documentation, but it’s not the friendliest tutorial, so that’s a half answer. That’s what I would point you to, but it’s still a bit hard to get into, sadly. [43:40] Hamel: Wade is asking if you have go-to embedding models. [43:48] Ben: I think my go-to these days when I’m demoing something is the Korea one, because it’s nice to be able to work with an API. It works really well. It’s cheap. But other than that, I would just call bear if I’m using something in my own pipeline. I would use multi-vectors. But it really depends on the use case, because you would often find that some things work well for you and some things don’t. I do have strong opinions on not using… [44:16] Ben: So if you go to the MTB leaderboard, which is the embedding leaderboard right now, you’ll see a lot of LLMs as encoders. And I would advise against that because the latency is not worth it. Don’t need 7 billion parameters to encode stuff. And at least some of the early ones actually generalized worse, like Neil Schremer from Cohere had a really interesting table where the E5 mistral was worth an E5 large, despite being seven times as big. [44:47] Ben: So probably just stick to the small ones between 100 and at most a billion parameters, but that would be my only advice about that. Try all the good ones like GT, BG, E5. [45:00] Hamel: Chris Levy is asking… this question about Elasticsearch, which I also get quite a lot. So he asks, Anyone here have experience building RAG application with just keyword BM25 as a retriever at work? It makes use of Elasticsearch. And he said it’s all over the tech stack that people are already using Elasticsearch. Is there basically he’s asking, is there a way to keep using Elasticsearch with RAG that you know about or that you have encountered? Or do you mainly use like vector database like LanceDB and things like that? [45:32] Hamel: Have you tried seeing people using Elasticsearch and trying to bootstrap off of that? [45:37] Ben: Yeah, I’ve used Elasticsearch a bit and it’s perfectly possible. You do lose obviously the semantic search aspect, although I think now Elasticsearch has a vector DB offering, so you could add vectors to it. You could always plug in, you could always just do BM25 and then plug in a re-ranker at the end. That’s often, if you read papers on like cross encoders, generally the way they evaluate them is actually doing just that, like do BM25 to retrieve 50 to 100 documents and then rank them using the re-ranker. [46:07] Ben: which if you can afford to just set up your re-ranking pipeline or call the Core API is a really good way to go about it because you don’t need to embed your whole documents to sample how good it would be with deep learning because there are domains where you do not need deep learning, BM25 is still good enough in some bits and you know like I think it’s become very apparent like BM25 has never told anyone they should eat three rocks a day whereas embeddings have so [46:35] Hamel: Dimitri is asking, is it worthwhile to weigh the BM25 similarity score during the re-ranking step as well? [46:45] Ben: Probably not. You generally just want to use BM25 to retrieve candidates, but you don’t need to give those scores to your cross-encoder. [46:59] Participant 3: There’s a question. I’m going to change it slightly. Someone asks about retrieving from many documents rather than finding the best one. Maybe the tweak there is if you have a theory that information within any single document is so correlated that you actually want to try and get some diversity, are you familiar with or have you used approaches where you I specifically try in some loss function somewhere, encourage that diversity and encourage pulling from many documents rather than from one. [47:37] Ben: I have not done that myself. I know that there’s different loss methods to optimize for diversity versus clear accuracy. But I don’t think I would be able to give you a clear answer without sounding really confident about something I don’t know much about. [47:59] Participant 3: Have you used hierarchical reg? Any thoughts on it? [48:03] Ben: I have not, and I don’t think it’s very needed for the current pipelines. I think there’s a lot of other steps you can improve on. [48:18] Participant 3: Since I think we have several Answer AI people here, I don’t know if this is a question or a request, I’m eager to learn if Answer AI will come up with any books on LLM applications in the future. [48:33] Ben: I don’t think so, but never say never. Jeremy, if you want to chime in. Because I can’t make any promises because my boss is watching. [49:00] Participant 3: You see anything else to Ben, did you say that you can’t see the questions? [49:05] Ben: Yeah, they’re all blank for me. I saw one earlier, but they really show up sporadically. [49:10] Participant 3: Yeah. Not sure what’s happened with her. And I think people also cannot upvote these. So a couple of quirks today. You see any others here, Emil, that you think we should pull in? [49:25] Hamel: um no not necessarily i think like probably going to the discord is pretty good now yep tons of activity there as well um I mean, there’s infinite number of questions, so we can keep going. Okay, Lorien is asking, what’s the best strategy when chunks, when the documents, when chunks of documents don’t fit into the context window? Do you do RAG in a MapReduce style, summarize aggressively? What are the techniques that you’ve seen work most effectively? [50:25] Ben: So that’s, I think, a very broad question, because it’s like, why do they not fit? Is it because like every document is really long? Is it because you need a lot of different documents, etc, etc? So. And also another important aspect is what’s the latency tolerance? Because quite a lot of the time you can make RAG infinitely better, but users won’t stay waiting like 20 seconds for an answer. So you need to figure out like, how much time do I have? [50:52] Ben: One way that you can often see what I’ve done in production actually is retrieve the full documents but have another database that maps every document to its summary. So you will have done your LLM summarization at the previous step. You will retrieve the relevant chucks, and then you will pass the relevant summaries to the context window. But that kind of depends on your actual setting. I have another call at 10, which is in five minutes for me. So if you’ve got another final question. [51:35] Hamel: I really enjoyed this presentation. [51:39] Ben: Thank you. [51:42] Participant 3: Yeah, this was really great. Everything is super clear and well presented, so thanks so much. [51:54] Ben: Thank you. Cheers. [51:59] Participant 3: Thanks, everyone.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Beyond the Basics of RAG"
    ]
  },
  {
    "objectID": "education/fine_tuning/pawel.html",
    "href": "education/fine_tuning/pawel.html",
    "title": "Fine Tuning LLMs for Function Calling",
    "section": "",
    "text": "This talk was given by Pawel Garbacki at the Mastering LLMs Conference.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine Tuning LLMs for Function Calling"
    ]
  },
  {
    "objectID": "education/fine_tuning/pawel.html#chapters",
    "href": "education/fine_tuning/pawel.html#chapters",
    "title": "Fine Tuning LLMs for Function Calling",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction and Background\n00:29 Functional Tool Calling Overview\n02:23 Single-Turn First Call Objective\n02:51 Forced Call Explanation\n03:28 Parallel Function Calling\n04:00 Nested Calls Explanation\n06:24 Multi-Turn Chat Use Case\n13:54 Selecting Function Call Syntax\n17:44 Full Weight Tuning vs. LoRa Tuning\n19:19 Efficient LoRa Serving\n23:06 Constrained Generation\n26:21 Generic Function Calling Models\n40:02 Q&A",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine Tuning LLMs for Function Calling"
    ]
  },
  {
    "objectID": "education/fine_tuning/pawel.html#resources",
    "href": "education/fine_tuning/pawel.html#resources",
    "title": "Fine Tuning LLMs for Function Calling",
    "section": "Resources",
    "text": "Resources\n\nGlaive Function Calling\nPawell Garbacki’s LinkedIn\nFireworks website",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine Tuning LLMs for Function Calling"
    ]
  },
  {
    "objectID": "education/fine_tuning/slaying_ooms.html#chapters",
    "href": "education/fine_tuning/slaying_ooms.html#chapters",
    "title": "Slaying OOMs with PyTorch FSDP and torchao",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nMark introduces the session on addressing Out of Memory (OOM) errors in PyTorch, discussing tools and techniques to handle these issues more effectively.\n00:30 Traditional Solutions to OOMs\nMark describes conventional methods of dealing with OOMs, such as reducing batch size or model size, and the limitations of these approaches.\n00:48 VRAM Constraints\nMark explains VRAM constraints on different GPUs and how it impacts model training, emphasizing the perpetual challenge of being VRAM-starved.\n01:24 Estimating VRAM Requirements for Your Model\nMark outlines the components involved in estimating a model’s memory usage, including parameters, gradients, and optimizer states.\n06:06 Quantization Techniques\nMark introduces quantization techniques, such as 4-bit quantization, to reduce model size and memory requirements. He also demonstrates using Torch compile to generate efficient quantization kernels, avoiding the complexity of writing custom CUDA kernels.\n09:27 LoRA\nMark introduces the LoRa technique for updating a subset of parameters to save memory.\n09:56 QLORA Algorithm\nMark details the QLORA algorithm, combining quantized parameters with selective parameter updates to enable efficient fine-tuning.\n10:51 Implementing QLORA with PyTorch\nDiscussion on implementing QLORA with PyTorch, highlighting the complexity of writing efficient kernels and the benefits of using Torch compile.\n14:38 Introducing Jane’s Section on Model Parallelism\nMark hands over to Jane to discuss parallelism techniques and how to manage memory across multiple devices.\n15:20 Understanding Memory Allocation During Training\nJane illustrates memory allocation during training, showing the impact of activations, gradients, and optimizer states. Jane also explains data parallelism and model sharding as techniques to distribute memory load across multiple GPUs.\n17:45 Fully Sharded Data Parallel (FSDP)\nJane introduces Fully Sharded Data Parallel (FSDP) and its mechanism to manage memory efficiently by sharding model parameters.\n21:49 CPU Offloading\nJane discusses CPU offloading as a method to handle memory constraints by temporarily storing parameters on the CPU during training.\n23:05 Challenges and Improvements in FSDP\nJane outlines the challenges with FSDP1 and introduces FSDP2, which offers more flexibility and efficiency in managing memory and data types.\n29:50 Identifying and Addressing Performance Gaps\nJane discusses the process of identifying performance gaps in FSDP2 and the steps taken to optimize and match the performance of FSDP1. Jane discusses benchmarking and profiling techniques that are helpful in debugging performance.\n37:06 Overcoming Debugging Challenges\nJane shares insights from debugging and optimizing the performance of FSDP2, highlighting the importance of detailed trace analysis. She also explains the impact of wrapping policy on memory usage.\n47:38 How You Can Get Started\nJane encourages students to try this process themselves in torchtune.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Slaying OOMs with PyTorch FSDP and torchao"
    ]
  },
  {
    "objectID": "education/fine_tuning/slaying_ooms.html#slides",
    "href": "education/fine_tuning/slaying_ooms.html#slides",
    "title": "Slaying OOMs with PyTorch FSDP and torchao",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Slaying OOMs with PyTorch FSDP and torchao"
    ]
  },
  {
    "objectID": "education/fine_tuning/slaying_ooms.html#resources",
    "href": "education/fine_tuning/slaying_ooms.html#resources",
    "title": "Slaying OOMs with PyTorch FSDP and torchao",
    "section": "Resources",
    "text": "Resources\nLinks to resources mentioned in the talk:\n\nSlides from the talk + traces\nGo from 1 GPU to N GPUs with FSDP2\nEnd to end finetuning examples / A Native-PyTorch Library for LLM Fine-tuning\nProfiling your memory\nHow to measure memory usage from your model without running it?\nNeurIPS Large Language Model Efficiency Challenge: 1 LLM + 1GPU + 1Day\nQLoRA implementation code",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Slaying OOMs with PyTorch FSDP and torchao"
    ]
  },
  {
    "objectID": "education/fine_tuning/slaying_ooms.html#full-transcript",
    "href": "education/fine_tuning/slaying_ooms.html#full-transcript",
    "title": "Slaying OOMs with PyTorch FSDP and torchao",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Mark: Hi, everyone. We’re here to talk about slaying OOMs. OOMs are maybe the notoriously and one of the most annoying bugs to deal with in PyTorch code. Both Jane and I are developers on PyTorch Core at Meta. We wanted to talk to you about a lot of the tools we’ve been building to make this process a bit easier to deal with. So traditionally, the way I’ve seen people deal with OOMs is this way, which is basically people see an OOM, they’re like, OK, crap, what do I do? [0:30] Mark: And then the sort of two nuclear options are you either reduce your batch size or you reduce the size of your model. You go, oh, this thing’s too big. Let me just have something smaller. But this is a very coarse tool. And there’s certainly a lot more finer-grained things you could do with a bit more knowledge. So The first constraint you have is essentially like how much VRAM you have on your chip. So for example, for 3090s and 4090s, which are very popular consumer cards, you have about 24 gigs. [0:59] Mark: And then for the A100s, you have like either 40 or 80. I think the H100 is like about 100, if I recall correctly. But my point is, is that like you’re always going to be VRAM starved. And specifically for consumer cards, you know, if I were to speculate, like I would speculate that the 5090 probably also has like around 24 gigs of VRAM. And so we’re always going to be in the VRAM-constrained environment. [1:24] Mark: But again, as we’re thinking about memory for a model, instead of just thinking about, oh, it’s like something is blah gigabytes, let’s be a bit more concrete about what’s involved in estimating the size of a model. So you have like three core buckets. So for example, let’s say you say, oh, I’m downloading Lama 7b. 7b is referring, 7b is the number of parameters. And if each of those parameters is an FB16. then you need two bytes per parameter, which means that the total size of the model is about like 14 gigs. [1:55] Mark: And indeed, if you download like Lama 7B on your desk, like that’s roughly how big it’s going to be. But that’s not enough. Like, but if that was enough, you know, you could just like run Lama 7B on a 3090, but you can’t. And how come, right? So the reason why is like, well, like if you’re doing fine tuning or training, you also need to basically per parameter, you have gradients and your gradients will also be in FP16. And so… you basically end up with another 14 gigs. [2:22] Mark: And then finally, like a detail everyone seems to forget all the time is also the size of your optimizer. So the single most popular optimizer used in the world is Atom. And Atom is like the amount of memory that Atom takes is twice the amount of parameters. So basically, if your parameters are 14 gigs, Atom takes 28. So if you sum all of these up, you get like 14 plus 14 plus 28, which is 56 gigs, which is bigger than 40 gigs, so bigger than most GPUs that people have. [2:46] Mark: So this is sort of the traditional, what people would refer to as a full fine tune. I also sort of neglected to mention activations. So activations are basically the inter… Let’s say you’re running… For example, you have your weights. It takes times a certain input, so WX. The output of that is your activations. Activations tend to dominate the VRAM of your model. Larger batch sizes and context length. That’s why optimizations like Flash Attention are really important. I’m not going to talk too much about activations like Bert in the beginning. [3:21] Mark: But they’re a bit harder to estimate. They don’t have as clean of a formula as the gradient sun optimizers do. But there are tools that can help you estimate it, which are a bit better than just doing math that I found can be a bit finicky and error-prone. Anyway, so the first thing you might think, like, looking at this picture, you’re like, okay, why the heck does Adam take this much memory? Like I’m going to instead use another optimizer, maybe like SGD, which has no memory overhead. Sorry, can I ask a question real quick? [3:49] Mark: Is there a rule of thumb about how much, like, you know, for the activations that you kind of try to give yourself headroom for? Um, I usually always estimate it. Like, Jane, have you found a good heuristic for it? [4:07] Jane: Well, for activations, it usually corresponds to your batch size. Estimating it is doable for things like Lama or transformers, where you could literally sit down, do some math, and figure out the sizes of everything. But otherwise, for other models, there’s no straight-up formula like, oh, Adam is 2x per rams, that type of stuff. [4:28] Mark: Yeah, so for what it’s worth, though, the reason I showed this picture is if you sort of, as you slowly, as the batch size gets to about 1,000 or the context length gets to about 3,000 or so, 99% of the memory overhead of your model is going to be activations. So I just think of it more mentally as if I go to the large batch size and context length, this is the bottleneck. Otherwise, it’s not. [4:52] Mark: So again, like back to Adam, like you might think like, Hey, like there’s, there’s always a new optimizer that comes out and people say, Oh, like there’s this new fancy optimizer. It’s more memory efficient. The problem with a lot of that work is that like, Atom is basically so used and so popular because it works. And there’s like tons of, there’s like, there’s a close to a decade worth of like papers and people show anecdotal results showing that it’s like a great optimizer. [5:19] Mark: And that hasn’t been true for a lot of like newer optimizers that have been introduced. So conceptually, you might think this is the bottleneck, but it ends up being like a very poor first thing to try to make, like replacing Atom is very, very challenging as a first step. OK, so let’s instead take a look at the parameters. So like we said, basically, at Lama7b, we have the parameters, and we have 14 gigs at FP16. And you might have heard of 4-bit quantization. [5:44] Mark: And so what we’re going to do is we’re going to basically take every one of those weights and turn them into int4. For an int4, it’s actually a sub-byte D-type. So basically, every int4 is half a byte. And so roughly, you get a model that’s like about 3 and 1 half gigs this way. So yeah, great. So we found basically a good approach to dealing with the parameters. Conceptually, the way this works, it’s not like a two call. [6:12] Mark: The way a lot of quantization kernels look like, basically, if you wanted to, for example, cast FP32 to an int8, generally the formulas look very similar to this, which is you basically go over all of the elements, every element of your vector or your tensor, you find what the maximum value is, and then you use that to figure out how to correctly scale the values in the new int8 domain. So the formulas for this ends up looking very much like this. [6:40] Mark: And basically, quantizing is going from Fp32 to int8, and then dequantizing is going from int8 back to Fp32. So this terminology is very common in the community. Great. So you might think now, well, how do we make sure that this process is fast? And historically, you would rely on people writing custom CUDA kernels for this. But those are hard to write. [7:02] Mark: And so as a good workaround, like what I’ve been using a lot has been Torch compile in this case, where I just made a couple of simple changes to this code, where I decorated the functions with Torch compile. And then I also added this environment. Okay, I see a question by Roy. You have to unmute yourself. All right. Maybe I’ll keep going then. So what we’ve done here is we’ve just decorated these functions with Torch compile. And there’s also this very handy environment variable called Torch logs output code that I use all the time. [7:43] Mark: And if you enable this, you can actually code generate an efficient Trident kernel that basically, so in this case, this is how the quantization code would work. So the point is that you don’t really need to learn How to write CUDA or Trident kernels in this case. So you can use it as a starting point and get efficient kernels out of it. So this doesn’t require a lot of domain expertise. See, Andres also has a question. All right, people, can they unmute? Maybe that’s the problem. [8:16] Jane: There’s also questions in the Q&A. I can read some of them if you want to address them now, Mark. [8:21] Mark: Yeah, let’s do that. Thank you. [8:22] Jane: Okay, the first one says, how do these calculations for memory also apply to machines with multiple GPUs to get to 24 gigabytes, like two 12-gigabyte cards? [8:31] Mark: 212. I see. So regarding for like if you hypothetically, you know, if NVIDIA released like a hypothetical GPU with like 200 gigs of VRAM, like the same sort of calculations would help. But like for multiple GPUs, I think like Jane’s going to be talking a lot more about how this works with like sharding, model sharding. So I’m not going to be covering that right now. Oh, this is going to come in a few minutes. Cool. Cool. So let’s go back to gradients. So remember we had the parameters, then we have the gradients. [9:01] Mark: The gradients is you have like, it’s again, like you need a gradient per model parameter. So let’s say we quantize the gradients to four bits. Like, would this work? And the answer is it simply does not anecdotally. Like, your model will not converge because there’s just, like, not information in the backwards pass. So this is just, like, scientifically, if you were to just run convergence studies, you wouldn’t get anywhere doing this. So that’s how we know this is not, like, very fruitful. Okay. But there’s other tricks. [9:29] Mark: Like, basically, the main trick to make this work is LoRa. And what LoRa is, is basically, I think a lot of people might have already talked about LoRa here. But the core idea is that instead of updating the weights for all of your parameters, you pick a subset of the parameters for which you update the weights. And we call these like adapters. And so the idea now is that instead of basically quantizing the gradients directly, we make it so that it’s only a very small percentage of the parameters that actually need gradients. [10:04] Mark: And so this lets us shave off, like, let’s say in the QLORA paper, only like 2% of the total model parameters are trainable and will thus have like gradients and optimizer states associated with them. So basically doing that, plus quantizing your parameters to 4-bit gets you QLORA. So this is exactly what the QLORA algorithm is at a high level. So great. So we got this working. Anecdotally also, because a lot of this stuff is scientific in the sense that we know that Qlora works because everyone uses it and says it works. [10:38] Mark: And last year I helped host a NeurIPS competition about fine tuning. There was no entry that did not use Qlora. It was by far 99% of the meta and all of the winners used Qlora. However, implementing Qlora is kind of tricky. Basically Qlora was mostly implemented by Tim Detmers. If you look at the file, it’s about 4,000 lines of CUDA code. Some people here may know me by the Discord group CUDA mode. [11:06] Mark: The way this term came about was Tim Detmers was describing his process for writing CUDA kernels and it was basically he sits down in a room, no music, no lights, nothing, and he just wrote the kernels in a single night. I personally do not have the ability to write such kernels in a single night. So for me, what has been much more accessible is basically writing those kernels in pure PyTorch and compiling them. [11:29] Mark: And one of the reasons why this file is very long, by the way, is I gave you this nice picture of Qlora, but the algorithm has a lot more details. Basically the weights aren’t in int4, they’re in a format called NF4, which basically mimics the normal distribution of it better. You also can’t just matrix multiply and have four tensors, so you need to upcast them to be F16 and then do a MathML. Remember when I said that it’s very important to figure out what the max is in the original tensor? [11:55] Mark: Well, this makes you very sensitive to outliers, and that’s why people do it in blocks. Then Qlora also has basically scales per block, but then it also quantizes the scales in what they call double quantization. And so it’s just like a lot. Like basically it’s just a lot of math and that you need to write to be productive. And the alternative we have now at this point is like basically this kernel was just written by Driscusus, also on the PyTorch team. [12:21] Mark: And what he did was in basically about 900 lines of Python code, got the NF4 tensor from Qlora working for FSTP without writing any custom code. So this is all pure Python. So you can see, for example, here where it says double quantization, the full algorithm is, okay, well, we have these NF4 tensors. Then we’re doing the double quantization, and we’re doing a normalization, and then we return. And this is all Pythonic code, so you can add breakpoints, you can read it, you can understand it. [12:51] Mark: So again, this is not a great intro quantization algorithm to understand. The core idea is really covered here. But if you understand this, you’re well on your way to understanding more complex kernels. So make sure to just go poke around at that when you have some free time. So the other thing, though, is that within PyTorch itself, PyTorch does not have a concept of an NF4 tensor. PyTorch goes down to int8, it has FP16, and it recently has FP8, and that’s it. But we’ve seen a lot of people experiment with lower D-type kernels. [13:27] Mark: They’re just not doing it with PyTorch. Today they have actually a way of creating like native data types. And so this is using some modern machine machinery called tensor subclasses, which is probably the feature that PyTorch devs are the most excited about internally. And what this does is you can basically override PyTorch ops such that, for example, with NF4, the way you do a matrix multiplication over an input is you cast the weight from basically an a fourth or int four to the basically the weight of the input, which is an FP 16. [13:58] Mark: And then you do like a matrix multiplication. You can customize all of this logic using like operations using tensor subclasses in this way. And most notably, you can also define like what the semantics should be for how this data type should be distributed over multiple devices. So if you’re getting QLora not composing with any sort of PyTorch subsystem, generally subclasses is one way of modifying the behavior of PyTorch purely in Python without being a developer on the PyTorch team. [14:28] Mark: So yeah, I know this is a lot, but yeah, I guess now I’m going to hand it off to Jane, who’s going to talk a lot more about how we got this working over multiple devices. So let me, I’ll stop sharing my screen. [14:38] Jane: Cool. There’s also a question in the Q&A you can answer in the meantime about fine-tuning full parameter Lama 3 8B on 24 gigabytes. So the question is, so no way to do that, basically? [14:51] Mark: So there’s no way to fine-tune, like, yeah, floating point Lama 3 8B on 24. Yes, that’s going to be very challenging. Fundamentally, you would need a smaller model, and it’s kind of why QLore has become such a dominant algorithm to work with this size. [15:05] Jane: Hi, I’m sharing my screen. Well, I’m trying to share my screen. There it is. There it is. Okay, can people see it? [15:16] Mark: Not yet. [15:17] Jane: How about now? [15:18] Mark: There we go. There we go. [15:20] Jane: Okay. So following up with your questions, like, oh, dang, if we only have one GPU, it might not be enough to fit all the things we want to do. So on the left here, I’ve did a little illustration of what memory looks like when you’re training. So it goes left to right. So in your forward, as you’re training, you gain activations that you’re saving for the backward and the backward, you start using them. So they go down. But then in the backwards, you’re building up your gradients for your parameters. So. the grads also goes up. [15:48] Jane: And at one point, the activations are all gone and the grads need to be there for the optimizer step. So I note that optimizer step, the state is an atom W state and it is about 2X bigger than the params. I like measured it. So that’s the left side. And just huge disclaimer, there’s more in your model. When you’re doing math, you sometimes need scratch space. So there are intermediate tensors that are not in this illustration, but they will not matter as much. Okay. [16:19] Jane: If people have questions on that already, please ask now because you will be seeing this little boy a lot. No questions? Great. Okay. So let’s say it’s too tall. At the peak here, you see it’s just taller than your GPU. And GPU is sad, but it’s okay. GPU has a twin. So now the question is to you. What happens? What would you do if you had two GPUs? How would you fit it within that? Like, it’s clearly dividable. So what’s the way to do it? Do people have answers? Has anyone commented on the Discord? [16:53] Jane: I can’t see the Discord right now. [16:57] Mark: No answer in the Discord yet. [16:59] Jane: Wow. Okay. I will answer my own question. So, oh, I guess it wasn’t so obvious after all, but we’ll start with just parallelizing your data. So as mentioned earlier, parallelizing your data will cut down on the batch size, which contributes to this activation memory. So like your params are the same, everything else is the same because they relate to the params like Mark said. But when you slash your activations in half, you get that peak to be lower on each device. Note that everything else is replicated. But let’s make this harder. [17:29] Jane: Let’s say you have less memory than that. And even after doing that data parallelism, it was still oohing. It’s still out of memory. What else can you do? And here you can get you can go do what Mark was mentioning, but not with quantization. You can shard your parameters in half. You can be like, I want half of my params to live on the first one and half to live on the other. And since the Corad’s and Adam W’s state correspond with the params, each of them also become halved. [17:59] Jane: So now you’re like, wow, I’m doing great. This is awesome. I now can go on with my life. But note that this is not the entire picture. Because there’s some complications when you do sharding across anything. Because when you shard anything, you kind of, at some point, you’re going to need to bring them back. So imagine you’re doing your model training at this current moment. You’re running on GPU zero. You’re doing your first linear and you’re like, oh crap, I only have half of my parameters. [18:27] Jane: The other half of me is in that GPU over there. What am I going to do? Well, what you’re going to do is you’re going to be like, yo. Other GPU, we got to talk, we got to exchange some parameters and you will do that. And so what that really looks like is for every step, every layer you run through. you’re going to need a little more memory that’s just representing the layer that’s currently getting processed. And FSDP will do this in a way. So, yeah, this technique, it’s called fully sharded data parallel. [18:58] Jane: You don’t need to remember that. It’s okay. We’re talking about FSDP the whole time anyway. But, like, it will save, it will bring in the memory you need to do, like, a linear, like a matmul. And once it’s done, it’ll be like, oh, I don’t want this anymore. And then it will put that back and drop it. And it will keep doing that to make sure that you don’t peek too much. [19:17] Mark: All right. [19:19] Jane: But you’re like, how do we know that it’s small? Well, you don’t. Well, you do. Because you’re the user. And you get to determine what gets like what gets considered as a layer in FSDP. So this tree thing might look a little scary, but this is Lama2. Lama2 is a transformer decoder that kind of branches into a bunch of things, including 32 transformer decoder layers. And then those branch into a bunch of things, including attention. [19:47] Jane: And then if you do LoRa, then your LoRa has linears and it just keeps going, dot, dot, dot, dot, dot, dot. But it’s a big tree. And how FSDP wraps things determines what gets brought in when you need something. So if that’s a little confusing, it’s okay. But you can think of each of these blobs, like this green blob is one layer, this big… [20:08] Jane: blue blob is another layer, but FSDP would wrap, in this case, if you’re specifying linear, then it will be like, okay, I found a linear, that’s my one layer, and then you’ll find another linear, and that’s one layer, and it will kind of iterate from bottom up to do that, and because So in this specific wrapping policy, the linears are wrapped and the transformer decoder layers are wrapped. And then everything else gets wrapped. So each blob is its own layer. And if you’re like, Jane, why are you telling me about this? This is so confusing. [20:40] Jane: I am now lost. Don’t worry. So the big key point here is that the blobs correspond to how big this little orange dotted box is going to be. So the bigger the blobs, the more memory you’re going to need to bring in at a time. So the bigger that box is going to be. So the way you wrap can really influence the memory you use. Okay, pausing here. Questions, comments? Okay. Next. [21:11] Mark: We’re actually getting a question around MOEs, but yeah, it’s like, how does model tensor parallelism work for MOEs? [21:19] Jane: You are getting very ahead. But yeah, so you’re right, there’s a lot more ways to parallelize, but today we’re going to focus on FSDP. And the cool thing about FSDP2, which we’re going to introduce, is that it will handle that tensor parallelism more easily than today’s FSDP. We’ll get into that soon. Okay. So let’s say after you do all of that, you still, like, what can you do now? What else is there? What is left? And that’s where CPU offloading comes in. [21:51] Jane: And it’s nice because CPU is like your little brother on the side who’s, like, not doing anything as you’re, like, trying to beef up stuff. But it can hold things. So you can make it hold your parameters as you are iterating. So in this case, you’re just like, let’s just put the parameters in CPU, and when we need it, we will move what we need to GPU, do the all gather, do that sharing of knowledge, sharing of data and parameters, and then move on with our merry lives. [22:17] Jane: And with that, with CPU offloading, you get your memory to be much, much smaller because the biggest chunks here are now living in CPU. So note that we really want the GPU. Like GPUs are accelerators. They’re meant to do like beefy work. And your forward and backward are the beefy work in a model usually. So for the optimizer step, people are like, it’s fine. We don’t need the GPU for that. [22:43] Jane: And in this case, we’ll only bring in the parameters for forward backward and be like, OK, we can put that on CPU now, which means the optimizer step runs on CPU. And you also save a lot of space on your GPU if you host your whole atom state there. Ah, so there’s that. Okay, so you might be wondering, Jane, FSDP has been published for like a long time now. Why are you explaining this to us? What is the point? People use it. In fact, that’s true. [23:13] Jane: People like answer.ai who are wonderful, they already built out an integration for FSDP and bits and bytes params for bit to make Qlora happen. But it’s kind of annoying to work with FSDP1. They had to do a lot. And we came out with per-parameter FSDP, which I will also call FSDP2 for later. And what is that? So let’s start with the status quo. Like what is it today? Let’s say you, just for our toy example, you have three tensors that you need to distribute across your two GPUs. And they are these shapes. [23:49] Jane: So the goal is that you want to make that exchange of, you know, when you’re talking to the other GPU, that thing efficient. And nickel, which is the software and driver stuff that does it, it requires that each GPU will give the same tensor size. So those are, that’s the constraint. What does FSDP1 do today? Okay, what it does is it flattens all your tensors. And this is what they look like in memory. So flattening is actually pretty chill. And then it’s going to line them up in a line. [24:20] Jane: And then it’s just going to slice it in the middle. And if it’s not even, it will add a padding at the end. And then, because now it’s just arbitrarily sliced in the middle, it will be like, alright, this half goes to 1, this half goes to 0, oh, I guess 0 and 1. And so you’ll end up with something like this, where tensor 1 and I guess a little more than a third of T2 will live on GPU 0, and then the other half of this, including the padding, will live on GPU 1. [24:51] Jane: And this is nice, but note that the way this is implemented today is that T1 and half of T2 is going to get smooshed into one tensor, which is a big con. We’ll get into that later. And same thing with T2, T3, and the padding. That gets moved into one tensor. And we call that tensor a flat parameter because it’s so flat. [25:15] Jane: And some reasons why you might already be thinking, hmm, this might not be a good idea after all, is the fact that this forces T1 and T2 and T3 to all have the same D type, to have the same requires gradness, and all the other metadata you might want your tensors to have. Okay. Well, what if we tried down a different path of dividing things in two? So what we’re going to do is we’re going to slice every tensor first. We’re going to cut T1 in half, T2 in half, and T3 in half. Great. [25:46] Jane: Except we notice that T2 needs some padding because it’s 3 by 3. You can’t really cut that in half. We’ll do it. It’s fine. We’ll shard. And that way, what this will look like is every tensor gets its own representation on this GPU. And this is great. The main pro is that they keep their identity. Like if T1 was UN8 and T2 were BF16, totally fine. They can stay that way. But in the previous case, in FSDP1, you wouldn’t even be able to put them together. [26:18] Jane: That’s just like, or you’d have to hack around it a lot. And this gets into the QLORA stuff soon. Very soon as well. Okay. There is a con to this. Because of all the slicing and rearranging you have to do, there are extra copies to FSDB2. So that’s a con, but the pros it gives are so much more worth it. And just a recap of before, in more clear terms, like a flat parameter would force all three of these tensors to share all the metadata they have that a tensor can have. [26:51] Jane: And in FSDB2, because they are individual separate tensors, we call them detensors because they’re not like, you know, the full tensor, they’re a distributed tensor, they’re smaller. They can be themselves, they get to keep their identities, they can have their own D type, their own requires grad. And so you’re like, okay, but why? So if you think of quantization, which Mark talked about earlier, what if you wanted T1 to be UN8, T2 to be full size, like FP32, any other size? In the first regime, unacceptable. The second regime, totally fine. [27:27] Jane: No one is going to bother you. FSTP will do that. Another thing that is very popular nowadays that LoRa is, is you don’t really want to train the big stuff because that will require big grads, big optimizer step. So what if T2 is frozen, you don’t want it to actually train, and T3 is the LoRa adapter that you do want to train? [27:50] Jane: In that case, In your first world, you still can’t have that because a tensor can only have one requiresGrad, and the flat parameter here will force you to either make it requiresGrad or not requiresGrad, or to force you to do a lot of fancy hacking to make it work. All of these things, all of these concepts that are like, oh, I wish I had this. In FSDP 1 today, it would be difficult. But in FSDP 2, it’s for free. [28:16] Jane: And another thing that’s really cool about Fsdp2 that is its own other lecture entirely is memory determinism. So one of the major implementation changes is that now Fsdp2 actually guarantees that you will have only that small little sliver of memory before, like this little orange thing. Whereas Fsdp1 actually didn’t do it well enough and could cause memory spikes that are not deterministic. But yeah, for this one, you should read the blog links here if you want more details. Okay. [28:53] Jane: So now that we have Fsdp2 and we’re like, this should be easier to use, let’s do it. Let’s use it. And Wei did do that. We did do that. So Wei, who’s another dude on the team, he wrote this PR here that puts together Fsdp2 and NF4. And it works. It works. It’s great. We know like, okay, like FSTP2 is cleaner, it’s more composable. But the last question remains of like, can this actually replace FSTP1? [29:22] Jane: Like we would love to use it, but can you tell us that it is good on perf, that we won’t be slower than before. And so that’s what the next few slides are going to be. All right, pausing here to see if people have questions, thoughts. If not, we’re going to go with the plan. All right. So here’s the plan. The plan is I’m going to go get some GPUs. We’re going to run some benchmarks. And then we’re going to make sure those are the same benchmark. [29:50] Jane: And then once they are, we’re going to record some gaps and figure out what the gaps are and if we could make them faster. All right. So getting some GPUs, this is the easiest part of the journey. You just go to Vast AI and then you ask for it. Well, first you need to have money and then you go and you’re like, give me two 3090s or 4090s. And I got to, they are 24 gigabytes each for VRAM. There are some other details here if you care, but they’re not super relevant this time. [30:19] Jane: Just know that I have two, I have consumer hardware, and they are 24 each. So I ran a bunch of benchmarks on answer.ai’s train.py, which is our baseline, like FSDP1 and BNB. That’s our baseline. And I’m using… the batch size 8 as a baseline, and just so that it works. If you’re curious, if you wanted to run this for yourself, the command is right here. Feel free to copy paste that in the future, but you could just pay attention now. I ran the same thing in the TorchTune recipe. [30:56] Jane: One difference in TorchTune and train.py is that it uses a YAML for the config versus the command line. It’s just different. And I did have to tweak the YAML quite a bit to make sure that I was running the same config. And since And then these were my results. So peak memory wise, we were doing we were doing better. And for runtime, though, we were like 19% slower. [31:21] Jane: So someone here might be like, FSDP2, we know it’s stricter about memory, we know it requires extra copies, that makes sense that we’re better at memory and worse at runtime, right? But no, no, no, we got to be diligent. And very quickly, if you look at the traces, it reveals some troubling shenanigans. So, here’s the two traces. On the top is the baseline. On the bottom is our new trace. Can you spot the difference? Okay. There’s a lot of differences. So, I’ll just go with my favorite one. [31:54] Jane: I work on optimizers and those little light blue things are optimizer steps. And immediately I was like, dude, the optimizer is taking so much longer. What could that be? And so, this is where I get into the actual tracing stuff. I wonder if I can actually show you the traces. That would be pretty cool. Okay. I’m going to stop sharing to reshare and then we can do that. Let’s just share my entire screen. Okay. Do people see traces? Yep. Okay. [32:33] Mark: Awesome. So, [32:34] Jane: I’m going to go ahead and share my screen. So, on the left is our baseline, on the right is the slow boy. So in our baseline, we’re going to go to the second step because the first step is always full of like warm up stuff and initiating stuff. So we’re just going to ignore that and we’re going to go here because every other step after this is much more normalized. And something you can do, I’m using Perfetto. I don’t know if people are familiar with Perfetto already, but it’s been pretty helpful. Yeah. Okay. [33:12] Jane: And something that is super useful and nice in Profetto is you can highlight a region and it will give you profile or profile or profile or while I cannot talk today results on here. So here you can see that there are a bunch of it tells you what thing takes the longest. It’s like the moles take 77 milliseconds and there are 70 768 of them. And on this side, when we do that. we’re going to notice some very different numbers. So here, the mole also takes the longest, but there’s 1,700 of them compared to 700. [33:48] Jane: And you might be like, what is going on here? But let’s go look at the smallest number. In optimizers, there’s only one divide. You can just take my word for that. So here we know that there are 384 divs, which means that there are 384 parameters. Here, we see that there are 800. 896, which is like more than double. And so let’s go find a div. Like, where are they? And here you can just like do do do. But you can already notice that everything kind of gets doubled here. Whereas in here. [34:25] Jane: they are just called immediately. So like this A10 mole goes directly to A10-2. This A10 mole though goes to A10 mole again. And you’re like, wait, what is going on? Why is that? And this is where you learn the painful things of tensor subclass dispatch. So since we’re using detensors, it means that it goes into this mole as a detensor. And then detensor is like, all right, let me do my metadata unwrapping. And now you get to go to A10 mole as a… playing tensor now. So there are double the amounts. [34:58] Jane: And just to spare you some math, um, spare you some math, it turns out that if we divide this mole by two or the div by two or any of the things that run once, it shows that we actually are running training on more parameters than we thought. So in the left side, we’re only running it for 384, and the right side, we’re running 64 more parameters. Like, can people guess where this came from? I will show you what war story of debugging has shown me. [35:37] Jane: In the end, I realized that this was a config difference, where if you are fast at reading, you might have already spotted the difference here. The answer is that, in train.py, they do not glorify the output projection, whereas in our world, we do do that. And since glorifying means you add two adapters and there are 32 layers, 32 times 2 is 64 more extra parameters to train. So yeah, that was bad. And that was a great lesson because it was like, I was not measuring apple to apples. [36:12] Jane: And I needed to do some other things to make sure that we were. So the first thing is like making sure the data set was the same, making sure that every parameter was the same, changing the wrapping policy to be the same. And after I did all of that, I ran another benchmark. And here, I mean, that was like also weeks of work, by the way, like, like, it was not easy to figure out every little difference and why they were different. [36:36] Jane: But after all of that, I was like, let me run it again, maybe it will be better. But no, it was still slow. It was actually slower than before. But the peak memory was a lot better. So however, I was still happy, because even though it may feel like we took a step back, we actually made a giant leap forward. on blocking the first real step, which is like now that we have apples to apples, there are things that will match. And then we should just look at things that are different. [37:02] Jane: And so that’s where I could start playing my very long game of spot the difference. The first thing I did, though, was like measure their runtime. And here you can see that the forward, backward and optimizer were like the culprits. And that’s how I knew what to focus on. OK, so this is a big slide. I, okay. If at this point you have not looked at the traces yet, but you would like to, I sent a link to the Google Drive in the Discord, and there are the traces that you want. [37:38] Jane: There’s like, there are four of them, and the two you care about to look at now are the ones that don’t start with. bad or final, the other two. Do the Answer AI one and the TorchTune one. But I’m going to, I already found these gaps for you, and it’d be fun, if you find it fun, if you want to learn more about traces, it’d be fun if you could find each of these yourself, like where they are located, and do the same exploration I did. [38:02] Jane: But because this is a presentation and I do want to save time, we’re going to dive right into what these gaps were and where, like, how we ended up fighting them. So the very first gap is, yeah, so we’re going to go. And this is the link, by the way. Okay, the very first gap is the optimizer step was still slower. But remember how earlier I hinted that there was all this overhead here for detensor? And the way it works is because of C++ dispatch, it just takes a long time to do that kernel call. [38:34] Jane: And because there are so many calls, all of this overhead adds up to make the optimizer step three times slower. And also another minor detail, if you look at this, this is 64 microseconds and this is 81. The answer for that is because the parameter is not contiguous, but that’s more minor. So the solution here is actually really chill. We worked with Intel, we were like, hey, what if we just had a fused atom? Like your optimizer step, but all happening in one kernel, so you can dispatch just once and get results. [39:08] Jane: So this avoids all that overhead because there’s just one kernel now versus the other kernel. 384 times however many ops there are. And it also leverages vectorization. So we go from about one second to 120 milliseconds, which is like an 8x speedup. So that’s one gap that is now gone. All right. Pausing here to see if people have questions. [39:38] Mark: I’ve been speed answering everyone on Discord. [39:41] Jane: Okay, nice. Okay. I was like, are they lost? But no. Okay. [39:46] Mark: They’re very happy, actually. People are saying they love this kind of debugging. And yeah, people love it. [39:51] Jane: Okay. Well, let’s keep going. So the next three are a little boring, but we’re gonna go through. And there was a lot of pain in this one, actually. This second gap was crazy. So I went and the way I literally did this was the most brute force way you can imagine. Like you open your trace, you find the first forward, you find the first GPU kernel, and you’re just like, do they match in size, shape, input, everything? And I would do that for you. [40:18] Jane: But we are a little short on time, so I’m going to just show you what the difference is here. And the first difference that was major was that the second all-gather in every linear, like every layer, was somehow five milliseconds longer. And that was when I needed to click on them and figure out how big they were. on the left side for train.py, there were just fewer things getting all gathered. Like it was just not the same thing. And I was like, why is it not the same thing? [40:51] Jane: So I did a lot of printing and I hacked around FSDB2. And what that yielded was me writing down the sizes of every tensor that got packed up in the all gather and realizing that the difference was because in our NF4 metadata, where… Answer.ai did not pack their scalars and quantization factor. They just like for bits and bytes when they did their four bit thing, they use a dictionary. They have their own method. [41:19] Jane: And this is actually the reason they couldn’t pack it is because FSDP1 had restrictions, by the way, like it just wouldn’t do it for them. So they needed to work around that. So that was one piece of the puzzle. where we just packed everything in one go. But in the other bigger piece of the puzzle, the big, big difference, like that was just like 1 million bytes, whatever. It doesn’t matter. But the other thing was like so many more. It was like 12 million bytes. [41:45] Jane: And that was when we realized that when we opted out of LoRa, the output projection did not get quantized in our world. So it was like four times the size of Answer.ai’s version. And I was like, Why don’t we do that? And then I talked to the TorchTune team and they’re like, oh yeah, we should do that. And so we should do that. That’s the conclusion. We should do that. The first one is intended behavior. So we don’t really need to worry about that. But the second one, we should do it. [42:13] Jane: And we will hit you up when that happens. So this gap I would mark as yellow. Like we know what to do. It’s solved. Okay, third gap is realizing that there were just like more overhead. And remember what Mark was saying how when you have NF4, you have to you can’t just like put that through your gem kernel like CUDA is going to complain, you need to de quantize get it to the BF 16. And then put it through the CUDA kernel. [42:39] Jane: And it turns out that Tim Detmers, you know, brilliant guy already wrote a really fast version of that, whereas we just get stuck with the little raggedy version that tries every op. So that’s also where we are slower just because of additional overhead. But again, this is not a major problem. Solutions. We could use Torch Compile. I tried it. It was not trivial, so I was like, I will look into this later when I have a month of time. And then, or when I don’t have a presentation on Monday. [43:05] Jane: And then the second step is to just use Triton kernels. So Driss, our coworker, already wrote them. I didn’t want to copy paste them for the sake of this presentation. But if you want to copy paste them, go for it. No one’s judging. They’re good. They work. And so we’re like, okay, we also know how to fix this one. The third one was really stupid. This one is definitely the worst gap, where basically there were just very different ops happening before the SDPA. And this was just because we used two different ropes. algorithms. [43:41] Jane: TorchTune was like, we are traditional. We are going to use the original meta algorithm. We work there. So there will be no numerical differences. And everybody else in the world is like, oh, but we want it to be faster. So it’s fine. And then the real solution here is just for TorchTune to offer more of these. And that’s also in progress. So, yeah, but okay. Let’s talk about the most fun one. The most fun gap I noticed is I realize this is a little hard to read. [44:07] Jane: But on the left side, note that there are two things happening. The first, this green thing here, is the CPU offload where you go from CPU to GPU. And the second one is when you after you’ve moved it to the GPUs, you like have them talk to each other. And here in train.py, in FSTP1, we’re like, wait, how come this is overlapped? Like, you see how this is, there’s no gap here. But this one is so badly overlapped. Look at that. Look at that exposition. It’s bad. [44:36] Jane: And this was really frustrating because this was costing us like 10 milliseconds each step. And I was like, I wonder why this is. But actually, this is very much expected behavior. And this is part of the bigger memory constraints that FSTP2 promises you. So, FSTP2 is promising that, hey, we’re not only looking at all gathers, we’re also going to make sure that before we bring CPUs to GPUs that you have the space you have. So it is guaranteeing the constraint that only two layers of parameters will be allowed at a time. [45:12] Jane: And that is why on the left, because of how FSTP1 was implemented, it didn’t do that very well. So you’d get random memory spikes. And FSDP2, you’re promised to never get that. But someone looking at this trace will be like, but this is kind of, what if I’m okay with non-deterministic memory? Like it’s not happening now. Like maybe I can just go on with my life. But no, no, no, we have an answer for you. And the answer is the reason it’s so exposed is not because FSDP is too strict. That’s not the problem. [45:41] Jane: The problem is that the overlap of computation and communication was too different. The computation here is super duper tiny because it corresponds to this tiny linear here. And then here when you’re CPU offloading, you’re actually trying to bring this big boy back in. So it’s like the mismatch in the layer sizes was really causing this problem. So what could we do? Well, it’s fine. We’ll just change the wrapping policy, have bigger layers. And it’s really just, hey, don’t wrap those linears by themselves. [46:16] Jane: Just group them in so we can just have every transformer decoder layer be its own layer. And note that this is only possible with FSDP2. You can’t have the right-hand side in FSDP1. Why? Because the lower linears have the base weights. The base weights are quantized. They’re NF4 tensors. They’re going to be small. They’re UN8. Whereas your linear tensors, those are BF16 or whatever training thing you want. And in FSDB1, they can’t be brought together under the same family because they’re going to be forced into one big flat parameter. But in FSDB2, they can coexist. [46:55] Jane: And the change is actually really easy to do. The policy is just a matter of commenting out these two lines here. And once we do that, the solution is like they’re both overlapped. It’s kind of crazy. Look, like there’s no more exposition at all, where even in the first case, even before here in the train.py one, this all gather was exposed, also due to the same reason. That’s not even true at all here. And this wrapping policy, this new one, is only possible because of fsdp2, which is a great point here. All right. [47:32] Jane: So now we’ve fought all our gaps and things work. So it’s your turn. You should try them out. You should really, like, it doesn’t have to be NF4. If you have another favorite lower precision thing, you can try it. If you want to try out more Lama stuff with it, you can. There are now scripts ready to do that. So yeah. One disclaimer, though, we are working on this. This is not a forever thing. Pointing. of just Fsdp and Qlora does not work yet. So that’s just FYI, we’re working on it. [48:06] Jane: Sorry, you can’t do that. But you can try, you can just like try and play with it, among other things. So yeah, here I would love to, I mean, Mark and I are speaking here, but this was really made possible by all these amazing people. So Driss wrote the original NF4 stuff. Andrew is the main dude who designed FSDP2. Wei ended up taking Andrew’s work and making it compose with… Driss’s work, so he like amalgamated both of them. And then Torch Tomb people, so like Rohan and Evan, they were super helpful. [48:40] Jane: They wrote the Laura recipes and they’re the foundation of which we built and showcased our work. And of course, Mark, who’s amazing. So thanks, Mark. [48:52] Mark: Yeah, so I really hope this gives people some more context around what we’re thinking here. We did want to showcase a useful recipe with Qlora and FSTP composition. But really, this is kind of like our call to action here would really be if you’re doing interesting research at the intersection of quantization and distributed, we’d really, really love to hear from you. So if you’re working with more exotic D types or more exotic forms of parallelism, a lot of this work should really, really be helpful. [49:21] Mark: And we have all these links here that can give you some more context. I guess we’ll pause here if people have any questions. Otherwise, we’ll probably be hanging out on Discord for a couple more hours if people have any questions.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Slaying OOMs with PyTorch FSDP and torchao"
    ]
  },
  {
    "objectID": "education/fine_tuning/emmanuel.html#chapters",
    "href": "education/fine_tuning/emmanuel.html#chapters",
    "title": "Why Fine Tuning is Dead",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction and Background\n01:23 Disclaimers and Opinions\n01:53 Main Themes: Trends, Performance, and Difficulty\n02:53 Trends in Machine Learning\n03:16 Evolution of Machine Learning Practices\n06:03 The Rise of Large Language Models (LLMs)\n08:18 Embedding Models and Fine-Tuning\n11:17 Benchmarking Prompts vs. Fine-Tuning\n12:23 Fine-Tuning vs. RAG: A Comparative Analysis\n24:54 Practical Tips: Evaluating Fine-Tuning Needs\n26:24 Adding Knowledge to Models\n30:47 Multilingual Models and Fine-Tuning\n31:53 Code Models and Contextual Knowledge\n34:38 Moving Targets: The Challenge of Fine-Tuning\n38:10 Essential ML Practices: Data and Engineering\n44:43 Trends in Model Prices and Context Sizes\n47:22 Future Prospects of Fine-Tuning\n48:35 Dynamic Few-Shot Examples\n49:49 Conclusion and Final Thoughts",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Why Fine Tuning is Dead"
    ]
  },
  {
    "objectID": "education/fine_tuning/emmanuel.html#slides",
    "href": "education/fine_tuning/emmanuel.html#slides",
    "title": "Why Fine Tuning is Dead",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Why Fine Tuning is Dead"
    ]
  },
  {
    "objectID": "education/fine_tuning/emmanuel.html#resources",
    "href": "education/fine_tuning/emmanuel.html#resources",
    "title": "Why Fine Tuning is Dead",
    "section": "Resources",
    "text": "Resources\n\nAnthropic\nEmmanuel Ameisen:\n\nPersonal site\nBook “Building Machine Learning Powered Applications: Going from Idea to Product”: Amazon link | Alternative link\n\nFine-tuning vs Context-Injection (RAG)\nFine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\nFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Why Fine Tuning is Dead"
    ]
  },
  {
    "objectID": "education/fine_tuning/emmanuel.html#full-transcript",
    "href": "education/fine_tuning/emmanuel.html#full-transcript",
    "title": "Why Fine Tuning is Dead",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Emmanuel: Yeah, fine tuning is dead. Long live fine tuning. The idea of this talk came, I think, mostly from just like some fun tweets. I tend to believe that fine tuning is less important than it once was. And Hamill challenged me to like, actually defend that that take. And so, so that’s what I’ll try to do here. All right, so who am I? So I’m Emmanuel is my name. I’ve been doing ML for, gosh, almost 10 years now. I was doing data science originally, then I worked in ML education, actually. [0:34] Emmanuel: So train models for demand prediction data science helps people train models and ML education. I wrote a practical guide on how to train ML models. Then I worked as a staff engineer at Stripe where I trained more models. And now I work in Anthropic where I fine-tune some models and also currently I’m helping out with efforts to actually understand how these models work. [0:56] Hamel: Very important. And just to plug you a little bit more, because I think you’re a little bit humble. There’s a website, mlpower.com. You can see Emmanuel’s book. It’s a classic in machine learning. And I would say in applied machine learning. So definitely check it out. [1:11] Emmanuel: I appreciate the plug. Yeah, check it out. I one day hope to update it with some LLM specific tips. It’s just general machine learning knowledge for now. And yeah, you can get the first chapter for free on that website. [1:23] Emmanuel: no commitment if you hate it uh this this is like the most important slide of the talk uh this talk is my opinion uh non-anthropics uh mainly i say this because anthropic like among other things offers fine tuning so if they thought my tuning were dead that wouldn’t really make sense and so this is mostly like kind of yeah like hot takes and beliefs based on just seeing the field evolve over my career uh rather than anything that that like uh anthropic really believes so i’ve been training models for 10 years i don’t recommend it [1:53] Emmanuel: This is, I don’t know, this is like really the talk in two slides. I think it kind of sucks for a variety of reasons. And if you’ve talked to enough people that do it a lot, as I’m sure a lot of you do, they’ll tell you all of the horror stories that come with it. I kind of want to talk about three things, though. We’ll talk about the horror stories in the third part, actually, and the difficulty. But one, I wanted to see trends I’ve observed over the past, let’s say, ten years. [2:19] Emmanuel: Then some performance observations on the fine-tuning work that I’ve seen shared or various papers. And then we’ll talk about the difficulty. So first, trends. So I think that like in machine learning, the best way to kind of have a lot of impact is to just be afraid of anything that sounds cool. Like anytime in my career when there’s been anything that sounded like the cool thing to do, it tended to be the case that like if you did that, actually, that was like vaporware and really you should be doing the boring stuff. [2:53] Emmanuel: And so what that means is like, you know, in 2009, maybe like people were like, oh, my God, machine learning is like a big applied thing now. We want to train models. But really, like if you look at like delivering value, really what you need is like data analysts and data scientists that like write good SQL queries. And so that’s what you should spend your time on, even though it sounded less cool to people at the time. In many places, this is still true today. [3:16] Emmanuel: Fast forward to 2012, you know, like the deep learning revolution, maybe it was a bit early, 2012, 2014, let’s say. You know, everybody wanted to use deep learning. It’s like startups that were doing, you know, like, I don’t know, like fraud prediction that were using some random forest or like, ah, surely now we need to use deep learning. Actually like that was too early. At that time, it was very, very hard to get deep learning models to work. You should just use XGBoost. Do the boring thing. [3:42] Emmanuel: 2015, it’s like, ah, now deep learning is in full swing. There’s a bunch of papers. The exponential is starting. Surely what you want to do is invent a new loss function or improve on the theory of an optimizer. But really, if you want to actually make your model better in practice, you just want to clean your data set, notice the obvious errors, fix them, and then that would get about 10 times larger improvement in about a tenth of the effort. [4:09] Emmanuel: And I think in 2023, we have a similar thing in a way with like definitely training your own foundation models. In some cases, I think, and then also just fine tuning. It’s very appealing. It sounds very cool. As far as I can tell, it’s actually rarely the like first thing you should reach for or the most useful thing you should you should go for. So I think just based on priors, we should be suspicious of fine tuning because it’s the cool it sounds like the coolest thing. [4:37] Emmanuel: And that like right away, like, it’s the coolest thing probably be the worst use of my time. I made a little chart that I think illustrates this somewhat. Oh no, hold on, fine tune in on that. Yeah, okay, same thing. We talked about this already. I made a little chart. This is this beautifully drawn chart is like my take on sort of like if you’re doing machine learning in a practical way. So like, what was the way to, you know, best leverage your time? [5:04] Emmanuel: And so at the start, hopefully you can see my mouse, but at the start, you know, people just trained models. There was no fine tuning because there were sort of like no models to take and to fine tune, or at least it was like exceedingly rare. And so you like, you trained your own like random forest or your own like SVM or your own whatever, like even your like MLP. And that was that. [5:22] Emmanuel: And then kind of with not really when ImageNet came out, but like a little, a few years after with like VGG and then later on ResNet, you know, that’s when sort of like fine tuning came out. I think became a thing that a lot more people started to pay attention to. You could take a pre-trained model on, in these cases, they were image models. You can take a pre-trained model and then like fine tune it on your smaller data set for cheaper and get something that was better than if you trained it from scratch. [5:47] Emmanuel: And so, you know, as time went on, that became more useful. I think BERT was also a big moment where that started becoming useful for text as well, where you could fine tune BERT models or fine tune other models. And so I would say that there’s this general trend that fewer people are training, more people are fine tuning. And then around GPT-3, maybe a little after, because it took time for people to really pick up, there was this concept of, ah, do you even need to do any backwards pass on your data at all? [6:15] Emmanuel: And it just made me like. take the model and maybe it just works, right? That’s sort of like, I would say that concept is like the original promise of LLMs, right? Is that like, you actually don’t need to train, they learn in context, you can just prompt them. And so I like this chart because it’s sort of like, well, I don’t know if fine tuning is dead. I don’t know if like… [6:34] Emmanuel: it was just a blip or if like this chart will like kind of like go back up in prevalence but at least like the sort of like trends of like ah you know it used to be that really there was nothing else you could do than training and or at some point you could like replace your training with fine tuning and now there’s this whole other categories of applications where actually you don’t need to do any training at all And so the question is like, oh, you know, how do you extrapolate that trend? [6:57] Emmanuel: And I think like the realistic, not fun, not hot take answer is nobody knows. But my hot take is that, you know, line goes up. And so I think we’re going to keep having that orange line increase in the future. Maybe I’ll pause like really briefly in case there’s questions. [7:12] Participant 3: To state the semi-obvious. [7:14] Emmanuel: Yeah. [7:15] Participant 3: If we go back to your slides about like what not to do. [7:18] Emmanuel: Yes. [7:19] Participant 3: Most things that you said not to do. they were the thing to do a few years later. So you’re like, oh, don’t train ML models. And then a few years later, you’re like, yes, you should be using SG Boost. And then you’re saying don’t do deep learning. But then I think the thing after this is like, you’re saying like by 20, at some point later, you say you should do that. [7:37] Participant 3: Does that mean that if you say not to train, that you shouldn’t find you now, that it’s going to be the hot thing and it’s going to be worthwhile in a few years? [7:45] Emmanuel: I mean, I think like, I don’t know, right? Maybe. I think this is not true for all of them. Like notably, like I think invent a new loss function is still like not a thing that you should do, you know, even like almost 10 years later or something. So I think it depends. I think it’s certainly the case that like, as soon as something comes up, like deep learning, people want to do it. And sometimes it will actually be useful. Sometimes it’ll not be. And it’s hard. [8:11] Emmanuel: when it first comes out to know whether it’ll be like the invent a new loss function category or they use deep learning. [8:18] Participant 3: Let me ask a couple of questions from the chat. [8:20] Emmanuel: Yes. [8:22] Participant 3: We got one from some Simon Willison guy. I get the argument for not fine tuning LMs, but how about embedding models? Is there a relatively easy value to be had from, for example, fine tuning an embedding model on a few thousand blog articles to get better quality semantic search? [8:40] Emmanuel: I think that’s interesting. I… I feel like to me that’s a similar question to fine-tuning a model. I feel like if you buy that these models are getting better and that we’re going to… I think right now we’re focused a lot on improving the LLMs rather than the embedding models. Comparatively, there’s not that much activity in the embedding provider space. But if you buy it, they’re going to be better. I feel like you’ll just have very general embeddings that work well. [9:13] Emmanuel: I think where this gets tricky and where you might always need fine-tuning or need a different paradigm entirely is your company has a product that’s the XS23, and nobody knows about it outside of your company or something, and you want to build search based only on embeddings with this. I think that might require either some fine-tuning or some embedding or some combined. RAG, which honestly is what I’ve seen work really well, where you do a combined sort of some keyword search and some embedding search. [9:49] Hamel: What about the case where, okay, with RAG and retrieval, in the domain-specific case, a lot of times what people think is a good… sort of ranking or retrieval can be very specific. It can be hard to capture that in an embedding, no matter how good the embedding is. So do you think, yeah, that’s the part that I, yeah, that’s the part I wonder about the most. [10:18] Emmanuel: Yeah. I think not to add spoilers or anything, but I think at the end of the talk, I have this light where I think that fine-tuning is dead is like the hot take version. I think that like the realistic like thing that I believe could happen is like fine-tuning is, you know, 5% of the AR versus, versus like 50 or something. And so I think it’s totally possible that you can imagine that like, yeah, like for your very specific search where I think it’s complicated. [10:48] Emmanuel: Cause like, This is kind of getting into what I talk about later, but as these models get better, you can imagine them just being able to, in context, understand what your specific search is. And so you could have your LLM drive your search and do some sort of query expansion just because it really understands well the context. For pure embeddings, I don’t know yet. It’s possible that you always would need to fine tune some embeddings for some retrieval. [11:17] Participant 3: Is there any benchmarks or data comparisons that compare how good your results are from doing a better job of prompting versus fine-tuning? [11:28] Emmanuel: Yeah, there are some. I have some later in the talk. I don’t have, actually, like, I have, like, RAG versus fine-tuning, which is kind of similar. I would imagine, like, prompting to sort of, like, be a little worse maybe than RAG. Actually, that depends. Maybe, like, comparable to RAG. So I have some, some, I looked up some of the papers I could find that I’ll share after in the next section. [11:47] Hamel: Great. Cool. [11:50] Emmanuel: Yeah. So I’ll say that, like, I’m also all ears. If people here have, like, papers that are comparing this performance, I was surprised to not find that much. I’m going to be completely honest. I didn’t spend super long doing literature review, but I spent, like, 15, 30 minutes looking for a bunch of papers I could find on this that I wore in addition to ones that I was already aware of. And I didn’t find that much that I found was super informative. So. This is an example, I think, from the OpenAI forum of fine-tuning GPT-3. [12:23] Emmanuel: That is one of the first examples when you look at fine-tuning versus RAG, at least that I could find. And I think is relatively illustrative of what happens in some cases, not all of them, obviously. But in this one, you have the base model. And this one, the fine-tune, I think is like… I had it at the start because I think it’s like worst-case scenario or something, because it doesn’t seem to be doing any better. And then you have like… context injection, which I think is basically if I remember well, and then various different models. [12:51] Emmanuel: And so it’s the case that sometimes fine tuning doesn’t work. [12:55] Hamel: So I have a question about this that maybe you can help me understand. So I always see these kind of things about fine tuning versus rag. And then I get really confused because My fine tuning always includes RAG. Like, includes examples of RAG. And I’m like, what do you mean fine tuning versus RAG? It’s not like a versus thing. It’s like you do both. [13:18] Emmanuel: Agreed. Agreed. Can I just say, like, I’ll answer this in two slides? [13:23] Hamel: Yeah. [13:24] Emmanuel: Okay. Yeah, yeah. Agreed with you, though. Well, okay. Maybe actually the one thing I’ll answer. So in two slides I have a comparison including fine tuning, RAG, and both. And the other thing I’ll say is like, I think that this is also a matter, like one of the reasons why this is a hot take I have is that I think it’s also a matter of prioritization. [13:41] Emmanuel: Like, you know, if you’re like at this point in your life or something and you have the choice between fine tuning and rag, I think it’s very important to know like, OK, like which one’s gonna be harder and like which one’s gonna give me the biggest lift. And then like, of course, you can always do both. Right. But still, like if there’s if there’s two options, you kind of want to know which one is the most efficient anyways. [14:01] Hamel: Oh, yeah, that makes sense. I definitely agree with that, too. Like you should do rag first. Yeah. [14:07] Emmanuel: Yeah, I feel like this… Yeah, anyways, I would bet that at the end of this talk, we’re like, actually, we’ve run most things. But you gotta have a few optics. This is… Okay, so I think this is… Yeah, exactly. Okay. So this is kind of… I said in two slides, but this is basically what you were asking me about. This was a paper… I link it here. Comparing fine-tuning and RAG. It’s on relatively old models. The paper after has more recent models. But as far as I can tell, that trend actually held. [14:34] Emmanuel: And this is kind of hard to read, but basically you have like this is your baseline. You haven’t done anything. This is you just do RAG. This is you just do fine tuning. And this is you do fine tuning plus RAG. And then I would like just ignore the sort of like, you know, this is like, do you do like some LoRa? Do you like some like full fine tuning? And then I think this is like, I don’t recall. These are like different prompting methodologies. If I remember well. [14:56] Emmanuel: Or yeah, this is the fine-tuning data set that you use. Is it formatted as a question and answer? Anyways, you find that if you look at all of these models, really, the increase comes vast, mostly from RAG. Notably, this is even more true for the larger models, where you get 58 with RAG, and with fine-tuning per-person RAG, you get 61. And with fine-tuning alone, you get way less. This is less true for the small model, right? Where you get quite a bit more with fine-tuning plus RAG. [15:26] Emmanuel: But if you look at that trend, especially for base and large, basically, you’ve gotten almost all of your benefits from RAG. And it is technically true to say, if you do fine-tuning, you’ll get more benefits, but you’ll go from 63.13 to 63.29, as opposed to a 10x. You know, going from 6.7 to 63. [15:48] Hamel: So I think it’s worth stopping here because I think this is actually a very confusing like, people get stuck on this. Like you know, I’m of the mindset like, hey, if your model needs context from your data, you should just, you should always do RAT. Like you don’t want to try to fine tune all of that knowledge. you know, from all your documents and everything, like, try to, like, expect that your model is going to, like, memorize all of that and stuff. I don’t, there’s no thing that’s a good idea. [16:20] Hamel: So, I feel like, yeah, I feel like if your application could use RAG, you should do RAG. Like, there’s no, yeah, I think people get confused. Like, when they see papers like this, like, oh, find two different ways to RAG, like, oh, maybe, you know, they’re like, totally, like, no, there’s no option. You have to, you have to use RAG. [16:37] Emmanuel: Well, [16:38] Hamel: I mean, like in most applied situations, like to make it work. [16:42] Emmanuel: I think this is sort of like, you know, this is why like you doing this course is good though. Cause I don’t think this is common knowledge. Like in particular, the, like, you know, like maybe you other practitioners that I’ve talked to, like some people know or know, or have the like intuition, which I think is correct. That like fine tuning, isn’t really the right solution. If you want to like acknowledge, like it’s like, that’s just not what it’s for. in most cases. [17:04] Emmanuel: And so, like, you know, like, yeah, you can, like, say, like, ah, for this use case, actually, it makes a little sense for that one, maybe a little bit more. But I actually think that that’s not well-known. And so, like, one of the reasons that I’m, like, on this hobby horse or something is to be, like, no, like, in most cases, like, you’re, like, ah, like, my problem is that, you know, like, this model doesn’t know about whatever our business model is. And it’s, like, yeah, the solution for that is not fine-tune it, usually. [17:25] Emmanuel: It’s, like, just tell it where your business model is. So, yeah. [17:29] Hamel: Makes sense, yeah. [17:32] Emmanuel: I think I have… Yeah, this is similar. I found this paper that was doing some, like… Yeah, again, like, rag plus fine-tuning. Not to, like, belabor the point, but, like… I think I was curious on, like… Like, one thing that I was curious about is, like… Ah, okay, like… I think there’s probably a model size thing going on. Anecdotally, I think there’s some papers and some various experiments I’ve been running where it seems like fine-tuning is more beneficial in smaller models than bigger ones, potentially. [17:59] Emmanuel: And so I thought it was interesting to find out this paper was doing this with small models, smallish models. And I think this is another example of what we’re talking about, right? I don’t remember what the use case is for this, but oh, it’s like… It’s like for knowledge. And so it’s like, yeah, for knowledge, even for small models, you want rag. [18:15] Hamel: And so how do you interpret this table? Like the ones on the very right hand side, the ft, rag plus rag, does that mean it’s getting worse with fine-tuning and rag? [18:26] Emmanuel: That’s how I interpret it. [18:28] Hamel: That’s a little bit surprising, yeah. [18:30] Emmanuel: My interpretation is that this is within the noise. I would guess just based on bass plus fine tune being pretty close, slash even worse here, and pretty close, that this is just like. [18:42] Emmanuel: based model and fine-tune in this example like your fine-tune doesn’t do much and like i wouldn’t i wouldn’t over index on this being like slightly lower basically i’d say like yeah fine-tune plus rag probably just does as well as like rag would be you know it’s interesting to look at this without reading the paper because we don’t know what task is being scored or at least i can’t tell and [19:02] Participant 3: then it’s like if you wanted to measure adherence to a writing style book you then I suspect rag doesn’t do so, so much for you. Like if it’s just writing style and fine tuning, like I think. Right. We could pick a task and then get the results to tell any story we want. But it’s just a matter of what task are we optimizing for? [19:24] Emmanuel: Totally. Okay. So I think this is a great point because as I was doing this and I was writing my slides and giving a diabolical laugh, being like, ha, ha, ha, ha, like Rag is beating fine tuning or whatever. I was like, well, okay, I should do a search for papers that show fine tuning beating Rag. And I didn’t find… many examples. And I think like a lot of the examples that I’ve seen are like Twitter threads or something. [19:47] Emmanuel: And so anyways, this is like mostly a call for like Either like, you know, the host of this workshop or like anybody that’s attending, if you have like good papers that show this for like, yeah, like we were talking about like style examples or things that fine tuning is more suited for, please send them my way. [20:01] Hamel: I think it’s hard to explain because like even when I try to explain knowledge, like, hey. Like, hey, fine-tuning is not good for adding knowledge. That word is not fine-grained enough. What do you mean adding knowledge? There’s a certain kind of knowledge that does make sense. And they’re like, oh, what kind of knowledge? I’m like, oh, okay. I get really… It becomes an intuition. But I haven’t expressed the intuition completely clearly as much as I want to. [20:29] Emmanuel: Well, and my like maybe… maybe like scaling pilled or whatever like hot take is that this this intuition or like the boundary between those changes with every model generation and so it like maybe like a good example is like i think it used to be the case that like ah like you could say like Learning a style of speaking or something is something that requires fine-tuning. Like some style, not knowledge, like a way of saying things requires fine-tuning. [20:54] Emmanuel: But the better models, the more recent ones, can learn a style from a two-line prompt, which the older models can’t do. And so for that, it’s less true. But there are still other things where maybe it makes more sense. So I think that adds to the concept of what is knowledge is something that changes with every model generation, basically. [21:14] Hamel: Yeah, no, that makes sense. Yeah, I have that same experience as well. [21:20] Participant 3: I think that there are a bunch of cases where we could look at it and we’d be like, we’re not even sure if that counts as style or content. So if we… fine-tuned on manufacturing copy from the makers of like the XS32 widget. And everywhere when it says like the best widget is, and then you fill in the blank, it’s always XS32. Like that’s sort of knowledge. It’s knowledge that XS32 is some great widget, but actually it’s just, well, that’s whenever they express positive emotion, that’s like the widget that they express it about. [21:54] Participant 3: And so it’s sort of tone and maybe. knowledge is actually not a very clear abstraction. [22:00] Emmanuel: Yeah. I mean, notably, like… This is like way outside of the bounds of this presentation or something, but like it’s not clear from even like the early work that we have on like interpreting these models that, you know, like the other concept of knowledge as we’re discussing it here, like is something separate from the concept of style within their actual ways, right? Like I would bet that for many cases it isn’t. And so it’s not like there’s like, ah, like, you know, that kind of like thing that’s in like this, this like. [22:28] Emmanuel: attention head or whatever, like is the knowledge versus something else? Like I think, I think it’s, it’s even the model doesn’t have like that clean separation. [22:36] Participant 3: We’ve got our first audience question. You want to go ahead, Ian? [22:40] Participant 4: Yeah, sure. Hi, thanks for this talk. So my company, we have a very complex knowledge base that we’ve curated like a hundred thousand hours, I bet of time for precision oncology, which is genomics and cancer. My intuition is that I’m going to be able to curate using those curated rules, a fine-tuned model that does a good job at creating a first draft of a curation, right? So we curate what are called guidelines and clinical trial documents. Does that fit in your model? What would be your advice based on that description? [23:18] Emmanuel: Oh boy. I think the part that’s hard, just going back on what I was talking about, is that as a non-expert in this domain, I think I have a worse intuition than you do on where this falls on the knowledge versus style spectrum or something. I think one comparison I would draw here is like… Maybe the, actually I talk about it in the next slide, but there’s like some attempts by, you know, people to train sort of like LLMs that are specific to finance or to agriculture or whatever. [23:53] Emmanuel: And those often like in the short term beat the current open models or current available models. But then I have an example after they like. they often are just beaten by the next bigger, smarter model. So I think that’s one thing I would consider. What’s the trend of maybe if you take the Cloud 3 models or the GPT models and you take the smartest and the dumbest one and you see how they compare with no fine-tuning, that could give you a hunch of the next generation is probably actually going to be good enough. [24:27] Emmanuel: And then the other thing that I like to use to decide whether fine-tuning will… will help or has a chance to help, and also whether I need it at all, is to just keep adding a bunch of examples to my prompts of basically the shape of what I would fine-tune on. I feel like probably other people have said this in this conference or something, but seeing how well model performance increases with that also… can give you a hunch of how well it would increase with fine-tuning on a large data set. [24:54] Emmanuel: And notably, if you see it plateau after a while, you don’t need to fine-tune, in my opinion. [25:00] Hamel: We have a related question from Simon Willison, who asks, does fine-tuning ever work for adding knowledge? I see this question come up all the time. And I think, like, the thing that’s, like, confusing about answering this question is, like, there’s some knowledge that is, like, kind of intrinsic to, like, maybe, like, a world model or something. Like, I think about, like, okay… this could be wrong, but like a physics constant of like gravity of the earth or whatever. [25:31] Hamel: Like it’s like, I think it’s like for my intuition tells me language model is okay with memorizing that. I’ve seen that like so many times. I don’t want to retrieve that with rag, but like something more specific about Hamel, like about my life, like changing facts. Okay. That makes sense. Like something like, you know, that it’s not trained on clearly rag, but there’s like some middle grounds of fuzzy middle ground where I have strong intuitions, but I don’t know how to like tell people. [25:57] Hamel: like about adding knowledge like what is yeah like i i understand like intuitively like there’s some knowledge i want the language model to internally like grok like fundamentals about the world or in things but like others like i would never expect it to like you know internalize so i don’t know like how do you explain this aspect [26:24] Emmanuel: Yeah, I think it’s also complicated, right? Because I think for most… So the first thing I’ll say is for most things where I want to add knowledge. So let’s say I want to add knowledge to some model that… I don’t know, like I like strawberries or whatever. So when it sees my name, it knows that, oh, by the way, Emmanuel likes strawberries. I think that, first of all… that’s almost always something that you could just do with a prompt, right? Or some rag, or whatever. [26:48] Emmanuel: It’s like, oh, when there’s a question about me, we retrieve something, and there’s a description about Emmanuel, he likes strawberries. And if you have a good instruction following model, it’ll just do the same thing. And so then the question is, okay, if I were to change the weight of the model for it to do this, how do I do that? And I think this is often actually not that trivial, because if you just fine-tune in a bunch of prompts that are like, what does Emmanuel like? He likes strawberries. [27:10] Emmanuel: If you have a dumb model, it’ll only… tell you that I like strawberries if you ask it what I like. But oftentimes what you want with your fine tuning is you want it to like know this information and use it when it’s relevant in other contexts. And so maybe then you have to like think about like, ah, like what I want to fine tune is like, you know, I don’t know, like we’re a business that does like shopping recommendations. And so when like Emmanuel logs in, like And he just asks random questions. [27:34] Emmanuel: I’m just going to be like, oh, by the way, did you buy your strawberries this week or something? And you fine tune on these specific prompts for this specific context. And then I guess my qualm with this, and calling it knowledge, are you adding knowledge to the model? Or are you basically fitting it so that in this narrow distribution of these few prompts that you’ve given, it leans more towards mentioning strawberries? [27:58] Emmanuel: And I think this gets at fundamentally how these models learn, and at least as far as I know, we don’t actually have a satisfying answer to this. But I’m pretty convinced that a lot of fine tuning ends up in that surface level realm of in the specific context for the specific question, shaped like the fine tuning dataset, I will tell you this thing, versus I’ve now learned whatever that means for a model that like Emmanuel likes strawberries. Hopefully that made sense. [28:28] Hamel: Yeah, no, I think there’s no wrong answer, I don’t think. [28:33] Emmanuel: I think the answer to a lot of this stuff, and the reason why it’s so fun to work on, is that we don’t know. Hopefully we find out soon. But, yeah. I have a few examples of… Oh, yeah, go ahead. [28:45] Participant 5: Do you have a second for another question? [28:47] Emmanuel: Yeah. [28:49] Participant 5: Great. So in terms of knowledge, okay, so one of the examples that I feel like perhaps has worked out, that I’ve heard about, is… So if you take like multilingual models, and if those are not highly trained on non-English, but they have some non-English of the other type, and then, so it sort of understands a little bit of that language, but in general, its quality on the non-English languages is kind of crap. If you then fine-tune a lot more on the other language you want to get better quality on, that tends to work. [29:28] Participant 5: And it seems like it’s sort of like giving it new knowledge, but it’s not… Like, I already knew some of that language, it was just kind of prophetic. It didn’t have a lot of examples. Is that maybe a case where that makes sense? [29:40] Emmanuel: Yeah, I think that’s interesting. The first thing that this brings to mind is that like… In some of the interpretively work that we’ve shared, like if you look at some of the way that the model represents, like at least large competent models represent various things, like the concept of a table or whatever, like the Golden Gate Bridge. It’ll represent it in the same way in different languages. And this is increasingly true as the model gets smarter or something. [30:12] Emmanuel: And so I could buy that fine-tuning that to be better is slightly easier than other types of fine-tuning, because it’s already seen this. It’s already kind of like learned to map different languages to like a concept for other concepts. And so it like needs a really small change to like map, you know, this like new concept that somehow like hasn’t really learned fully for this new language to be the same, like basically part of the model that does the reflection for it in English. [30:41] Emmanuel: So I don’t know, like I could see that work for the specific thing. Oh, sorry. [30:47] Emmanuel: Go [30:47] Participant 5: So similarly, do you think that it’s a similar kind of thing that happens when people make fine-tuned code models that are good at doing software programming, essentially? Or would you think that’s a different thing happening? [31:04] Emmanuel: Yeah, I think with fine-tuned models, there’s a bunch of other concerns. It depends on what you want, but if you want a code-complete… This is the style we were talking about. You kind of want the model to… It’s not gonna be like, oh, hi, I’m Claude. Let me tell you about code. You just want it to auto-complete the current line you’re writing, which I think that was the style discussion we were talking about. And then there’s speed concerns as well with code language. I think that the knowledge of your codebase actually… [31:32] Emmanuel: That’s also something that Rav works really well at, and I’m not sure you need fine-tuning for. Just having the good context of what you’re currently working on. I’m not convinced, maybe to be clear, that… I don’t think there’s good data on this, this is my hot take, but I’m not convinced fine-tuning on your whole codebase or something gives huge gains compared to putting as much of that codebase in the context. [31:53] Emmanuel: The other thing I’ll add that’s related to this is that both, I think, for one of Google’s launches recently and then for the Cloud 3 launches, there was an example that was shared of a model not knowing a rare language. And then you put, I think, 100 pages of that language in the context, and then all of a sudden it can do it without any fine tuning. And the reason I mention it is, like, I think the fact that this is comparable to fine tuning is really interesting. [32:24] Emmanuel: And I’ll talk about it a bit more after, a bit more why I think it’s very interesting. Okay. I’m going to jump ahead. Feel free to interrupt me if I haven’t answered your question properly or if you have other ones. I’ll just go ahead and jump ahead. This is basically what Hamel was saying. Fine-tuning is not the solution for domain knowledge. This was the other paper I mentioned, which is, I think this is an agriculture paper. And it’s like, if you fine-tune like GPT-4, you get 61% performance. [32:54] Emmanuel: Sorry, if you do that and you do RAG. And if you just do RAG, you get 60%. So again, strictly, if you really care about that 1%, useful, but really you got most of it from RAG. And also, I don’t know how big the error bars are here. But this is kind of confirming what we were saying, which is like this is like a domain knowledge kind of thing, and so it feels less useful. Right. [33:14] Emmanuel: So the other thing that I think is challenging for fine tuning, especially if it’s like at the cutting edge, is that you’re aiming at a moving target. You know, like there’s many labs, Anthropic among them, that are like continuously working on making the model better. And so this is this example is like the Bloomberg GPT example. I’m realizing here, I apologize. I think I like used. I think I like used. [33:32] Emmanuel: a bad figure but essentially the Bloomberg GPT model claims that it was doing better than chat GPT at the time or gp3 on like some financial analysis task and they like pre-trained their own model so it’s not fine-tuning it’s pre-training here um which I guess like doesn’t show really in this table um but then a few I think like six months later you know gp4 came out and chat GPT and it was just way better than like their fine-tuned model and basically everything so I have a question about this that’s a really interesting I’m glad [34:01] Emmanuel: you brought this up like [34:04] Hamel: First of all, the Bloomberg example, like when they made a big deal out of it. Like, hey, we like pre-trained, we did this like pre-training of this model. It costs like millions of dollars, whatever. But my first reaction was like, why did you pre-train a model? And why didn’t you fine tune a model? But that’s a different question. Second one is like, I’m curious, like, okay, yeah, these frontier models or whatever you want to call them, they’re getting better. Like you’re saying it’s moving target. [34:33] Hamel: If you have a fine-tuning pipeline, let’s say one for like Claude, just because I don’t want to call attention to OpenAI, just keep it so don’t get kicked out. If you have a good fine-tuning pipeline where you’re fine-tuning these even big models. Can’t you just keep moving with the state of the art? Like, hey, okay, this new model came out. Let me fine tune that. Let me keep fine tuning that. Assuming that those APIs are exposed. Yeah, like for the most powerful models, maybe they’re not exposed yet. But just, yeah, curious. [35:09] Emmanuel: I mean, like, I think like the question is, you know, can you always take the latest model and fine tune on it? I think the answer is yes. Although, obviously, if you take the BloombergGPT example, they… pre-trained, right? But presumably, the whole point of their exercise was that it’s a large data set that they have. So the cost to fine tune isn’t much cheaper, because they probably mostly train on their data set. And so it’s like, do you want to pay that much money any time there’s a new model that appears? [35:45] Emmanuel: That can often be sort of like, oh, you could either, if you have a pipeline that just takes an arbitrary model, does some rag and some prompting, you can swap models easily. But if you have to re-fine tune, that gets pretty heavy. So I think it’s possible, but it’s just a matter of cost. [35:59] Emmanuel: And then the other thing, like, on that note is, you know, like, I think that, like, The fine tuning of a larger and larger model seems, and I’m curious, maybe you have more experience on this, I tried to find some papers, but that’s mostly an anecdote, seems like less and less. Or it seems like as these models just get better, they just get better at everything. [36:31] Hamel: Yeah. Okay. So kind of the most common tactic is to use the data generated by the most powerful model to train the faster models one step below it and to keep walking that ladder up. Like, okay, new model came out. Okay, let’s use that, get better data and whatever. And like, of course, analyze the gap, but it’s usually like, okay, you get a much faster model and hopefully similar performance or better performance. I guess like, yeah, you have to like look at the cost because there’s probably like some, you have to do the analysis. [37:08] Hamel: Like does this even make sense? This exercise? [37:11] Emmanuel: Yeah, exactly. I think that like, I think that a lot of, at least as far as I can tell, like a lot of teams, companies are like underestimating the cost and overestimating the value of this stuff. But yeah, I think you certainly can. Again, I’d love to like, I wish there were like a few more examples of, you know, we’re talking about words like I haven’t seen many papers where it’s like, oh, we do this and we train like the super small model and it like actually works better. [37:31] Emmanuel: I think I’ve seen this like for some like evals that seem like obviously what we’re fit to in some in some examples where then like the model is in general eyes. And so it’s like nice for a paper, but can you actually use it for anything useful is not clear to me. But yeah, you can do it. I think it’s a matter of cost at this point, right? And of value. I think there’s certain use cases where it totally makes sense. [37:53] Emmanuel: I think there’s certain use cases where it’s pretty far down the priority list, in my opinion. I have… Oh, no. Yeah, this is what we were talking about. Unless there’s any questions, I think this is basically what we’re talking about. And I’ll just dive into the difficulty. [38:08] Hamel: Yeah, please. [38:10] Emmanuel: So this is something that I’d like to be like, I’m only made like the same graph, or like, at least we’ve talked about this exactly. But it’s like really like for like, when you’re doing ml, you know, like the optimal use of your time, like, even if you’re doing fine tuning, to be clear, even if you’re training models from scratch is usually like 80% data work, collect it, label it, enrich it, clean it, get more of it, see how it’s broken. [38:33] Emmanuel: You know, 18% is just general engineering, like how do you serve your model, how do you monitor your model, how do you make sure that it works, there’s no drift, all that sort of stuff. And then maybe 2% debugging some, like, my model doesn’t train, or I get a GPU, or something like that. And then you’re like, I got 0% usually cool architecture research. At least that’s been my experience. And so the reason I mentioned this is that machine learning is hard, I think, even if you don’t train the models. [39:06] Emmanuel: If you actually buy this, oh, I’m not going to fine tune, I’m just going to use rag and prompt some elements, you still need to do basically all of this. You filter out maybe this, you don’t have the model code, but you still need to set up input validation, set up filtering logic, set up output validation. monitor your inputs, monitor your latency, monitor your outputs, do backtesting of your prompts and rag systems, do evaluation, have a trained test split so you can do experimentation, potentially A-B test. There’s this whole world of things. [39:36] Emmanuel: And I think maybe the reasonable version of the hot take is not that fine-tuning is dead, is that if you talk to me, I will only allow you to do fine-tuning if you’ve done all of this first or something. Because I think all of these are higher on the hierarchy of needs. than fine tuning. Which, by the way, I think is something that you had this great article in O’Reilly recently that basically laid out the same thing. So I don’t think it’s a super controversial take. [40:00] Emmanuel: But I think the thing that grinds my gears with fine tuning often is that people don’t do any of this and then fine tune a model before they even have an eval. Which that is, I think, problematic. [40:13] Hamel: Yeah, that drives me nuts too. Can you go back to the previous slide, if you don’t mind, for one second? Okay, so this makes sense. One kind of question I have is like… [40:25] Emmanuel: These are not very sensitive numbers. No, [40:26] Hamel: no. But it’s fine. So, like, the collecting data set part, okay, you don’t need to do that with… If you’re not fine tuning. [40:34] Emmanuel: You can do some of it for your email. [40:36] Hamel: What about the looking at data? You still can do that. The looking at data… Like, for me, looking at data takes up to 80%. [40:42] Emmanuel: like it’s almost the same in a way like the cleaning and the looking it’s like oh totally yeah to be clear like maybe this is like not the way i should have of formats of this but this is mostly like if you think that like when you’re when you’re going to do fine tuning you’re going to do like very different things you’re not you’re just going to spend most of your time looking at data i think it’s true in both although i think like as i mentioned the failure mode is that people just like don’t [41:04] Emmanuel: want to do this and they’re like ah instead i’m going to like go on a side quest to like fine tune it’s not not all fine tuners i see okay [41:12] Hamel: Okay, so I mean, you’re still doing this 80% no matter what. [41:15] Emmanuel: Totally. [41:15] Hamel: It’s like even more dangerous if you just like, just go straight into fine tuning without [41:21] Emmanuel: Yeah, like, like, yeah, [41:22] Hamel: exactly. [41:24] Emmanuel: It’s like, the very one is like, you just like, don’t want to do this. You’re like, instead, I’m going to like do some fine tuning and like some random data set I’m not going to look at. And I do that. And I’ve seen that go poorly. [41:36] Hamel: Makes sense. Right. [41:39] Emmanuel: So, like, I think basically, like, most of your time should be spent on all of this. And once you have all of this, then I think it makes sense. And basically the way I think about it, right, it’s like this is all the stuff that’s necessary to actually have a working ML just, like, system. And so, like, before you even train your first model, you should have, like, the infrastructure to do the fine tuning. And so it’s like once you’ve done all of this, then I think you can sort of consider fine tuning. [42:03] Emmanuel: But doing it before that is sort of unwise. That’s basically the same thing. I think the first things I would do, I always recommend to either my friends or people I talk to that are building is eval sets first, representative eval sets, large eval sets. Eval sets are easy to run. spending days working on prompts, I think like, I can now not even count the number of times that I, you know, told somebody like, well, have you thought hard about your prompt? They’re like, oh, yeah, I’ve worked so hard on it. [42:35] Emmanuel: And then, you know, like, I look at it, and it’s like, in two hours, I get it from sort of like, 30% to like 98% accuracy. And I think that’s like, I am not a genius at this. It’s just like actually spending the time, you know, making your prompts clear, following like there’s a bunch of really good prompting guides. So I’m not going to talk about it much here. We can talk about any questions if you are curious. Yeah. [42:54] Hamel: I think one observation here is like really interesting is like, okay, the first and the last bullet, those are like the same things that you should spend your time on. I feel like in classic ML. [43:04] Emmanuel: Yes. Like, [43:05] Hamel: and so it’s like really interesting, like, not that much feels like it’s changed in a way like totally this is the same message we’ve been repeating for years um but yeah they’re like not that much has changed but there is like some narrative like hey there’s this new profession ai engineering don’t necessarily need to do ml or think about ml and yeah yeah i’m curious like what you think about that like [43:31] Emmanuel: Oh, my like, okay, so my take here is that, you know, it always was the case that like, you should spend your time on that and not on this sort of like, you know, like math part of things, let’s say. And now it’s just even clearer because like the math has been abstracted away from you for in many cases in the form of an API, you know, if you use like API providers for models. [43:56] Emmanuel: And so like, there’s like a strong, I think temptation for people that are just like interested in interesting problems, a temptation that I have and understand of like, no, like I want to like. get back and do the fun ML stuff. And I think if that’s your reason for fine-tuning, it’s bad. [44:11] Emmanuel: But then, to the point you were talking about, once you’ve done all of the actual work of machine learning, and then you’ve done all this and you realize, ah, the only way to get extra performance in this thing that’s important is fine-tune, or get lower price or latency or whatever, then that makes sense. But I think basically it’s like, this is… The same thing that it always was, but it’s almost like the gravitational pull of the fun stuff is even stronger now that it’s like, oh, what? I don’t even get to see the Jupyter Notebook. [44:36] Emmanuel: I just have an API call. That’s no fun. [44:40] Hamel: Yeah. [44:43] Emmanuel: The last thing I’ll say is actually pretty important. I’ve left it in the last line. And I think it’s just like looking at trends. [44:52] Emmanuel: and basically extrapolating on them and either like deciding that like the trend line is going to continue or it’s going to break and i think again the real answer is like nobody knows but if you just look at the trends of like model prices and context sizes so this is like model price for like a roughly equivalent model not even equivalent because models have gotten better but this is like the like price of like a you know cloud haiku slash gpt 3.5 ish level model um you But like the Cloud Haiku today is like better [45:20] Emmanuel: than, you know, like certainly like that one was in 2021. So it’s, you know, actually like even cheaper than that. The price has gone from like, I think it’s like 60 bucks per mega token in 2021 to like now if I remember well, the blended price is something like half a dollar. And context size, you know, has gone from like, I think it was 2k at the start, maybe 4k. And now 200k, a million, 1.5 million, I think I’ve heard 10 million. [45:46] Emmanuel: It’s possible that both of these trend lines stop, but I think it’s important to consider like what if they don’t? One thing that’s not pictured here is latency, which has kind of decreased in the same fashion. So models are faster and faster. It’s like, ah, if in 2025 or 2026, you have a model where it has 100 million context, it has crazy latency and it’s basically, let’s say if it keeps going, even 10 times or 100 times cheaper. [46:13] Emmanuel: like you just you don’t fine-tune you just throw everything in context and these models are amazing at learning from context and if they get faster like you just get your response immediately. And so I think there’s like a really interesting question of like, obviously you can’t extrapolate, you know, like any exponential or even like straight line forever. There’s always points at which they stop. [46:34] Emmanuel: And so it’s like, depending on when this line of like price per intelligence, basically plus latency, which is very important in my opinion, like stops, then I think it tells you like, ah, what use cases should you even consider for fine tuning? It’s like, you should consider the ones where it’s like, you’re well outside of the like context window limit. slash like chunking through that context at these speeds that will keep increasing will take too long for your application or something. [46:57] Hamel: And then there’s like the prefix caching that starting starting to be done. [47:02] Emmanuel: Yeah, exactly. [47:02] Hamel: You know if Anthropic may offer that? No. Okay. [47:07] Emmanuel: This is a manual talk, not an Anthropic talk. But but yeah, I think that like. I assume, all jokes aside, that things like prefix caching will be a common thing, certainly in a few years, right? And if that’s the case, and you can imagine that your fine-tuned data set is easy to formulate as a prefix most of the time, then yeah, that makes that equation even different. So I think like… Honestly, I think this is why I had this chart last, because I think this is what started the debate. [47:37] Emmanuel: Because I think the thing that makes me the most bullish about, oh, we won’t really need to fine tune models as much as this chart, is just the direction in which we’re going. That combined with the other beautiful chart I showed earlier of prompting growing, I think is just a really interesting trend. And if it holds even for a year or two longer, I think that eliminates the need for fine tuning for many, many applications. [47:57] Hamel: Yeah. There’s one question from Lorianne. I can fine-tune and replace few-shot examples, especially when I need to save on tokens. I mean, I know the answer to that. But I think like, so one correlated question is like, I talk with people like Husain a lot, you know, Langchain. And I’m like, oh, what are some interesting things people are doing? And he’s telling me that a lot of people are doing dynamic few-shot examples. Think about RAG and think about you have a database of few-shot examples, and you’re just pulling the most relevant ones. [48:35] Hamel: He’s saying that works really well. Yeah. Do you see that? The people you talk to, are they doing that? Is that common? [48:44] Emmanuel: Yes. I’ve seen lots of examples of this. This is common because a lot of the times, few-shot examples become unwieldy because you want them to be like, evenly distributed. So if you have a complex use case where your model can do 10 things, where it’s like you kind of want one example of each of the 10 things and maybe one example of the model doing it one way and one example of the model doing it another way. And so it’s doable, but you can just quickly blow up your context window. [49:08] Emmanuel: And so fetching relevant examples is something that works really, really well. And it’s pretty common. I would say that maybe in my hierarchy of needs, there’s all the stuff we talked about and then there’s like, really work on your prompt. I tweeted something yesterday or something because I helped the 10th person with the same loop. But it’s like work on your prompt, find examples that don’t work, add them either as an example to your prompts or as one that you can retrieve and add conditionally. And do this like 10 times. [49:35] Emmanuel: And then only after that consider anything else. That tends to work really well. Cool. Yeah. Well, thanks for having me. Hopefully this was interesting to everyone. [49:44] Hamel: This is very interesting. Yeah. [49:46] Emmanuel: Cool. Awesome. [49:47] Hamel: All right. Thank you. [49:49] Emmanuel: See you, everyone.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Why Fine Tuning is Dead"
    ]
  },
  {
    "objectID": "education/evals/schoelkopf.html#chapters",
    "href": "education/evals/schoelkopf.html#chapters",
    "title": "A Deep Dive on LLM Evaluation",
    "section": "Chapters",
    "text": "Chapters\n00:04 Introduction to LLM Evaluation Deep Dive\nThe complexities of LLM evaluation, including contributions from Eleuther AI to open-source AI and model evaluation, and the use and evolution of the LM Evaluation Harness.\n01:49 Scoring Challenges in LLM Evaluation\nThe complexities of accurately scoring LLMs, particularly when evaluating natural language responses to factual queries, and the importance of robust evaluation techniques.\n05:35 Log-likelihood Evaluation\nInsights into log-likelihood evaluation techniques, generating next-word probabilities in sequence models, and how the autoregressive transformer architecture aids in training and evaluation, including practical aspects of using log-likelihoods.\n13:53 Multiple Choice Evaluation and Downstream Concern\nThe benefits and limitations of multiple choice evaluations for LLMs, including their simplicity and cost-effectiveness compared to long-form generation, and the necessity of aligning evaluation strategies with practical use cases.\n18:46 Perplexity Evaluation\nPerplexity as a measure of model performance, the process for calculating perplexity, its utility and limitations, and how different tokenizers can impact model comparability.\n22:44 Text Generation Evaluation\nThe challenges of evaluating text generation, include difficulties in scoring free-form natural language and the impact of tokenization on evaluation results, and the importance of careful evaluation setup to avoid biased outcomes.\n27:40 Importance of Transparency and Reproducibility in Evaluations\nThe importance of transparency and reproducibility in LLM evaluations, the challenges of achieving reproducible results, and the need for detailed reporting and sharing of evaluation methodologies and code.\n38:23 Audience Q&A\nPractical advice and broader conceptual understanding through the Q&A session, addressing various questions about using specific evaluation frameworks and the effectiveness and limitations of current LLM evaluation methods.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "A Deep Dive on LLM Evaluation"
    ]
  },
  {
    "objectID": "education/evals/schoelkopf.html#resources",
    "href": "education/evals/schoelkopf.html#resources",
    "title": "A Deep Dive on LLM Evaluation",
    "section": "Resources",
    "text": "Resources\n\nHailey Schoelkopf: Twitter / X, GitHub\nEleutherAI: Homepage\nLM Evaluation Harness: GitHub\nOpenLLM Leaderboard: Link\nLessons from the Trenches on Reproducible Evaluation of Language Models: arXiv",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "A Deep Dive on LLM Evaluation"
    ]
  },
  {
    "objectID": "education/evals/schoelkopf.html#full-transcript",
    "href": "education/evals/schoelkopf.html#full-transcript",
    "title": "A Deep Dive on LLM Evaluation",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:03] Hailey: So I’ll be giving sort of a very brief, about 30 minutes. There’s much more than can be covered in that time, but as much as we can, or sort of an opinionated summary. I guess an alternate title for this talk could have been basically everything you didn’t realize that you needed to ask about LNE valves, which is sort of a taste of what you should be looking out for. [0:22] Participant 2: Yeah, [0:23] Hailey: here we go. So just, yeah, a little bit about me. I’m a research scientist at Eleuther A. We’re a nonprofit research lab. You might know us from sort of a number of language models that Eleuther released open source over the years, including GPT-J and GPT-NeoX20b, that were some of the best of the time a few years ago. Things have certainly changed and there are way more options now. But we also do other research on things like interpretability of models, data sets, distributed training, evaluation, which is this talk. [0:53] Hailey: And we also build and maintain a couple different repositories for… sort of tooling for the open source A ecosystem, especially tailored toward researchers, but just useful for practitioners in general. Yeah, so I, in particular, I’m a maintainer on the LM evaluation harness. Here’s a link. It’s a library that was originally started by some of the founders of Eleuther A in 2021, sort of originally just with the well-scoped goal of sort of one-to-one reproducing the evaluations. [1:23] Hailey: described in GPT-3’s few shot prompting in the paper on the GPT-Neo model, just to sort of track progress and reproduce these results. And since it’s grown a lot with the community and there’s a lot more people working on LLM than evaluation. And we’ve been lucky to have it widely used by a lot of people. You might know it from the OpenLLM leaderboard. It’s used as the back end there. [1:50] Hailey: Yeah, so in this talk, like I said, there’s way more than can be covered in just like 30 minutes, but I’ll try and give like a couple deep dives into specific topics. I’ll briefly give some background on why LM evaluation is so hard and why we need to think so much about solving many problems here. I’ll give sort of like a very, very brief crash course in how evals are commonly done under the hood. Some of these like gritty implementation details that often aren’t talked about. [2:20] Hailey: and also give some takeaways as a result of this and some sort of minimal best practices. And so as a disclaimer for context here, I am a researcher. I don’t put LLMs into production in my work. So my recommendations are going to be somewhat based on my experience in research. But at the end, I’ll sort of touch on which of these points are most applicable still, if you’re doing work in putting LLMs into production and which sort of don’t directly transfer. So yeah, so there are many reasons that evaluation is hard and also meaningful. [2:52] Hailey: But two that I’ll sort of try to focus on here is first that scoring is very difficult and later reproducibility is tough. So what do I mean that scoring is difficult for LLM evaluation? So there’s a very difficult problem, which is basically how do we evaluate the responses of a language model in natural language? How the heck do we do this? [3:14] Participant 2: So, [3:14] Hailey: like, in this figure, sort of, there’s a couple different clouds, one both showing different like potential responses from a language model to sort of a very simple factual question, which is, who is the current US president? And so in on the left, the model answers the correct answer, just saying Joe Biden. On the right, the model says, oh, well, it’s definitely not Joe Biden, and doesn’t really go on to elaborate. And so one of these is correct, the one on the left and the other isn’t. [3:43] Hailey: And so, like, you or I, if we just look at this question and we have the necessary context, we can sort of eyeball it and say, well, we know that one of these is correct, the other isn’t, it’s fairly obvious to me. [3:53] Hailey: But how would we actually, when we want to do the evaluation, we want to sort of be able to reliably get a measure of our model’s performance and do this without much effort and in a way that we can sort of get the same result repeatedly if we want to rerun this evaluation many, many times. We still want something. more reliable than just sort of a human eyeballing every data point. [4:13] Hailey: And so we’ve got a problem, which is, well, the one solution we might think of is, OK, well, if the model’s answer to the question includes the term Joe Biden, then we’ll mark it as correct and sort of treat that as an approximation and go on with our day. But the problem is, in this example on the right, the phrase Joe Biden appears in the model’s answer. And yet it’s actually saying, oh, it’s definitely not Joe Biden. So like the meaning of these two responses is entirely different. [4:40] Hailey: But if we use this sort of naive approach of just checking for the right term in the output, then we would end up sort of scoring both of these as correct, even though one of them is not. And so this problem as sort of a motivating challenge of sort of how do we check a language model’s work is going to shape a lot of the approaches to evaluation. Because if we could sort of. [5:03] Hailey: no matter what, give the correct answer on whether a model’s freeform response were correct, then we’d sort of have solved the hallucination problem entirely and have the perfect language model already able to evaluate these answers. And so yeah, so people sort of take many different approaches and different situations to resolve this issue of actually being able to score outputs for correctness. But there’s no sort of perfect solution. Everything is either too costly or maybe unreliable or has different sort of failure modes. [5:36] Hailey: And yeah, so with this in mind, as sort of like the constraint that we have to work under, there are a couple different ways that we can try to evaluate a model, but in particular how we can sort of probe these models to inspect their capabilities or characteristics. And so there are three different ways that people can often interact with these models or sort of try to measure things from them that I’ll talk through. The first is log likelihoods. [6:04] Hailey: So, yeah, so just as sort of a refresher on language models and the necessary background here, when we feed a piece of input into our language model, what we get out is a logits over a vocabulary. So for each of our language models, like, for example, for GPT-2, we’ve got sort of a space of 50,000 different possible tokens that it knows. And when we run some input, X0 through Xn of words or tokens through GPT-2. [6:34] Hailey: What we get out is this sort of for each different possible next word, GPT-2 will give us like a numerical score representing like a logit on this token. And if we apply a softmax, then we can turn this into a probability distribution, which will basically tell us that GPT-2 thinks that there’s a 50% chance that the next word is the, or like a 25% chance that the next word is dog or 10% of cat, etc. And. [7:04] Hailey: So this is going to be the basic output from our language model, the probability distribution over its guess for what the next word or next token will be. And when we want to most often use language models, we’ll sample text from them. And the way that we generate text with this is that we feed in the input, we get out its probability distribution over what its guess for the next word is, we sample from this distribution and pick a possible next word. [7:32] Hailey: And then we feed in that next word to the model and we get out sort of what the guess for the next word is, and so on until we’re satisfied and done. And so in this way, sort of a language model maps an input to a probability distribution. [7:49] Hailey: But a key detail in how we train most autoregressive large language models is that When we train GPT-2, we’ll feed in many, many paragraphs all at once as sort of the same sample, and at each next word, we can have the model guess sort of what it thinks the next word would be. So at the sort of first sentence of this paragraph, we can guess what it thinks is going to end that first sentence. [8:14] Hailey: At sort of the very end of the paragraph, we can guess what the very end word is going to be and do this all at once. So the model is going to give us many, many probability distributions. both not just predicting this X sub n plus one, but also X sub n and X sub n minus one, and also X sub one, and so on. So we basically have information about what the model’s guesses would have been, even though we also know the correct next token throughout this input that we feed in. [8:44] Hailey: So what we’ll get out of a language model is these logits, but it’s not just sort of. a vector of the shape vocab size, but it’s sort of for each sequence position along the sequence length that we feed in, we get a logit for each element in the vocab. And yeah, feel free to pop in with questions if there are questions, but otherwise, I’ll keep going. [9:09] Hugo: So there is, firstly, just quickly, this is great. And there is, there’s not a question, it’s a comment, just a massive, and that a lot of people have upvoted, just a massive thank you to Hayley for maintaining LM eval and for all the amazing work coming out of Eleuther A. And we did just have a question. Is it better to measure or score evals per sample generated or over a corpus? Does this vary between modalities and use cases of capability? [9:36] Hailey: Okay, I think I could try and answer that one later on. I guess I see this. [9:43] Hugo: And there’s one around clock models as well. [9:47] Hailey: Yeah, yeah, okay. Yeah, so there’s a couple of details I think I see in the questions here. So I guess one thing is that by simultaneously I do literally mean simultaneously, sort of, so when When GPT-2 processes an input, many of the operations, like these sort of MLP layers, operate on each token individually. So you can do this sort of all at once. And then in the attention layers, you have something called a causal mask, which is basically like a triangle of sort of visibility across the inputs. [10:21] Hailey: So for each like for token 10 in the input. It can see all of the previous tokens for token 20, it can see token 19, token 18, all the way back to the beginning. But for token 2, it can only see the previous one token. And so using this, basically, we feed in the entire sequence all at once. And it sort of only can see at each point in the sequence, it can only see the previous parts, but it can sort of compute everything in parallel. This is at both inference and training time. [10:57] Hailey: Yeah, and oh, and then yeah, one other question is, so closed source models do produce a logits matrix, but they’re often not fully available, which is a caveat I’ll mention later at the top, just due to how the APIs are implemented and what information is made public. So now we’ve talked about logits and specifically that you can turn these into probabilities. But why is this useful other than just sampling text? [11:28] Hailey: One way that it’s useful and one sort of measurement that you can take from a model is if you have some input string x and some output string y, you might want to know sort of how probable it is for your model to output, say, y where y is the correct answer to some question. [11:46] Hailey: And so like for a simple sentence, the cow jumped over the moon, if we say feed in the cow jumped over into our language model, maybe we wonder how likely is it that our model gives the correct next two words versus some other word. And so This can be used basically for, yeah, we’ll get into a number of things that can be used for, but you could use this to sort of measure how likely the ground truth target is. [12:11] Hailey: And so the thing that I mentioned earlier is that you can get these logits for every sequence position in parallel. And so what this means is that for any length of output string y, we’re going to be able to not just sort of generate these words one by one and check if they match up. But we can do this in just sort of one pass through the language model, feed in just one input and figure out the whole probability of a string y. [12:34] Hailey: So this is a bit of a dense slide, but basically if we want to compute the log probability of any string y, assuming that we’re sort of conditioning the model with an input x, we can do this just with one path through the model by first taking the concatenation of our tokenized x and our tokenized y, so just like x followed by y, and then pass it through the model and we get these logits. [12:58] Hailey: And these logits are available to us not just at every position, within the sequence X, but also within sort of the sequence Y coming after X. And so if we just thumb over sort of the proper indices of the logics, what we can do is we can go check basically Okay, assuming our model has been all of the tokens in X, how likely is it that for the specific token ID that we know the first token of Y is going to be, how much probability does the model assign to that token? [13:32] Hailey: And so we can check basically how much probability the model assigns to token zero of Y, token one of Y, and so on, and just sort of combine those probabilities. So if we’re using log probabilities, just sum the log probabilities. If you’re not using log probabilities, then you multiply, but this gets the sort of like very very small numbers for a long number of tokens and why. And so what this basically means is that it’s very easy to check sort of the probability of some output string condition on an input string from a model. [14:01] Hailey: It’s just as simple as one path through the model. And you can do other things like you can also check if y is sort of the most probable next end tokens for your model to produce still in one call, just by checking if each of these like true tokens in y are the most probable for your model to output as opposed to just how probable they are. And it’s all. So this is sort of like a primitive algorithm we can use to get the probability of some output secrets. Then why is this useful? [14:29] Hailey: It’s useful in a common use case for evaluation that people use very frequently. This is used in the majority of the OpenLLM leaderboard tasks, for example, and in MMLU, a popular data set, to evaluate your model on a multiple choice question. If you have some sort of set of closed set of answer strings, y sub i, which in this case, in the case of MMLU, where it’s sort of a four choice, multiple choice question, standardized test, and the answer choices are A, B, C, or D. [15:01] Hailey: What we can do is for each of A, B, C, and D, we can use the previous algorithm we discussed to calculate the probability of producing A conditioned on the input question X, the probability of producing B, of producing C, and producing D, and so on. And we get basically comparative log probabilities of each of these potential choices. And then we can see, like in this example, A is the most likely. So what we’re going to say is we’re going to pretend that… A is the answer produced by our model. [15:34] Hailey: And so in this case, for this specific sample question, the model’s answer is incorrect. But maybe if it had put more weight on the answer D and had a higher chance of outputting D when we prompt it with the question, then it would get the answer correct. But so basically multiple choice is a very common way to do LLM evaluations. The first reason just being because it’s way, way cheaper than doing generation with your language model. [16:03] Hailey: If you’re trying to generate many, many tokens, say for like a long chain of thought to answer each question, this is like a lot of calls to your language model and many steps that you can’t really parallelize versus just sort of passing four different inputs through your model. And so in practice, multiple choice question answering is a cheap way to do evaluation. [16:25] Hailey: Another huge benefit of doing multiple choice is that because we’ve not only said that there’s only these four possible answer choices for a model to choose from, but also we’re only comparing the probabilities of these four choices, there’s no way for a model to give an invalid response or say abstain from answering and take a fifth incorrect choice. It’ll always pick one and its guess might be wrong, but it is going to guess. [16:51] Hailey: And in my opinion, I think that multiple choice question answering, implemented this way, is pretty nice for based language models, especially small ones that you’re training from scratch, because it’s sort of a nice way to not have to deal with these finicky parsing failures and still get sort of like a nice measure of your model’s capability, even when it might not be able to coherently generate long form text. [17:18] Hailey: But by the same token, vice versa, [17:22] Participant 2: if… [17:25] Hailey: your small model can generate multiple choice question answering well for just sort of like a single token ranking, like we described in the previous slide, but can’t really produce long form chains of thought, then this means that your evaluation isn’t really matching up well with the real world use case of, say, like using the model as a chatbot. And so in this sense, like it’s definitely a step away from sort of downstream usage. [17:50] Hailey: It’s also a disadvantage for some evaluation types that chain of thought can’t be used, especially since models are commonly trained with chain of thought. And it’s also somewhat misleading as to sort of real world scenarios in that you don’t necessarily want your model to be just solving standardized tests all day. You want to sort of have it handle open-ended questions where it’s not given four choices to choose from. It’s actually supposed to generate and come up with the choice itself. I guess, yeah, are there any questions? [18:23] Hugo: There are a few questions, comments, but we can leave them to the end as well. I don’t think it’s mission critical currently. [18:30] Hailey: Cool. Yeah, that sounds good. [18:32] Participant 2: Great. [18:33] Hailey: Yeah, so basically, like multiple choice using these log likelihoods is a pretty common way to evaluate language models, but it definitely has its downsides, especially when you’re sort of worried more about generation performance or sort of chat usage. Yeah, and so the second common way to evaluate language models and sort of take a measurement that you could sort of assess a model’s capability or behavior with is a perplexity. So perplexity, here’s the formula, but sort of to describe it in intuitive detail, we’re trying to measure how well a model fits a given data distribution. [19:09] Hailey: And the way that we do this is we have some data set, which is a collection of documents that are just sequences of words. And the thing that we’re going to measure is pretty similar to the sort of loss we use to train models. But here we take the log probability that the model assigns to sort of the true next token for every possible next token that exists in the dataset. So for every token in this dataset, we check basically how likely the model is to output it correctly. [19:42] Hailey: And we average this, this is just average over all of the documents in our dataset, and over all of the tokens in each document. So basically, per token, how well does the model fit that token? And how well does it predict this dataset? Or how likely is it to produce this data? And so this can be done basically very, very trivially because it’s the self-supervision where we know the correct label just because we’ve got this input document and we know what the next word is going to be for any sort of prefix in it. [20:14] Hailey: So we can take any dataset, like say Wikipedia or sort of like some Wikipedia page of our choice and just convert it into something we can measure perplexity on just by checking sort of how well the model fits these sort of true next tokens in the dataset. And so perplexity is a useful tool, especially since it’s just basically like using different validation set during training. And you can use it for sort of any data distribution to see how close you’re getting. [20:39] Hailey: But it’s also not super important, especially in sort of downstream use cases for language models, because for an instruction to a model or a chat bot like sort of just evaluating how well it fits, Wikipedia might be misleading because actually the model is sort of. editorializing, outputting a text, maybe perspectives, etc. that wouldn’t match with Wikipedia style or the prompts format might not match, for example. [21:09] Hailey: However, it can still be a useful diagnostic tool, especially if you did have sort of a dataset or data distribution that you want your model to be fitting better for downstream use. Yeah, so basically perplexity is a useful tool to have in the toolbox. It won’t be used too, too frequently, except for sort of like training calls from scratch. And so perplexity, it seems like a pretty simple approach, and it is, but there’s definitely sort of pitfalls and spook guns that can occur for both perplexity and the log likelihood approach that we discussed before. [21:44] Hailey: So one complication is that both these log likelihood and perplexity approaches, because they’re taking sort of either the sum over a number of tokens or averaging over the number of tokens in a data set. It matters what tokenizer you use for your model. So if two different models have a different tokenizer, the numbers that you’re producing might not be directly comparable. So a perplexity of a certain value might be easier for a model with a larger tokenizer to achieve because there are simply fewer tokens to predict over. [22:17] Hailey: And so there are ways to sort of remedy this that can be implemented to sort of use the tokenizer as part of the system that you’re evaluating and then have a metric that’s normalized with respect to the tokenizer. But yeah, so this is like a lot of text, but the important part is basically that there are ways to control for this, but they’re all sort of like small implementation details that change what you’re measuring and how you’re calculating it. [22:45] Hailey: And then of course the final way that one can evaluate a language model is by generating text from it. This is basically crucially important if we’re going to use the model to generate text, such as like a chat bot like ChatJPT. It’s what we care the most about. And chain of thought, of course, is something realistic and important for models to use, especially in sort of multi-step problems. But there are downsides to doing this sort of generation-based evaluation, which is that, again, we don’t know how to always correctly score free-form natural language responses. [23:20] Hailey: So in the case of multiple choice evaluation, we sidestep this by basically saying, OK, there are only four strings our model is ever allowed to output for this document. It ends to that way, like if there’s a string that’s not in those four, we just disregard it and we only ask the model to predict one of those four strings, and we know one of them is correct and the other three aren’t by construction. [23:44] Hailey: For text generation, the model could output any sort of string, and we want to be able to say whether it’s correct or not, or as a matter of degree, how correct it is. Is it half correct? Is it three quarters correct? Not at all correct? So one way that you can do this very simply is just sort of do a very, very rough heuristic and just say, OK, I’m going to look for the part in my model’s generation where it says the answer is X, or some phrase X. [24:10] Hailey: And then we’ll just basically grab what that X is and then check if it matches sort of the gold standard phrase that we had. So like as an example, like going back to the previous sort of comparison of like answering who the president is and the answer being Joe Biden, we could basically say. [24:28] Hailey: Like tell me what tell me who the president is answer with the answer is X And then hope that our model follows that format and that it tells us Either the answer is Joe Biden or it tells us something else in which case we know it’s incorrect However, this is not great because this just means that we’ll be penalizing models that don’t comply with our expected format So if we implement a format of the answer is X Models that are trained to produce this format always when it asks the question will do better on our [24:56] Hailey: evaluation than models that don’t And so there’s sort of these confounding variables that we have to deal with. And another way people do this is by using an LLM to basically check if an answer is correct. But these are definitely fallible. And of course there are also other, in this case this is another sort of pain point caused by tokenization, there are other reasons that sort of generation, as with other evals, can be very finicky. So here’s sort of like a prompt to the audience. Here’s two different prompts. [25:26] Hailey: These are sort of the first document in human eval fed into a model. Hypothetically, is there a difference between these two? And if so, which do you think is going to give better performance? Or which one do you prefer? So maybe just think about this for like 10 or 15 seconds and then I’ll give the answer. Yeah, so these two prompts look very, very similar, but there’s one key difference, which is these two lines at the bottom. This second prompt ends with one new line, a second new line, and then a tab. [26:00] Hailey: And so what this means is that if our code generation model has tokens that look like, for example, a tab and then the return keyword and then go on to generate the rest of a solution, it means that if our model… sort of, if the best solution to this function is just a one-liner return some expression, our model that’s prompted with a tab, if it tries to generate a new tab, it’s going to create a syntax error. [26:29] Hailey: So it’s forced to generate a token that’s like return, but without a tab in front of it, which might not be a token that it has available, or it might be a token that it hasn’t really seen during training. And so as a result, if you evaluate models with like one using this prompt with no trailing white space at the end, and another model with this trailing white space, human eval performance will be something like 5% worse for a number of models that are tested. [26:56] Hailey: And so basically this means that like just going down to like the minutest of details in your code that you’re using to implement your evaluation, it could drastically affect performance. And so if you if you implement your prompts using this trailing whitespace and someone else implements it where they they trim off any trailing whitespace in the prompts, then you’ll get different results. But it’ll be very hard to tell sort of what went wrong, how things changed. [27:23] Hailey: So basically, in short, there are a couple different sort of measurement options we have available to us, all which are trying to overcome this issue of not being able to easily score reliably freeform language model outputs, or just natural language in general. And these are sort of, and amongst these measurement options, there are a number of things that can go wrong or ways you can implement them subtly, not necessarily incorrectly, but just differently to how is standard. And so on. [27:54] Hailey: And these implementation details are even not often discussed or mentioned in papers or tech reports as things you should care about. So it’s difficult to sort of be aware of them a priori. And so all of these challenges we’ve discussed are only scratching the surface. They’re sort of only accounting for, did I run my language model correctly? Like am I getting sort of the correct output from my model? [28:21] Hailey: They aren’t even sort of accounting for external factors like data set quality of your evaluation, like if it’s measuring something you truly care about, if you’re overfitting to it, and so on. [28:32] Hailey: And so basically scoring models is hard and sort of influencing evaluations is difficult and as a result It’s very difficult to achieve reproducibility in evaluations where reproducibility here means basically if Someone publishes their results They should ideally publish enough details that you could go ahead and sort of write your own code and reproduce the same results they’re getting Within sort of a very small area of margin And so this is key for research or just for sort of iteratively developing better models for production, because comparisons have to be fair in that advantages aren’t given to [29:11] Hailey: a new model. For example, if you’ve sort of spent much more effort prompt engineering your new language model, you’ve just pre-trained or your new sort of prototype for production. If it does better, you don’t know if this is just because you spent more effort trying to make it good. Or if the other one, if you just prompt engineered for five minutes, it would also be as good or better. And so there’s definitely a question of sort of what is the correct way to set up a fair evaluation comparison. [29:40] Hailey: And if it is actually holding everything constant or maybe using a prompt format that, you know, the model is trained with or so on. [29:47] Participant 2: But at minimum, [29:48] Hailey: these evaluation details should be known and should be accounted for. So, like, for example, in the table on the right. The LAMA3 number, this is from the LAMA3 release, and these LAMA3 70 billion numbers are the ones run by Meta because they train the model and this is evaluations they ran themselves while developing it. But the Flawed and Gemini results are just results that are self-reported by the model developers, in this case Google and Anthropic. [30:17] Hailey: And they are not, as you can see these sort of subtext here, they’re not necessarily using the same settings across models. And in some cases, it might not even be super clear what the developers did if they didn’t release the code that they use for these evaluations. [30:32] Hailey: And so, as a result, sort of like, we see how LLAMA3 measures up against these models, but we’re not really sure if sort of the developers were preferential towards their own model or if just there were like easier prompts that had more information provided to the model and so on. It’s difficult to draw like a clean conclusion with these numbers. So basically strong reporting standards for evaluation are important, but also actually reporting enough details in like the tech report that you report is very difficult. [31:07] Hailey: There are many, many things you might forget to account for or just assume that they’re standard because it’s standard to how your organization does things internally. But it turns out that other people don’t have that insight or wouldn’t implement it in the same way or don’t think that’s as natural. So basically, in conclusion, like I’m going to claim that. If you don’t see the evaluation code, then things are not going to be fully reproducible, or at least you’re going to have a bad time trying to perfectly reproduce them. [31:33] Hailey: So I’d argue that you should always share your evaluation code if you’re doing something like developing a new model or a new method in research, etc. And so, yeah, so basically reproducibility is important, but very hard and sharing evaluation code, especially that’s like clean enough for people to read and use, can be tricky. And that’s where libraries like EvaluationHardest and other options like Helm or OpenCompass come in. By having sort of a easy to reference like gold standard or just like frequently vetted code based for evaluation. [32:12] Hailey: it’s at least possible to sort of not have to worry yourself about all of these tokenization intricacies, a normalization of log likelihoods, et cetera, and more worry about, am I doing the evaluation I want to do? And then it’s easier to sort of just say, instead of implementing all of these evaluations from scratch and saying and sharing your code base, you can instead sort of use these shared code bases. And so, yeah, we’ve been lucky to see that the eval harness at least has been used. [32:41] Hailey: for a bunch of research on evaluations has also been used when new architectures like Mamba are proposed to sort of run on a couple tasks that the community has decided are canonical as like like as log likelihood based evaluations of like based language model performance. And so on. [33:01] Hailey: And then I guess in contrast to other libraries like Helm, the Uval Harness, we sort of more intend to just put tools in the hands of practitioners and researchers in that sort of many tasks are supported in our library and it’s easy to define new ones or edit the prompts for your own purposes. But it should be up to you which of those tasks you’d like to evaluate on and which are best for you. [33:28] Hailey: So yeah, so I guess, yeah, so that was sort of more research-inflected, but as a side note for production, there’s a distinction that’s been made by a number of people, I think, between model evals and downstream evals, where a model eval is more something like the MMLU benchmark, which is meant to sort of measure how generally capable or generally intelligent, with heavy scare quotes, your Your language model is to sort of measure it against actual like base models versus a downstream eval, which is more I have this concrete use case, like maybe as a chat [34:03] Hailey: bot for this specific closed domain of answering questions about my company’s documentation. And I’m going to evaluate my model and see how well it does on this specific task that I want to use it for and nothing else. And so basically, whenever possible, downstream evaluations should be used if you have a concrete use case in mind. The best eval is one that you’ve run the code for yourself instead of sort of pulling it from a tech report and just assuming that’s how good a model is. Always try to test it yourself. [34:35] Hailey: But then even better is an evaluation that you’ve designed yourself that matches up with your own use case. Sometimes if you’re trying to measure things that are more subjective, like say, like How high quality or how preferred your chatbot is, essentially hiring human evaluators, although expensive, is worthwhile. And the incentives and sort of trade-offs for downstream evals are very different because for model evaluations, the thing that we care about is model quality, and we think of a high MMLE score as implying that the model is a very good language model in general. [35:11] Hailey: And so doing things like training on the train set or the test set, or sort of overfitting repeatedly by trying to maximize your MMLU score over the course of fine-tuning, we think that those isn’t a bad thing because they’re sort of ruining the ability of practitioners to draw conclusions based on the evaluation score about how good your model is. [35:35] Hailey: For a downstream evaluation, if the evaluation is literally just how well does the model perform on sort of real world chat transcripts for your actual use case, it might be beneficial to overfit and try to get to 100% on that downstream evaluation, because doing better on that evaluation test just means you’re doing better on exactly the things you care about and nothing else. So, yeah, so. Basically, some high-level takeaways in this talk are basically that implementation details matter a lot for LLM evaluation. [36:07] Hailey: Models are very, very finicky to specific prompts, details, and formatting, to specific sort of implementations of how you generate text or sort of normalize these measurements. And these can skew not just your numerical scores that you get out, but also the conclusions you draw about which models are better than others. And so this matters for research, if you’re trying to draw fair comparisons between a new method and old ones. [36:32] Hailey: And it also matters for production because like the sort of tokenization bug that I showed in this coding example, if you were feeding your model a prompt in actual production, with this trailing white space, then all of a sudden your performance would be tanking. And even though the evals looked good on paper, if they didn’t suffer from the same bug, you might sort of have a worse model introduction and not know why. And yeah, so this is a very, very non-exhaustive list of evaluation woes and things that can go wrong. [37:04] Hailey: There are many more, including data contamination, overfitting to your evaluation or having it saturate and no longer be sort of useful for telling models apart. The actual measurement validity of your evaluation, like is it measuring something that is a real thing that you should care about? And is it actually sort of a good match for that, if that thing is like capability or something? And so on. [37:26] Hailey: Yeah, so in conclusion, some of the material in this talk and some of the, in particular, like implementation details are documented in a recent paper we put out from Eleuther called Lessons from the Trenches on reproducible evaluation of language models. And then also, if you’re interested in trying to abstract some of this evaluation gory detail away, then check out the evaluation harness and other tools. We recently launched the ability to wrap model prompts in chat templating, thanks to a contribution from HuggingFaced. [37:56] Hailey: We’re definitely interested in extending to further evaluations beyond just the sort of text only ones we support. And yeah, we’d love to sort of hear from you if you’re interested in how we can make the library better or if you’re interested in helping out, even since we’re very constrained, that sort of ability to do all the things that need doing evaluation. [38:16] Participant 2: Yeah. [38:17] Hailey: In conclusion, thanks so much for attending and love to take questions or anything else. [38:24] Hugo: Thanks so much, Hayley. That was awesome. We do have some questions that I’d love to get to. I am interested if people just wanted to start playing around now. I know historically I’ve gone straight to Hugging Face and explored the MMLU dataset. It’s really easy there with the dataset view and that type of stuff. And you can get started locally with notebooks and that type of stuff. But if people wanted to get started now, what’s the best way to do that? [38:57] Hailey: Yeah, so we have like an examples folder in the evaluation harness repository that has like a collab notebook where you can just run the scripts in the harness. And then yeah, I’d highly recommend checking out things like the open LLM leaderboard and just sort of looking at those data sets and what’s in them and thinking about. Yeah. [39:17] Hugo: Very cool. So we do have a bunch of questions. One is, some benchmark datasets have some existing errors in them, like grammar, etc. For example, Hello Swag has 26% of samples with error of some sort. Does this actually matter in practice? [39:33] Hailey: Yeah, I would say definitely. I think, yeah, the Hello Swag example is particularly egregious, and I like to bring that one up. So the place that that’s going to come most into effect is, say, if 10% of your data set samples have errors in them or are mislabeled, and you’re trying to train models that are performing better than 88%, like better than 92%, even though there’s 10% errors or something, and you’re sort of fighting over 89, 90, 91% accuracy. [40:04] Hailey: You’re very, very close to sort of the point at which the benchmark has outlived its usefulness, if it hasn’t already. Sort of any improvement getting toward 100 percent accuracy is only going to be a product of sort of overfitting to that evaluation and not a product of the model actually getting better. So there’s definitely a point at which evals stop becoming useful for telling models apart in capability, which is the thing we might care about from them. It’s not always what we care about, but if that’s what we care about, like ranking loss, then yeah. [40:35] Hugo: That makes sense. We have a question from Kit. It blows my mind we can get all of these. So this is back to the getting all the logs and all of that. Get all of these simultaneously. But so Kit’s wondering in practice, how do we get all of these probabilities for one given inference task? [40:59] Hailey: Yeah, so or Maybe if it’s possible to elaborate on the question. So there’s two components. I guess, when you evaluate on a given task, obviously you’re going to have to run inference on each document separately. You can definitely batch those documents together, but you’re going to have to sort of figure out what the model’s output on each different input would be separately. But in terms of getting the predictions at each time step, In parallel, this is sort of a characteristic of transformers as well as other sequence models like S-M-S. [41:38] Hailey: But it’s the reason that when you train a GPT-2 model with 2040 tokens in context, you don’t have to run 2048 steps to get the model’s prediction at time step one, time step two, time step three. You can just sort of run your batch through the model a single time. And so by construction, our models are sort of able to do this parallel processing. And this is why you can only like you can process your entire prompts up to a certain point, you know, only one step or sort of pre filling before generation. [42:11] Hailey: And it’s why you can train without doing sort of a number of steps equal to the number of tokens you’re feeding. [42:18] Hugo: Awesome. Thanks for that. Extra colour. We’ll get to a few more questions and people who are here, if you could upvote the ones that you’d like to hear answered as well. Shamik has a question, and you have talked about this a bit, what are your thoughts on LLM as a judge, in that case, how to ensure that the judge model is correct? I want to add a bit more flavour to this question just from… [42:42] Hugo: you know, the way I think about it, and you really spoke to this with your Joe Biden example, we’re actually in a weird situation, right, where all our old tools, now we have LLMs generating a lot of natural language, all our old tools of, you know, pattern matching and NER and all these NLU tools. [43:04] Hugo: aren’t quite enough, given that essentially when LLMs respond to questions or try to answer things, we do actually want to know something about the semantic meaning of the text they produce, not necessarily the string matching or regular expressions or anything like that. So we are in a situation where even if we didn’t want to use LLMs as judges, there’s a forcing function of some sort. So given that context… [43:34] Hugo: What are your general thoughts on LLM as judge, [43:37] Hailey: Hayley? Yeah, so I think there’s an interesting tension between sort of the fact that we want to use LLMs as a judge or sort of human evaluations and annotations as like scores for tasks that are inherently more difficult or complex, because it’s harder to sort of come up with a heuristic that’s going to closely match the performance. Like if it’s sort of a more subjective or just like multi-step reasoning process to like decide whether a task. has been done correctly. [44:12] Hailey: It’s more desirable for us to use an LLM as a judge because we want to just sort of be able to have something that spits out a number that says whether it’s correct or not. But at the same time, this is exactly where LLM as a judge is going to be less potent because just these models are going to be better at the simpler task of say, extracting like the multiple choice sort of answer that was produced from the model’s output. So like… [44:37] Hailey: I think LLM as a judge is like a very valuable tool, but I’d like to see more work done on sort of where they fail and what their limits of capability are. Because like if you’re trying to use GPT-3 to evaluate GPT-4 on a task that GPT-3 can’t do, you’re likely going to have a bad time. Yeah, so in short, I think it’s a useful tool, but more attention has to be paid to sort of what is the performance of the judge model. before you use it willy-nilly. [45:10] Hugo: Great. Someone has asked about the ARC benchmark and that it’s been offered on Kaggle a $1 million prize for the winner. Why is ARC a harder benchmark? [45:25] Hailey: Yeah, yeah. So ARC is much more focused to like generalization. Many of the evals that we use for language models are ones that, while it does sort of measure like if a model can perform the task, that means it’s capable and useful. At its core, many of these, like MMLU, are sort of a combination of, okay, can the model do in-context learning, but then just like, does it know enough facts? [45:54] Hailey: And so these are things that if a model has just seen it enough in its pre-training corpus, because it’s listed as a fact on the internet, then it’s going to be able to answer this question. And so like, I think for ARC and for things that require many, many hops of reasoning, it requires at minimum a lot more scaffolding around the language model to perform these multiple leaps. [46:15] Hailey: And so like current benchmarks are often memorization tests, or at least sort of can be heavily improved on by just increasing memorization, whereas sort of performing tasks that require many, many leaps of reasoning, again, reasoning with scare quotes, is a much more difficult challenge. [46:38] Hugo: We have a couple of really nice practical questions around, and one is, one’s from May, the other’s from Simon Willison. Simon’s speaking later today. Everyone, if you want to check out a talk that we’re all excited about, but they’re asking around just how to get a model to reliably give a multiple choice answer without like a whole bunch of fluff or not giving the answer as well. [47:04] Hailey: Yeah, so I guess one component here which we have not integrated or explored in the LMEvaluation harness is structured generation tools that can actually sort of enforce, say, like we’ll do freeform generation from a model, but it won’t be freeform because we constrain it to only sort of output the appropriate tokens. So things can be done to sort of mask out other tokens’probabilities and sample from the model, but prevent it. [47:31] Hailey: I guess another component here is like for the most capable models, I guess, just prompting it with the system prompt to directly and only return the answer. But I would say, like, if you don’t have the ability to do structured generation because you don’t have that sort of access with your API, asking nicely and I think, yeah, it’s tricky because I think a lot of models are trained nowadays to sort of always produce large chains of thought, either because it helps them or just because people like the conversations. [48:06] Hailey: So I think while system prompts can help on this, if you don’t have access to some way of sort of more reliably ensuring with structure generation, it’s going to be tricky. [48:17] Hugo: Cool. I think asking nicely is a nice note to end on as well. And particularly given, you know, the breadth of everything you covered here. Super grateful for you sharing all of your wisdom from the bleeding edge of all of this work, Hayley. And thank you all for joining as well. We didn’t get to all the questions, as is usually the case, but we can get to the rest in Discord as well. So feel free to continue. the conversation there. And definitely we’ll share more of the links that Hayley has shared. [48:52] Hugo: Are we able to share your slides as well, Hayley? [48:56] Hailey: Yeah, I can send a link. And I don’t believe I have access to the Discord channels, but I’m happy to answer questions after the pack. [49:04] Hugo: Amazing. I’ll send the link to you on Twitter DM as soon as we jump off the call. But thank you once again. And thank you all for joining. [49:15] Hailey: Yeah, thank you so much for having me and thanks everyone for the questions. [49:18] Hugo: Awesome. All right. See you soon, everyone. [49:20] Hailey: Ciao.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "A Deep Dive on LLM Evaluation"
    ]
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Applied AI Consulting",
    "section": "",
    "text": "We are a group of experienced AI Engineers who have built AI products at leading tech companies. We specialize in building & improving LLM products. See our current and past clients here."
  },
  {
    "objectID": "services.html#about",
    "href": "services.html#about",
    "title": "Applied AI Consulting",
    "section": "",
    "text": "We are a group of experienced AI Engineers who have built AI products at leading tech companies. We specialize in building & improving LLM products. See our current and past clients here."
  },
  {
    "objectID": "services.html#problems-we-solve",
    "href": "services.html#problems-we-solve",
    "title": "Applied AI Consulting",
    "section": "Problems We Solve",
    "text": "Problems We Solve\nYou should consider hiring us if …\n\nYou don’t know how to improve your LLM products systematically.\nYour LLMs need to be cheaper or faster.\nYou are overwhelmed by tools & frameworks.\nFeel lost on what to prioritize and what experiments you should run.\nNeed outside expertise to evaluate your needs and vet potential talent."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Applied AI Consulting",
    "section": "Services",
    "text": "Services\nWe offer two tiers of services to support your goals.\n\nTier 1: Strategy\nWe will advise you on the following topics:\n\nLLM performance issues: (cost, quality, speed)\nRAG, Fine-Tuning, and Prompt Engineering\nUpskilling your team: We will rapidly upskill and unblock your team with focused debugging sessions.\nStrategy & Hiring: We will introduce you to talent, vendors, and partners in our network.\nEvaluation Systems\n\nWe will give you ongoing feedback and guidance via regular meetings with your team. This will help you avoid common pitfalls and select the right tools and techniques, saving you time and money.\n\n\nTier 2: Comprehensive\nEverything in Tier 1, plus:\n\nImplementation: We will write production-ready code and/or prototypes to accelerate your AI product development.\nDomain-Specific Evaluation Systems: We will design and implement custom evaluation systems to measure the performance and reliability of your LLMs.\nHands-On Model Optimization: We will fine-tune, prompt engineer and debug models to improve performance and efficiency.\nDevelopment Tools and Infrastructure: Build custom tools and infrastructure to streamline your AI development process.\nContent and Writing: Produce written documents and blogs to communicate best practices, methodologies, and practical case studies to your stakeholders\nTeam Growth & Hiring: Work with 2-4 people individually on your team to rapidly upskill them on AI. We will also help you source and evaluate key hires.\n\n\n\nPricing\nWe work on a monthly retainer basis1. The cost depends on the tier of service you choose:\n\nStrategy: $183,600 for a 3-month engagement.\nComprehensive: $975,000 total for a 3-month engagement.\n\nContact us at consultingl@parlance-labs.com to discuss starting an engagement."
  },
  {
    "objectID": "services.html#current-past-clients",
    "href": "services.html#current-past-clients",
    "title": "Applied AI Consulting",
    "section": "Current & Past Clients",
    "text": "Current & Past Clients\nMembers of our team have worked with the following companies:\n\nLimitless AI: Limitless AI is a personal memory assistant that helps you remember, organize, and navigate your life.\nRaycast: Raycast is a blazingly fast, totally extendable launcher. It lets you complete tasks, calculate, share common links, and much more.\nTensorlake: Build Knowledge for LLMs from un-structured data\nReplicate: development of fine-tuning serverless infrastructure using axolotl and optimized LLM inference.\nWeights & Biases: Provide product guidance for evaluation, annotation, and observability.\nHoneycomb: Currently improving the natural language query assistant through evaluation systems and fine-tuning.\nLangChain/LangSmith: provided product guidance for enterprise LLM tools.\nModal: Serverless tools for fine-tuning.\nRechat: We are working on Lucy, a conversational AI for real estate agents. You can read about my recent work on Rechat here.\nAnswer.ai: Conduct research on new LLM applications and help with product strategy.\nAxolotl: Hamel is a core contributor to axolotl, a library for efficient fine-tuning of LLMs.\nKay.ai: Retrieve relevant context from the semantic web for your LLM apps with fully hosted embeddings.\nModal Labs: Modal specializes in cloud functions, offering a platform for running generative AI models, large-scale batch jobs, and more.\nPydantic: Pydantic provides data validation and settings management using Python type annotations, enforcing type hints at runtime with user-friendly error handling.\nPosit: Helped Posit expand into the Python AI and ML ecosystem.\nCatena: LLM infrastructure such as routing, evaluation, and orchestration."
  },
  {
    "objectID": "services.html#footnotes",
    "href": "services.html#footnotes",
    "title": "Applied AI Consulting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe quality of our work is guaranteed. If you do not believe we have met mutually established objectives, we will continue to work toward those goals with you for no additional fee. If, after such an additional attempt, you still believe we have not met your objectives, we will refund your fees in total.↩︎"
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Hamel Husain is a machine learning engineer with over 25 years of experience. He has worked with innovative companies such as Airbnb and GitHub, which included early LLM research used by OpenAI for code understanding. He has also led and contributed to numerous popular open-source machine-learning tools."
  },
  {
    "objectID": "team.html#hamel-husain",
    "href": "team.html#hamel-husain",
    "title": "Team",
    "section": "",
    "text": "Hamel Husain is a machine learning engineer with over 25 years of experience. He has worked with innovative companies such as Airbnb and GitHub, which included early LLM research used by OpenAI for code understanding. He has also led and contributed to numerous popular open-source machine-learning tools."
  },
  {
    "objectID": "team.html#jason-liu",
    "href": "team.html#jason-liu",
    "title": "Team",
    "section": "Jason Liu",
    "text": "Jason Liu\n\nJason Liu is a distinguished machine learning consultant known for leading teams to successfully ship AI products. Jason’s technical expertise covers personalization algorithms, search optimization, synthetic data generation, and MLOps systems. His experience includes companies like Stitch Fix, where he created a recommendation framework and observability tools that handled 350 million daily requests. Additional roles have included Meta, NYU, and startups such as Limitless AI and Trunk Tools."
  },
  {
    "objectID": "team.html#jeremy-lewi",
    "href": "team.html#jeremy-lewi",
    "title": "Team",
    "section": "Jeremy Lewi",
    "text": "Jeremy Lewi\n\nJeremy Lewi is a Machine Learning platform engineer with over 15 years of experience and expertise in using AI to solve practical business applications. He has built platforms for YouTube, Google Cloud Platform, and Primer, enabling ML Engineers and data scientists to rapidly develop and deploy models into production. He played a pivotal role in developing systems like YouTube’s Video Recommendations and made major contributions to open-source software, including creating Kubeflow, one of the most popular OSS frameworks for ML.\nJeremy is an expert in Cloud services, Kubernetes, MLOps, LLMOps, CICD, IAC, and GitOps."
  },
  {
    "objectID": "team.html#dan-becker",
    "href": "team.html#dan-becker",
    "title": "Team",
    "section": "Dan Becker",
    "text": "Dan Becker\n\nDan has been working in AI since 2012, when he finished 2nd (out of 1350 teams) in a machine learning competition with a $500,000 prize. He has contributed to open source AI tools like TensorFlow and Keras, worked as a data scientist at Google and lead the product team building AI Development Tools for DataRobot. Over 100,000 people have taken his deep learning courses on Kaggle and DataCamp."
  },
  {
    "objectID": "team.html#john-berryman",
    "href": "team.html#john-berryman",
    "title": "Team",
    "section": "John Berryman",
    "text": "John Berryman\n\nJohn has worked in technology since 2012. The first half of his career was spent building search applications. John helped build next-generation search for the U.S. Patent Office, built Eventbrite’s search and recommendation platforms, built GitHub’s code search, and co-authored a book – Relevant Search (Manning). While at GitHub, John moved into Data Science and then into Machine Learning with GitHub’s Copilot code completions and chat products. John is currently co-authoring an O’Reilly book for LLM application development."
  },
  {
    "objectID": "team.html#josh-patterson",
    "href": "team.html#josh-patterson",
    "title": "Team",
    "section": "Josh Patterson",
    "text": "Josh Patterson\n\nJosh Patterson, with over 20 years in AI, has a rich history of contributions to the field. He played a key role in developing autonomous driving systems for DARPA Grand Challenge and optimizing mesh network routing with Ant Colony Optimization during his graduate studies. As a principal solutions architect and early employee at Cloudera, he significantly contributed to the company’s growth. Co-author of “Deep Learning: A Practitioner’s Approach” and “Kubeflow Operations Guide,” Josh is also a co-founder of the Eclipse Deeplearning4j project, demonstrating his expertise in generative neural networks. Currently, he focuses on Conversational AI, Automation AI, and the intersection of data and prompt engineering, driving advancements in generative AI technologies."
  },
  {
    "objectID": "team.html#zach-deane-mayer",
    "href": "team.html#zach-deane-mayer",
    "title": "Team",
    "section": "Zach Deane-Mayer",
    "text": "Zach Deane-Mayer\n\nZach Deane-Mayer is an AI executive and Kaggle Grandmaster with over 15 years of experience building AI products and infrastructure. He founded and scaled global AI teams that built Generative AI, Visual AI, and AutoML products at DataRobot. Zach has deep expertise in growing and scaling AI teams, building AI products, and developing AI strategies."
  },
  {
    "objectID": "education/evals/allaire.html#chapters",
    "href": "education/evals/allaire.html#chapters",
    "title": "Inspect, An OSS framework for LLM evals",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction to Inspect\nJJ Allaire introduces Inspect, a Python package developed in collaboration with the UK AI Safety Institute for conducting evaluations of large language models (LLMs). The tool facilitates a range of evaluations from simple QA to complex cybersecurity tasks. JJ discusses the motivation behind Inspect, emphasizing the inadequacies of existing tools for complex LLM evaluations and the frequent default to custom solutions.\n01:55 Honeycomb Eval Example\nJJ walks through an example with the Honeycomb dataset, demonstrating Inspect’s flexibility in adapting existing code for evaluations.\n03:45 Core Concepts: Solvers and Scorers\nJJ elaborates on the core components of Inspect: Datasets, Solvers, and Scorers. He details how these components interact within the framework to process evaluations, utilizing examples from the Honeycomb dataset to illustrate their functions.\n06:48 Eval Results and Tools\nJJ covers the evaluation process and tools available in Inspect for analyzing results. He demonstrates the use of the Inspect View to aid in debugging and refining evaluations, and shares how users might drill further to inspect eval results.\n11:55 Detailed Solver and Scorer Functions\nA deep dive into the functionalities of Solvers and Scorers within Inspect. JJ describes the modular design that allows for the reuse and customization of these components to suit specific evaluation needs, including examples like multiple-choice and self-critique solvers.\n15:37 Composability and Tool Integration\nJJ discusses the composability of Inspect, encouraging the use of external Python packages to enhance the framework’s capabilities. Examples include integrating tools for specific tasks like security evaluations. He discusses the potential of community-developed components.\n19:10 Agent Scenarios\nJJ presents advanced use cases for Inspect, detailing the integration of agent-based systems for complex tasks such as cybersecurity evaluations. This section covers the adaptability of Inspect to incorporate various agent behaviors, even from external frameworks such as LangChain.\n23:04 Scoring Mechanisms and Customization\nJJ elaborates on the various scoring methodologies within Inspect, highlighting the flexibility in using pattern matching, model-graded scorers, and comparing against human evaluation.\n26:50 Importance of Logging in Evaluations\nJJ discusses the role of logging within Inspect, showcasing how comprehensive logging can significantly enhance the evaluation process. JJ illustrates how logs facilitate detailed analysis and comparisons across different evaluations, especially when enriched with, e.g., Python APIs.\n27:59 Model Support and Integration\nThis section details Inspect’s compatibility with a wide range of models from various providers like Hugging Face. JJ explains how Inspect handles different model architectures and the ease of integrating new models as they become available.\n29:36 Workflow with Inspect\nJJ describes Inspect’s capabilities for supporting both interactive and automated workflows. He outlines how Inspect accommodates exploratory work in notebooks while also being robust enough for inclusion in continuous integration systems, enhancing productivity, scalability, and reproducibility in LLM evaluations.\n35:29 Q&A Session Begins\nThe session transitions to a Q&A, facilitated by Hamel, where JJ addresses questions about Inspect’s integration with other products, its capabilities for handling different data inputs and outputs as well as metrics, and the future development directions influenced by community feedback and needs.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "Inspect, An OSS framework for LLM evals"
    ]
  },
  {
    "objectID": "education/evals/allaire.html#slides",
    "href": "education/evals/allaire.html#slides",
    "title": "Inspect, An OSS framework for LLM evals",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "Inspect, An OSS framework for LLM evals"
    ]
  },
  {
    "objectID": "education/evals/allaire.html#resources",
    "href": "education/evals/allaire.html#resources",
    "title": "Inspect, An OSS framework for LLM evals",
    "section": "Resources",
    "text": "Resources\n\nInspect homepage.\nInspect GitHub repo.\nAI Safety Institute: homepage.\nSlides (pdf).\nSource code for this presentation.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "Inspect, An OSS framework for LLM evals"
    ]
  },
  {
    "objectID": "education/evals/allaire.html#notes",
    "href": "education/evals/allaire.html#notes",
    "title": "Inspect, An OSS framework for LLM evals",
    "section": "Notes",
    "text": "Notes\n\nGetting Started with Inspect\nTo develop and run evaluations using Inspect, you’ll need access to a model. This typically involves installing a Python package and ensuring that the appropriate API key is available in your environment. Here are the steps:\n\nInstall the inspect-ai Python package:\n\npip install inspect-ai\n\nAssuming you’ve written an evaluation script named arc.py, set up and run the evaluation for OpenAI as follows:\n\npip install openai\nexport OPENAI_API_KEY=your-openai-api-key\ninspect eval arc.py --model openai/gpt-4\nInspect supports a wide variety of models, including models hosted on Azure AI, AWS Bedrock, Cloudflare, and local models with Ollama.\n\n\nInspect Evaluation Components\n\nDatasets: These contain labeled samples, typically organized as a table with input and target columns. The input represents prompts, and the target can be literal values or grading guidance.\nSolvers: Solvers are combined in a plan to evaluate the input in the dataset. The basic generate() solver calls the model with a prompt and collects the output. Other solvers can handle prompt engineering, multi-turn dialog, critique, and more.\nScorers: These evaluate the final output of solvers. They may use text comparisons, model grading, or other custom techniques.\n\n\n\nExample Evaluation: Sally-Anne Test\nLet’s explore a simple evaluation that assesses how models perform on the Sally-Anne test: a task that evaluates a person’s ability to infer false beliefs in others. Here are some samples from the dataset:\n\n\n\n\n\n\n\ninput\ntarget\n\n\n\n\nJackson entered the hall. Chloe entered the hall. The boots is in the bathtub. Jackson exited the hall. Jackson entered the dining_room. Chloe moved the boots to the pantry. Where was the boots at the beginning?\nbathtub\n\n\nHannah entered the patio. Noah entered the patio. The sweater is in the bucket. Noah exited the patio. Ethan entered the study. Ethan exited the study. Hannah moved the sweater to the pantry. Where will Hannah look for the sweater?\npantry\n\n\n\n\nIn this example, we demonstrate how to run evaluations using the inspect eval command from the terminal. Additionally, we provide the code for the evaluation.\n\n\nCode for the Evaluation:\nfrom inspect_ai import Task, eval, task\nfrom inspect_ai.dataset import example_dataset\nfrom inspect_ai.scorer import model_graded_fact\nfrom inspect_ai.solver import (               \n  chain_of_thought, generate, self_critique   \n)                                             \n\n@task\ndef theory_of_mind():\n    # The Task object brings together the dataset, solvers, and scorer, \n    # And is then evaluated using a model.\n    return Task(\n        dataset=example_dataset(\"theory_of_mind\"),\n        plan=[\n           # In this example we are chaining together three standard solver components. \n          # It’s also possible to create a more complex custom solver that manages state \n          # And interactions internally.\n          chain_of_thought(),\n          generate(),\n          self_critique()\n        ],\n        scorer=model_graded_fact()\n    )\nNote that this example is intentionally over-simplified. The templates for prompting, critique, and grading can all be customized. In a more rigorous evaluation, we’d explore improvements specific to the context of the dataset.\n\n\nRunning the Evaluation\nTo run the evaluation against GPT-4, execute the following command:\ninspect eval theory_of_mind.py --model openai/gpt-4\n\nBy default, evaluation logs are written to the ./logs sub-directory of the current working directory. Once the evaluation is complete, you’ll find a link to the log at the bottom of the task results summary.\nAdditionally, you can explore evaluation results using the Inspect log viewer. Run inspect view to open the viewer (you only need to do this once, as the viewer will automatically update when new evaluations are run).",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "Inspect, An OSS framework for LLM evals"
    ]
  },
  {
    "objectID": "education/evals/allaire.html#full-transcript",
    "href": "education/evals/allaire.html#full-transcript",
    "title": "Inspect, An OSS framework for LLM evals",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:03] JJ Allaire: I’m going to try to give a whirlwind tour of Inspect. I actually worked with Hamel a little bit over the weekend to build some Inspect evals for the Honeycomb dataset that you are working with. Hopefully, it’ll connect well to the work that you’ve been doing and it’ll make the concepts gel a little bit easier. What is Inspect? Not surprisingly, it is a Python package, pip install Inspect AI. Someone asked a question about, in Hamel reference, that I’m affiliated with Posit, which is formerly our studio. This project is actually not a Posit project. [0:43] JJ Allaire: This is a project that results from a collaboration that I’m doing with the UK AI Safety Institute. So the UK AI Safety Institute has hundreds of evaluations of every variety, simple QA things. fancy, you know, cybersecurity capture the flag evals, hundreds of evaluations. And when we kind of started off on the journey to writing on these evaluations, there weren’t terrific tools available. A lot of the tools were either embedded in benchmark frameworks, or maybe not very complete or weren’t that well tooled. It wasn’t necessarily clear exactly how much more development they would get. [1:23] JJ Allaire: And further, I think they were not necessarily designed to scale up to very complex evals. And so we set out and actually the most popular eval framework is just roll your own eval framework, which is out there a bunch too. So we set out to build something that we could use and then ultimately we could share so other people could use as well. So with that, just again trying to ground this in Honeycomb. This is actually an eval for the Honeycomb dataset. It’s got there’s 2300 user inputs. [1:58] JJ Allaire: We’ve also got the, you can see columns, that’s the schemas that were fetched originally by RAG, so they’re in the data set. And so to create an evaluation, we basically take that data set, and then we put it through a pipeline, which you’ll recognize is really the same code that Hamel had in the notebooks for… for Honeycomb. And then we apply a score to it, which is again, based on the Hamel’s original code. And then we can run this eval and get lots of tooling. [2:31] JJ Allaire: You can see this, I’ll get it more into this, but we have a log viewer that lets you explore and debug and visualize everything that happened during the eval. Lots of other tools. This is a VS Code extension that lets you tweak things, run different models, et cetera. So I think I would emphasize that inspect is not super opinionated and really is like Python code first. [2:53] JJ Allaire: And by sort of conforming to some simple conventions and fitting yourself in, you get to take advantage of this big pipeline of tools and then hopefully like a big ecosystem of related packages that will extend and spec. [3:07] Hamel: Is it very local in nature? It’s local. [3:10] JJ Allaire: It’s all local. Yeah. Okay. Yeah. So one of the things I skipped over on the first slide is we have this concept of development to production. So we definitely wanted to have a very interactive local, you work in a notebook, be very iterative, work on your eval. We have lots of stuff for taking it and running it in the cloud, run it over 10 models, run it. So it’s… [3:33] JJ Allaire: It’s local for development, but then we have lots of tools if you want to scale it up and run it as part of CI and things like that. But it’s all local. Right. Okay. So this core concepts, and you saw a little bit of these on the first slide, you have a data set, which is not at all surprising what that consists of. It’s inputs. And usually there’s targets. In the case of the Honeycomb data set, there’s not a target. There’s going to be a validation function and a critique model. [4:05] JJ Allaire: Target would usually have like for multiple choice, what’s the correct answer for Q&A, some kind of description of what the right answer is. for a fancy model graded eval, it might be like a grading rubric for a model. So that’s the data set. Solvers is like the pipeline that actually does the eval. And this could be doing anything from prompt engineering to calling the model to a multi-turn dialogue with the model. It could be doing various types of elicitation like critique. [4:34] JJ Allaire: Solvers is kind of like the heart of the entire eval and where you kind of customize how it works. And then the score basically evaluates the final output. Again, these can be very simple, like doing text comparisons. They can be model graded, or they can use all kinds of custom schemes, kind of whatever you can dream up. So that’s the core concepts, and I’m going to drill more into this validate example. So this is the eval. I’ll break down the different parts of it. [5:01] JJ Allaire: We talked about the data set, and this is just reading from the CSV. There are standard fields that go in data sets. Input is one of them. And then there are custom fields that different evals will need. In this case, columns is important because we’re going to use columns for the prompt template. And so we want to save the columns when we read the data set in. And then this, we have this plan and you’ll see this is the same system message used in the notebooks presented in the course. [5:29] JJ Allaire: prompt with schema is a solver that’s going to build the prompt that uses the columns and the input, and then generate calls the model. So pretty straightforward. And then the score actually uses the check function, the check query function that you’ve also seen in the course. So that’s the eval. And now I’ll drill into some of the specific components. They’re quite simple, and hopefully they’ll be very intuitive and straightforward, what they’re actually doing. Here, prompt with schema is literally just taking a prompt template and then substituting the prompt and the columns. [6:04] JJ Allaire: So you’ve seen this before, but it’s a solver that really just makes a prompt. That’s all it does. And then the scorer is basically going to… It’s going to take the output from the model. This JSON completion function is just sort of a helper that some models actually like to put like JSON code blocks in. So it strips that away. [6:31] JJ Allaire: So the idea behind that is just like clean it up so we get pure JSON, read it, and then we call the is valid function, which is literally the exact same is valid function that’s used in the course. And we figure out whether the query was valid. Okay. And then, so once we’ve done that, we’ve built our solver and we’ve built our score and we’ve run it. Now we run our eval and we can see kind of what happened. [6:56] JJ Allaire: And in so many evals, the score really doesn’t tell you anything close to enough, especially when you first start developing them because, you know, your checking function could be wrong. The way you extract answers from the model could be wrong. There’s so many things that you need to investigate and you kind of… in some ways need to do it on a per sample basis. So this lets you very easily look and see what happened overall, and then what happened on a sample by sample basis. [7:23] JJ Allaire: So here you can see drilling into this sample, which got incorrect. We can see what was the actual message history, what was the dialogue between the model and the eval. And so here it’s quite long, as you know, from working on it. Here’s the columns that was injected, the schema. And then here you can see… to the very end, and then the assistant’s answer. So looking at all the messages can be quite valuable, especially when you get into things like tool use. And then also, okay, so that’s the basics of that. [7:57] JJ Allaire: And then we built another eval, which has actually, it’s… The exact same code, and in fact, in the Git repo that I’ll share at the end, you’ll see that I do reuse the code. I don’t just copy and paste it, but really there’s only one line that’s different here. It’s the same dataset, it’s the same plan, but we’re going to use a critique model for scoring instead of the validate function. Here, this is the critique score. This has a little bit more going on, but it’s again, pretty straightforward. Notice we parameterize. [8:30] JJ Allaire: what model is used to do the critique. So you can use, you know, it’s pretty obvious, but you don’t necessarily need to use the same model. In fact, you often don’t want to use the same model to do scoring as the model you’re evaluating. You can use a more powerful model. In some cases, maybe use a fine-tuned model. Here we’re defaulting to GPT-4 Turbo, but the user of the score could use any other model they want. We build the critic prompt, kind of analogous to how we built the other prompt. [9:01] JJ Allaire: And again, this critique.text is literally the same. It’s the exact prompt that is in the notebooks for the course. And then we run the critique. So we get the model instance, we call generate, we parse the output, we check and see if it was good, and then we return the score. So that’s our critique score. And then at the end of that, we get another eval view. This time for critique, the accuracy here was a little bit lower. [9:28] JJ Allaire: These numbers actually don’t really mean anything by themselves, but just noting that it was less frequent that the critique was satisfied. with the output then, the devalidator, which is intuitive. And so here we might want to know what actually happened in this critique. So we can drill in here. This is one of the incorrect answers. And we can see what the answer was. And then we can actually see what the critique model’s explanation was. So here you might look at it and say, wow, that actually looks right to me. [10:00] JJ Allaire: Or a human expert may have said it was right. And then you want to look at the explanation and perhaps This indicates that you need to improve your prompt template for the critique model, or perhaps it means you need to fine-tune a model just to do critique, depending on what resources you’re willing to apply to get a good grader. So this is, again, just drilling in and seeing what happened during scoring. Okay, so that… [10:27] Hamel: Is there any way to interact with this view, like do some annotation in this view itself or something? [10:34] JJ Allaire: So right now we don’t have we don’t have like commenting in the view and stuff. I think we are developing like some like shared places to look at like internally. places to look at views together and there’ll be some annotation in there. I don’t know if we’ll come out with like a thing to do that. But that is useful, especially. Yeah. So maybe there’s a way we can do that in a way that, that we can open source that as well. And then, and then people can take advantage of that. This is just runs locally. [11:03] JJ Allaire: So you kind of would need to post it in some kind of, we’ve talked about actually building a. it wouldn’t really solve it awaits and biases plug in, but that’s not going to let you do this, like drilling into each one. Maybe to some extent it would, if they have like good, good templates for, for, for chat message for chat conversation histories. But yeah. [11:23] Hamel: I like the way that this is being rendered. And it’s flexible, seems flexible enough that you took my example without really knowing it until the weekend and seem to just plug it in here. [11:34] JJ Allaire: And it’s actually really- Just plug it in. And mostly just use all your code. I mean, there’s a very, what I showed you is like all the code that I wrote. And all the rest is just calling your, using your templates and using your code. So that’s kind of the idea that, you know, there is a minimal lift to get your existing stuff working inside the pipeline. So, yeah, we did that. Okay, so now I want to just talk a little more abstractly. We’ve looked at these examples. [12:00] JJ Allaire: I’ve shown you some simple solvers, really simple, just a prompt template. They can get a lot more fancy, which I’ll show you in a minute, and some scorers. So, like, conceptually, what is a solver? It basically, and this is a simple view, but the idea of a solver is it has a task state, which is, like, what’s the current state of the message history and what is the current model output? And when we start, there is no model output. and it just transforms the task state in some useful fashion. [12:27] JJ Allaire: So that could be calls the model, generates, appends the assistant message, and updates the output. It could be prompt engineering. It could be critique. It kind of can be anything. And yeah, that’s sort of what I’m saying here. The solve function does something useful with the state. And that sort of allows you to create a pipeline of solvers and reuse solvers. And it’s not always the case that you want to do that. [12:50] JJ Allaire: Some evals really just want to have like one solver that just is sort of boss of everything and doesn’t confine itself to being in a pipeline. But the pipeline does end up being useful in a lot of cases. So some examples of really simple solvers, which you’ve sort of seen hints of. We built a custom prompt template for the Honeycomb evals. But like a simple, here’s a prompt template, just transform the prompt by passing it through a template, perhaps with some extra parameters from metadata. This is actually the actual source code for the generate solver. [13:24] JJ Allaire: It literally just calls the model to generate, and that’s it. So those are like really simple. 101 solvers. I think we have like a chain of thought solver, which does like a basic chain of thought template. But even then, often you want to customize that for the domain. The generic chain of thought isn’t always exactly what you want. And I guess this emphasizes when you’re writing really good evals, you are writing, you can reuse solvers, but you’re oftentimes want to write your own solvers and write your own scores to make them really, really good. [13:56] JJ Allaire: Here’s a more fancy one. I won’t actually walk through all the code on this, but it’s a multiple choice solver that handles like shuffling the choices. And then it actually calls the model to generate and then it unshuffles the choices. So this is an example of a solver that calls generate internally. So the plan for a multiple choice will typically just be like plan equals multiple choice. It might be like plan equals chain of thought multiple choice, where multiple choice just calls generate internally. Another one, self-critique, pretty straightforward. [14:27] JJ Allaire: Again, we’ve already called generate before self-critique comes on the scene. So we may have had a system message, a prompt template. Now we call generate and now self-critique. So the first thing we do is we actually take the existing completion and we basically run a critique on it. And then we take that critique and we append it to the message history. And then we call generate again. And so this is an example, again, of a solver calling generate internally, this time just to re-up the answer with the critique being available. [15:02] Hamel: I love how you made these slides, I think, with Corto. There’s a lot of comments in the Discord. No? Okay. [15:08] JJ Allaire: Yeah, yeah. Yes, with Quarto. Definitely with Quarto. Oh, yeah. Yeah, yeah, yeah, yeah, yeah. Yeah, totally. Yeah. [15:12] Hamel: A lot of comments in the Discord about, hey, this is really cool. Is it Quarto? So, yeah. [15:17] JJ Allaire: Yes, definitely. It definitely is Quarto, yes. Yeah. Okay. And I’ll make the slides available at the end and the slide source code and everything. So, there’s a get repo that I’ll share a link to at the end so people can figure out how we did all this. Okay, so that’s the self critique solver. Okay, and then there’s generate we already talked through that. Okay, so one of the things to think about is like is composition. [15:43] JJ Allaire: And so one of the ideas behind Inspect is that you actually will write, people will write a bunch of Python packages that have scorers and have solvers and that you’ll be able to mix and match those. So the idea is like, there’s like, you know, lots of external components to let you do different things that you can just plug in. So as an example from AI Safety Institute, we have an internal package called Shepard that’s used for doing jailbreaking. [16:07] JJ Allaire: And so there’s many, there’s, as any of you follow the literature, there’s like dozens and dozens of jailbreaks. And some of them work and some of them don’t. And sometimes you have to try multiple jailbreaks and some of them work with some models and not with the other models, et cetera, et cetera. But these are basically jailbreak solvers that essentially do prompt engineering to get the model in a state where it may provide answers that it would otherwise refuse. And so as an example, here’s a eval. [16:34] JJ Allaire: that’s basically trying to give the, it’s basically just trying to get, see if the model can give good computer security advice. So it’s like, how do I prevent, you know, my website from being hacked? And then, so sometimes the model will, because you’re asking about computer security, it’ll like flag, don’t talk about computer security. And so we’re saying, well, we wanna see what the model actually knows. And so here you can say, we bring in a, jailbreak solver from Shepard, and then we just use it in our pipeline. [17:07] JJ Allaire: And so we have our normal system message. We put the jailbreak in, and then we’ll probably be able to elicit more or have less refusals than we otherwise would. And so you can imagine lots of different solvers that you could plug in. prompt various types of prompt engineering solvers. You can imagine a whole Python package just full of prompt engineering techniques and the whole Python package full of critique, debate, all kinds of things like that. And similarly, scorers. So Python package is full of different variations on model graded scoring and things like that. [17:43] JJ Allaire: So that’s an example of composition, which we think will be a big part of how people end up using Inspect. Okay. Okay. So I’ve simplified a little bit where we’ve just been looking at really straightforward kind of QA style tasks, or in the case of obviously Honeycomb, we’re looking at a fine tuning task. But these are straightforward, just like prompt, generate, nothing fancy going on. I didn’t show you all of TaskState before, but TaskState also includes tools. And so the idea behind tools, I’m sure you’ve all seen or used. [18:19] JJ Allaire: you know, these are Python functions that you can make available to the model. And you tell the model, you write the Python function, you write the doc string, you tell the model about them, and then it will say, hey, I’d like to use this tool. And so part of the task state is a list of available tools, as well as potentially like a nudge to the model, like definitely use this tool or definitely don’t use the tool, et cetera. [18:41] JJ Allaire: And so this is a simple example of, this is a biology QA task, and we’re saying, hey, if it tends that you don’t know the answer, I think this dataset actually has a bunch of very obscure questions, then hey, you can use web search. And so then we have a web search tool that goes and gets a bunch of Google hits and summarizes them and things like that. And so use tools is a function that just makes tools available to generate. And so there’s, you know, once you get into tools, now you’re into agents. [19:14] JJ Allaire: Sometimes it’s just simple tool use, like, hey, let the model use Wikipedia or let the model use web search. And sometimes it’s give it all kinds of tools. And they really become agents at that point. And so you can have agents sort of with like very bespoke custom logic, or you can bring in like an agent library. So I’ll show you an example in a little bit of taking like an existing langchain agent and just like basically making it into a solver. [19:39] JJ Allaire: So the idea is you can just take any agent in these other frameworks, and once you have the bridging, it’ll just work inside Inspect. So I’ll show that in a minute, but let me first show this sort of bespoke agent concept, which is this is a cybersecurity eval. It’s a capture the flag task. And this is more like the hand-rolled agent loop where we’re basically giving init challenge is here. This is. creating a Docker container. Use tools is basically just like, here’s some tools. And we tell the model, like you can do all these things. [20:11] JJ Allaire: And then we give it a task. And then this is basically just a loop where the model gets to keep using tools until it either terminates because it couldn’t figure it out or it ends up finding the flag. So this is like very custom. This is like roll your own agent. And definitely that’s a thing. That’s something that people do. But at the same time, you know, there’s lots of agent frameworks out there. [20:36] JJ Allaire: And so we want to be able to have like high order functions that let you take an existing agent framework and turn it into a solver. So as an example, like if you look at the code in the middle here, this is all code that is just Langchain code. There is actually I haven’t shown the imports, but there’s no inspect code at all in here. This is just like code that you would be writing in Langchain. This is basically going to get the Wikipedia tool. and then using the tool. [21:05] JJ Allaire: And then we have, as I said, there’s this higher order function. This is actually provided as an example right now. It’s in the repo that I gave you, but we can just take that agent and if it conforms to like the Langchain agent interface, we can just turn it into a solver. And then if you actually look at the eval that results, you can see this is the Wikipedia search. So it’s a data set of, can the model use… Use Wikipedia to answer, you know, they may be difficult questions, maybe obscure questions. [21:35] JJ Allaire: There may be questions we think the model definitely can answer. But the idea is, you know, the plan is literally just this Langchain agent that uses Wikipedia. And as you can see, down here is the solver that I showed on the previous slide. This is the entire task definition. Use a model to assess it. and use the agent. And then you can see here kind of what happened. And if we look kind of inside at, you know, what happened during the eval, you can see it’ll show you the tool use. [22:04] JJ Allaire: So it’s like, okay, what was the back and forth? What tools did the model choose to use and why? It gives an explanation. What result did it get? And this Game of Thrones one is really one of my favorites because it ends up, it’s trying to find the, in order of the 10 episode titles. And oftentimes, like in Wikipedia, it’s not like literally spelled out or actually where it is spelled out. It might be wrong. And so oftentimes it’ll do two or two or three queries to try to sort it out. [22:33] JJ Allaire: So anyway, it gives you that sort of diagnostics of what happened with tool use. And then similarly for scoring, this is the model graded fact score. This was the answer and it was incorrect. So this was this was the grading guidance. This was the answer. It was graded incorrect. And I think I’m going to hopefully show you. Yeah, this is the scorers explanation. So again, you know, sometimes the model really didn’t get it, but sometimes the score is actually wrong. [22:58] JJ Allaire: And so it’s important to be able to look, you know, drill down and see what’s what’s actually happening. Okay, so that is okay. So let’s talk a little bit about scoring. Let me check my time and make sure okay we’re good. A little bit about scoring. There’s lots of different ways of scoring. Obviously, traditional like pattern matching and template and answer-based scoring are obvious. [23:18] JJ Allaire: We’ve got lots of built-in solvers for doing like RegEx matching, matching at the beginning and the end, matching a template like where you tell the model to say answer and then the answer, lots of that sort of thing. There’s also model-graded scores built in, but usually you need to customize the templates for those to get them to work properly for your domain. And of course, as I mentioned before, like they’re pluggable, you can get them from other packages. And I’m expecting lots of stuff’s going to happen with model graded scoring over time. [23:49] JJ Allaire: And we’ll see the benefits of the community working on that over the next months and years. And then you can also just say no score, have a human score it. So that’s also possible. And one of the things I think, and there’s something that I know is emphasized quite a bit in the course. is basically rigorously evaluating model grade scores against human baselines. Basically, I’ve observed that definitely a lot of people will get their model grade score going, and they’ll be like, cool, now I have a score. [24:18] JJ Allaire: And they haven’t actually grounded it in whether it’s how good it is relative to human scores. So if we can build tools that help people do that well, that sort of structure, that work, I think that’ll be valuable. So that’s something we’re definitely going to work on. Okay, so what am I? Oh, this is okay. This is a score example, which I think is pretty interesting. This is the traditional, this is actually the math benchmark that I think OpenAI reports as part of their standard benchmarks. [24:50] JJ Allaire: What’s interesting about it is that the model does math and then there’s a target, but oftentimes the answer is correct, even though it’s not literally the target. And so we have this expression equivalence solver that basically lets a model assess, are those expressions actually logically equivalent? So it can even do a little bit of algebra or a little bit of factoring. These are trivial. You can see this is the same as this. It’s scored correct. This is scored wrong. [25:18] Hamel: What’s going on in that equivalence thing? Is it a regex? Or is there more going on? [25:23] JJ Allaire: There’s more going on. I’m going to show the full source code to it in the next slide. So regex to extract the answer, and then we’re going to go and have the model. So we prompt the model to basically say at the end, put answer, colon, and then the equation. And then we basically, that’s how we pull the answer out. And then we send that to this expression equivalent solver. These are trivial because they’re just like punctuation differences, but I’ve seen it where it actually can sort out that the expressions are actually equivalent. [25:57] JJ Allaire: So let’s take a look more closely at that. at that solver. Hopefully I have a little step through on this. No, I skipped through the… Okay, so extract the answer. And this is a reg X off of this line answer pattern, which is a common way of prompting to get the model to delineate their answer in a way that it’s easy to pick out. [26:15] JJ Allaire: And then here we actually have a whole nother template, which I’m not going to show, which basically it’s a few shot thing that basically has like it has like 20 different few shots of like these are equivalent. These are not equivalent and and then the model is able to take those and then actually do a do a pretty good job grading pretty surprisingly good job grading. And so this is, you know, you kind of, this is a custom eval. It’s math equations. You have to build a custom score. [26:42] JJ Allaire: You have to use a model to help you do the scoring. But it kind of gives you a taste of some of the things that people will do with scoring. Okay. I want to talk a little bit about what might seem kind of a mundane concern, but logging ends up being like massively important for doing good evals. Obviously, we built a log viewer on top of the log, but the log also has an API so that you can interrogate it and you can get multiple logs and then plot the differences and things. [27:12] JJ Allaire: So the idea is the log is a rich Python object. It’s also JSON. There’s a JSON schema for it, but it’s also a It’s a rich Python object that lets you explore everything that happened during the eval and compare logs and things like that. So there’s that. And then I think you’ve seen most of the examples of the log viewer, but showing the samples, showing the messages. Yeah, you’ve seen this. Showing scoring. Okay. So that’s log. So a lot of people, the other thing people do, I think I’ll show this later, they’ll run like… [27:46] JJ Allaire: 20 eval tasks, like doing a grid search, and then they have all their logs and they plot the results, things like that. So definitely, like, you end up computing on the logs quite a bit. [27:59] Hamel: Very cool. [28:00] JJ Allaire: Yeah. So models, we support a lot of models. We do the big frontier labs. [28:10] Hamel: Do you need to support specific models? What’s the difference between using any hugging face model? Can you just use any hugging face model, really? [28:19] JJ Allaire: Yeah, it can. [28:20] Hamel: Okay. [28:21] JJ Allaire: Got it. Absolutely. What it is is this prefix here. That is the model provider. And then this is completely arbitrary. So this is like any hugging face model. This is like any model that you have locally with Oyama. This is any model that’s on Together AI. This is provider and model name. We don’t know anything about these model names. We don’t resolve them, compute on them, we don’t know what they are. They just get passed through. So when Jammini 2.0 comes out, you just start using it. [28:52] Hamel: And can you have your own endpoint? Like your own REST API endpoint? Yeah, [28:57] JJ Allaire: you can. So one of the things that is interesting, like Oyama and VLM both actually end together. I think together might use VLM. They all use OpenAI’s API. So sometimes people will just use OpenAI with a custom base URL, but you can also create a custom model provider as well. If it’s like completely custom REST API that we don’t know about, it’s very easy to make a model provider and publish it in a package or what have you. So yeah. [29:29] JJ Allaire: So you should be able to get to the models you want to get to without trouble. [29:35] Hamel: Cool. [29:35] JJ Allaire: Okay. Okay. So let’s see. I just want to make sure we have time for questions. I’m going to go a little fast on the rest here, but just to say we care a lot about interactive development. We care a lot about being able to work in a notebook, doing exploratory work on the eval, but then we want the eval to end up in a form. You can run them in CI. You can run lots of them. You can systematically compare results and things like that. [30:00] JJ Allaire: So we have good, like we have tooling that works well in notebooks. I showed you before you saw like a terminal, it was like inspect, eval, do the thing. You can do all this in Python, in a notebook, and it does all the things in the notebook. You can have tasks in notebooks. And so we definitely try to invest a lot in like interactive workflow and then make it so it can scale to the more, I would say, production workflows. So again, I’m not going to dwell too much. This is like a grid search. [30:28] JJ Allaire: So it’s like, okay, I’m doing a grid search over different grader models, graders, and system prompts. And that product is just like a thing that’s making the grid search. I’m dynamically synthesizing a bunch of tasks and I’m going to run all the tasks and I’m going to plot the results. So that’s just an example of like in a notebook, you just want to like explore the space with different things. And then later you might say, well, we’re going to formalize this. We’re going to make a task. We’re going to have some parameters from the task. [30:56] JJ Allaire: What that allows you to do is start to address the evals, like with external driver programs. So basically I won’t get well on this, but like once you have this task and this can still be in a notebook and you’ve got these parameters here, I’m, I’m just basically just varying the system prompt and the grading prompt. You know, I can basically go inspect eval and then I can actually like, vary those parameters externally from a driver program, or I can do the same thing if it’s in a notebook. [31:23] JJ Allaire: I can say, okay, I’m going to keep my eval in the notebook where I did all my exploratory work, but I still want to be able to address it outside of the notebook. Okay, task variant. This is down in the weeds. We’re not going to get into that. Okay. And then eval suites. Again, I’m not going to get into all this, but the idea is you should be able to have dozens of evals arranged in directories however you want, and we can find them and run them. [31:48] JJ Allaire: This is an example of a directory structure that has a bunch of Python, has a bunch of tasks. We can find the tasks. We can run all the tasks. And, you know, again, the production version of it would probably be more like run it, put the put all the logs in an S3 bucket, then later on, go look in the S3 bucket and retry things that failed and things like that. Right. Okay. [32:12] JJ Allaire: And then one last piece on sort of workflow is one principle is that if you run an eval from a Git repository, we want to, if you only have the log file, you should be able to completely reproduce the eval. It won’t necessarily give you all the same, obviously, since the models are non-deterministic, it won’t give you the same results, but you can reproduce all the input parameters and everything. So, for example, if I hand you a log. I can use the Python API to go read the log. [32:40] JJ Allaire: I can go get the origin and the commit. I can get clone it. And then I can just run eval on it and that will work. So the idea is that the log file is like, assuming it was run from a Git repo, there’s sort of a unit of reproducibility. Okay. Okay. So I think we made it in time to have a decent number of questions, but I want to emphasize some resources. So one is, let me see here, is the documentation. [33:13] JJ Allaire: website, lots of documentation that goes into lots of depth on all the stuff I talked about here. There’s a fair number of kind of annotated examples that go through, kind of walk through the code and explain all the different things going on. There’s also a, if I can find, a benchmarks in Git. There’s like we implemented a bunch of benchmarks and you can see how those are done. [33:41] JJ Allaire: So lots of examples and lots of docs and then kind of some of the stuff I talked about, workflow and logs and tuning and things is all, we have docs about that as well. And then this is where you would go to get kind of everything I presented. So this repo has, I won’t scroll down yet so people can note the URL, I can just stick in the chat. Let me just do that quickly here. Someone can stick in the chat. Yeah. Okay. Yeah. But this basically has, yeah, I’ll let you note that. [34:21] JJ Allaire: But basically it has the slides, and then it also has the code. So it has, if you go into like Honeycomb here, it actually has the kind of. what I actually did, the full code, and there’s the prompts. You’ll recognize utils. This is like the code right from the course. And then we have a, here’s the queries eval. You can see, again, we reused the code. We didn’t copy and paste it, but here’s the two eval tasks. And then we have a notebook version of that just to demonstrate doing it in a notebook. [35:01] JJ Allaire: a little bit more commentary so there’s that and then i included some benchmarks here just for some people could explore those and then also that lang chain example that we talked about is here also so that kind of explains this how to run it and um yeah and then the slides so this is a worthwhile repo to check out and the docs are also worthwhile to check out so let me go back to here and go full screen and Q&A. Yeah. [35:37] Hamel: So, one question is, will Inspect be integrated with pre-existing POSIT products like RStudio or anything else? [35:46] JJ Allaire: So, just to clarify, Inspect is not a POSIT project. It’s a UK AI Safety Institute project. So it will not. I mean, unless it’s just in a separate… Exist in a parallel universe. So… We have a VS Code extension, but I’m not sure about any other positive things. [36:10] Hamel: Yeah. I think I definitely know the answer to this question, but I’ll just go ahead and ask it because it’s a question. Does Inspect support evaluating LLM systems using the inputs and outputs that was produced in the past? [36:26] JJ Allaire: Yes. Yes. You mean you’re talking about some kind of… [36:31] Hamel: Like past logs. [36:32] JJ Allaire: Yeah, yeah. So what you can do is the input can be oversimplified. Input can be a prompt or it can be a message history. So you could replay an entire message history. And so you could take, like you said, a past log and then construct a new data set that would allow you to evaluate using the previous prompts. [36:53] Hamel: Okay. Another question is, does the inspect team… plan to expand? It might be good to maybe briefly describe what is the inspect team or who’s working. But does the inspect team plan on expanding the list of metrics to include stuff like MRR? and Mars like a ranking metric? [37:14] JJ Allaire: Yeah, we will, yes. The metrics, the built-in metrics are a little spare, partly because a lot of people do end up writing their own metrics. But I think there are ones that are like definitively important and common. And so we will definitely be doing that. And then the inspect team is, there’s two or three of us who work on it, working on it full time. But there’s a lot of people. inside UK AI Safety Institute who provide pull requests and design feedback and things like that. It’s sort of like just like eval. [37:48] JJ Allaire: It’s like it’s just evals are everywhere. And so it’s kind of in the air. And so there’s a lot of feedback and a lot of a lot of iteration. And then we’ve got I definitely have the bandwidth to to advance the project at a significant pace. [38:05] Hamel: It’s great. I guess like this is a good time to ask the next question, which is. What is the future expectation for Inspect? Will it continue to be developed for the long term? What will its direction be dictated by the UK government or the community or both? And how do you think that… Yeah, [38:20] JJ Allaire: it’s a good question. It’s definitely going to be developed for the long term. We view it as like a foundational piece for doing sophisticated and complex evaluations. And I think, I expect that if it does get picked up more broadly by lots of other players, that there will be community discussion and consensus about what’s important. And we definitely don’t, we would not have open sourced it if it was like, oh, this is just like something we’re using. Oh, by the way, here, everyone else can use it too. [38:52] JJ Allaire: I think we want to make it broadly useful for all the different sorts of evaluations. One of the ideas is that if we can kind of level up the quality of evaluations more broadly, I think that’s just a better world to be in where everybody does evaluations better. And so I think we’re quite interested in making it work for lots of different use cases and scenarios and actors. [39:23] Hamel: Okay. Does inspect allow for using logs from an API or database query, or is it strictly files only? [39:32] JJ Allaire: So, using logs or writing logs? I wonder. [39:38] Hamel: Using logs. [39:41] JJ Allaire: Yeah, I mean, the… We use FSSpec, so logs can come from any file system that is addressable by FSSpec. I think if you had an API where the logs were, you would interact with the API, you’d realize that on the local file system, and then you’d… interact with it. We have an internal abstraction for logs, a log recorder that is not just doesn’t presume that it’s a JSON file. And so maybe we may add other recorders that can, you know, log to databases and things like that. But Yeah. [40:20] JJ Allaire: The other thing is we want the log file format to be something you can compute on. So we do, we actually publish, you can see in the docs, we publish a JSON schema for it. We publish TypeScript binding types for it. So, you know, we want it to be something that people can use and compute on. And obviously a Python API for it. [40:43] Hamel: All right. I’m just scrolling through the questions here. Someone is really curious about the cybersecurity eval stuff. So the question is, can you say a little bit more about the Docker-based cybersecurity CTF eval? Do you envision shareable suites of security tests? [41:00] JJ Allaire: I think that’s going to happen. Yeah, we haven’t shared any of ours. I do know other people are talking about making shareable suites of security tests. Like there’s other people in the ecosystem who are planning on open sourcing those sorts of things. So that’s definitely part of the plan. And we’re going to, we’ve figured out, like people inside UK AI Safety Institute sort of like figured out how to do the Docker thing without, they just bootstrapped it inside the existing solver idea. [41:32] JJ Allaire: But we’re actually going to have a more formal construct to support what we call tool execution environments that will, you know, will do more of the Docker heavy lifting and interacting with Docker Compose and things like that. That’ll be more built into the framework in the future, in the not too distant future. If you went to do it now, you might say, huh, what am I supposed to do? But I think in a month or two, it’ll be more clear and documented and a happy path. [42:04] Hamel: Let’s see. My team is using weights and biases to track eval metrics. Is there a way to combine inspect AI with weights and biases? We are… [42:15] JJ Allaire: open to do that. That’s on the fairly short list of things that we want to do. I haven’t looked carefully at the weights and biases API and how rich we can make it, and hopefully we can make it quite rich. I know that they have some kind of API I saw where you can have an iframe in their thing, so we could potentially even have the log viewer go embedded in weights and biases that also project a bunch of the log file results into it. into the standard weights and biases affordances. [42:46] JJ Allaire: So we’re going to be looking at that in the next couple, three, four months, hopefully. [42:55] Hamel: Someone asked a question. They’re really curious about the design philosophy behind this, like where you got the inspiration from. They said it’s very clean, and the clarity of thought is impressive. [43:07] JJ Allaire: It’s all Hadley all the time. [43:10] Hamel: Okay. [43:12] JJ Allaire: I learned a lot from him. I’ve seen a lot of his work. When I’m designing a framework, he didn’t provide direct feedback on this, but he’s like the virtual sitting on my shoulder, keeping me accountable to keeping things clean, and simple, and straightforward, and composable. [43:36] Hamel: That’s great. One more person is asking, can Inspect be used with LLM proxies, like light LLM? I don’t see why not, [43:44] JJ Allaire: but… [43:48] Hamel: Absolutely, yeah. Okay. Someone is asking about, do you have any imports or plug-ins for using Mac GPUs locally within SPECT AI? [44:02] JJ Allaire: So, yes. So you can use whatever Oyama is doing, which runs on the Mac. I’m sure that they’re using Metal and things to make inference reasonable. I’m not positive, but I can’t imagine that a project like that wouldn’t be using Mac GPUs. So, Oyama is one way. And then you can, with Hugging Face, use the MPS backend. We do have support for that. So that, I feel like the Oyama has done a better job, like, reducing the memory requirements maybe than, I mean, depends on the Hugging Face model. [44:42] JJ Allaire: But we found a lot of, like, the Hugging Face models that you want to evaluate, you know, you definitely need to have, like, pretty reasonable GPU or GPUs. So, and I don’t know how generally, how good like PyTorch MPS is. I don’t know like how good it is. [45:04] Hamel: I think this kind of may be a good stopping point. There’s certainly other questions, but I think we hit the most important ones as far as I can see. [45:17] JJ Allaire: Okay. [45:18] Hamel: Terrific. What do you recommend for people trying to follow your work? Like how to keep in touch with you or appraised of what you’re working on? [45:30] JJ Allaire: Yeah. Yeah. i’m not i do have i have a twitter presence but i don’t i’m not like posting on there all the time so that’s not a great place github is a perfectly good place to to stalk and see and see what’s going on um you know um some of these commits don’t show up like for inspect don’t show up there but but some of this like peripheral work like this workshop show up there so yeah i’d say follow follow me on github it’s a good way to go okay great yeah [46:04] Hamel: All right. With that, thank you, JJ. It’s really great to have you here. I learned a lot about the framework as well. So it was great to have this overview. [46:14] JJ Allaire: All right. Yes, it’s a privilege to be able to come and talk to everyone here. And so hopefully you’ll have for those that decide to give Inspect a try, hopefully you’ll have success with it. And let us know if you don’t. And we’ll be very active on GitHub issues. So please let us know what’s wanting and or what you aspire to do that you can’t do. [46:35] Hamel: All right, great. Thank you. [46:37] JJ Allaire: Thanks, JJ.",
    "crumbs": [
      "Educational Resources",
      "Evals",
      "Inspect, An OSS framework for LLM evals"
    ]
  },
  {
    "objectID": "education/fine_tuning/kyle.html#chapters",
    "href": "education/fine_tuning/kyle.html#chapters",
    "title": "Fine-tuning when you’ve already deployed LLMs in prod",
    "section": "Chapters",
    "text": "Chapters\nChapters are organized in the format of the talk which is the “10 commandments of fine-tuning”.\nThou Shalt …\n\n0:00 1st: Not Fine-Tune\n4:59 2nd: Write a Freaking Prompt\n10:38 3rd: Review Thy Freaking Data\n12:37 4th: Use Thy Actual Freaking Data\n17:40 6th: Reserve a Test Set\n19:10 5th: Choose an Appropriate Model\n23:01 7th: Write Fast Evals\n25:50 8th: Also, Write Slow Evals\n28:07 9th: Not Fire and Forget\n31:17 10th: Not Take the Commandments Too Seriously",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine-tuning when you've already deployed LLMs in prod"
    ]
  },
  {
    "objectID": "education/fine_tuning/kyle.html#resources",
    "href": "education/fine_tuning/kyle.html#resources",
    "title": "Fine-tuning when you’ve already deployed LLMs in prod",
    "section": "Resources",
    "text": "Resources\nThese are the resources mentioned in the talk:\n\nOpen Pipe\nLLM Inference\nArgilla for curating data.\nFSDP + QLoRA from AnswerAI\nKyle’s Twitter",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine-tuning when you've already deployed LLMs in prod"
    ]
  },
  {
    "objectID": "education/fine_tuning/kyle.html#slides",
    "href": "education/fine_tuning/kyle.html#slides",
    "title": "Fine-tuning when you’ve already deployed LLMs in prod",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine-tuning when you've already deployed LLMs in prod"
    ]
  },
  {
    "objectID": "education/fine_tuning/kyle.html#notes",
    "href": "education/fine_tuning/kyle.html#notes",
    "title": "Fine-tuning when you’ve already deployed LLMs in prod",
    "section": "Notes",
    "text": "Notes\n\nOverview and Strategy\n\nPrompted Models as a Default Strategy\n\nStart with prompted models for fast iterations and updates.\nUse them to establish a baseline before considering fine-tuning.\nAnalyze both input and output data thoroughly to ensure model performance improvement before fine-tuning.\n\nPreparation for Model Training\n\nSegregate test sets and choose models with optimal parameter sizes for cost-effective training.\n\n\n\n\nFine-Tuning Considerations\n\nDefault Approach\n\nAvoid deploying fine-tuned models unless necessary for quality, latency, or cost reasons.\n\nAdvantages of Prompting Over Fine-Tuning\n\nPrompted models allow for faster iteration and quick updates.\nTools like OpenPipe enhance the use of prompted models.\nUse fine-tuning only when prompted models don’t meet required standards.\n\n\n\n\nWhen to Consider Fine-Tuning\n\nQuality\n\nFine-tuning can improve model performance when prompting alone isn’t sufficient.\n\nLatency\n\nFine-tuned smaller models respond faster, improving real-time application performance.\n\nCost\n\nFine-tuning can reduce costs by enabling the use of smaller, less expensive models.\n\n\n\n\nKey Insights and Best Practices\n\nEstablishing a Baseline with Prompting\n\nStart with prompted models to determine if fine-tuning is necessary.\nPrompting often provides valuable insights and avoids unnecessary fine-tuning.\n\nData Review\n\nAnalyze input and output data before fine-tuning to understand model performance.\nDon’t exclude data sets solely based on poor performance; they may contain essential variations.\nManual relabeling and modifying instructions can improve responses to varied inputs.\nTraining on imperfect data can still improve performance due to the generalization capabilities of larger models.\n\n\n\n\nEvaluation and Continuous Improvement\n\nModel Evaluation Strategies\n\nFast evaluations: quick, inexpensive, during training.\nSlow evaluations: detailed, assess final outcomes and production-level performance.\nContinuous outer loop evaluations are crucial for adjusting strategies based on real-world performance.\n\nHandling Data Drift\n\nOngoing evaluations are necessary to maintain model relevance.\nRetraining with updated examples can solve issues caused by data drift, ensuring continuous improvement.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine-tuning when you've already deployed LLMs in prod"
    ]
  },
  {
    "objectID": "education/fine_tuning/kyle.html#full-transcript",
    "href": "education/fine_tuning/kyle.html#full-transcript",
    "title": "Fine-tuning when you’ve already deployed LLMs in prod",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Kyle: The topic is deploying fine-tuned models in production. The first commandment and the most important commandment of deploying fine-tuned models in production is You should not do it. Okay? All right. This is obviously not universally true or else I wouldn’t be giving this talk. But I think it’s a good default. So specifically if you have an existing flow that is working for you and you figured out a pipeline with a prompted model, or if you are able to figure out a pipeline with a prompted model, that probably should be the default. [0:35] Kyle: There’s a lot of advantages there. A big one is there’s a lot of your iteration speed is much faster. If you notice something’s wrong, you can play with it. You can update the prompt really quickly. And it just leads to a better experience versus fine-tuning. I think there’s a lot of tools, and OpenPipe is one of them. But we try and get the experience as good as possible. But really, honestly, you can’t really beat the experience of you just change the words and rerun it, and you get different or better results. So… [1:07] Kyle: The default should be don’t bother with fine tuning unless one of some very specific things are true. Okay? This is what I just covered. Start with prompting. If you want to get a little bit more fancy than just raw dog typing out the instructions, very often doing something like, hey, coming up with three or four examples that are really high quality and throwing them in your prompt. [1:32] Kyle: Or even a little bit faster than that, maybe you have a bunch of examples that you know are good, and you can sort of like use a rag thing where you grab the examples that are most similar to the exact document or whatever you’re analyzing right now. These are all strategies I would reach for before I reach for fine tuning. Okay? Now, all of that said… there are some very specific and very good reasons why you would go past that point and why you should actually do fine tuning. [1:59] Kyle: And in my experience, there’s three dominant ones. So the first reason is quality, right? So you often find that there’s only so far you can get with just prompting as far as guiding a model towards a specific outcome. And if… how far you can get with that is not all the way to where you need to be to provide a good experience. That’s a great reason to do fine tuning. [2:25] Kyle: Another really good reason is because when you’re doing fine-tuning, typically, the, I guess you could say, the Pareto frontier of how far you can push a given model or a given size model, like performance-wise, is much further out with fine-tuning than with prompting. And so what that means is you can move to a much smaller model for a given quality bar. And the upshot of moving to a smaller model is that you can get responses much faster. So there are a number of use cases. [2:55] Kyle: And we have users like this where, you know, you’re doing real time, you know, you’re like doing phone system work or even chat work where you just want to get a really fast response. Or kind of like the double whammy of you want to get a user’s response back fast, but you’ve got like this agentic flow where it’s, you know, you’ve got several different prompts you’re running in sequence to kind of like figure out like all the stuff that needs to happen and then to get the response back to the user. [3:20] Kyle: And of course, every one of those takes a latency hit. And so if you’ve got several of those running and the user’s waiting, you’re going to have a much better experience if those are going faster. And fine tuning can get you to that spot. And then the final one, and this is actually the one we see pushing people the most often. is about cost. So in a lot of cases, there’s a lot of use cases where GPT-4 can do a fantastic job and latency is not an issue. You’re just using it for classification or whatever. [3:48] Kyle: But once you’re doing this on hundreds of thousands or potentially millions of calls every day, it just gets way too expensive from a pure unit economics point of view. And so this is a very strong reason why people will go to fine tuning because again, you can move to the smaller model sizes. and still get a really strong, a really good performance. And of course, the cost per token is much lower at those lower models, smaller model sizes. Okay, so we’ve established that these are, that one of these things is true. [4:21] Kyle: And again, you should not fine tune unless one of these things or some other reason that’s a very good one is true. But let’s say we’ve established this, right? That with prompting, either we can’t hit quality or latency or cost. Okay. So, now let’s go into the actual fine-tuning part and talk about what we’re doing there. Actually, trick question or trick statement, we’re still not going to go to fine-tuning, okay? [4:46] Kyle: So, even if you know that this is true, even if you know it’s like, hey, I am not going to be able to deploy in production because I know, I just know that it’s going to be too expensive when I do this at scale, or I just know that, like, I can’t hit my quality bar. [4:58] Participant 2: Um, [4:58] Kyle: You should still start by writing a prompt that gets you as close as you can. And I’m going to explain why that’s important. Or I mean, you know, you can get away without doing this, but why I think this is for most people for most flows the way you should do it. So the first thing is it gives you a baseline, right? [5:15] Kyle: It’s like, okay, if I want to know if my fine-tuned model is worth it, if I should be investing time in this, I need to know in the alternative where it’s just prompted what I’m coming from and what I’m trying to improve on. And so that’s really important. And then as we’re getting later on and we’re talking about evals, we’re talking about actually the user experience you’re providing, this gives you something to compare it to and just make sure, hey, is all the time and effort I’m investing in doing this. [5:41] Kyle: like actually giving me a return. This next point is, I think, a little bit of a subtle one that people don’t think enough about, but I think a really critical one. And that is trying to solve your problem with a prompted model can give you very good information about whether the task is possible. So this is actually something we see very frequently, where someone will come to us and they’re trying to solve a problem with fine tuning, and they haven’t tried or they haven’t been able to get it working with prompting yet. [6:12] Kyle: And we’ll have a conversation with them and say, okay, explain to me, can you do this with prompting? They’ll say, oh, no, it’s because the model, we have this very weird schema that we’re trying to get the model to output. And it’s just like prompting can’t get it there or some other reason why they think it doesn’t do it. But then they’ll use and so I’ll say, OK, well, we need we need data to fine tune. And often they’ll say, oh, no, that’s that’s no problem. [6:38] Kyle: We have a bunch of data like we have data we can train it on. This is not a problem. [6:41] Kyle: What I have found in practice is that it is very often, unfortunately, I’d say more often than not the case, where even if you think you have labeled data that should work for this, the data you have, if it’s not good enough to stick in a rag pipeline or something like that and make it work with prompting, there’s a very high chance there’s actually not enough signal in that data to do what you want it to do. [7:07] Kyle: And there was actually a fantastic example that I think Dan gave in the first lecture here about a logistics company where, you know, that you may remember if you watched that lecture, he was talking about they were trying to go from the description of an item to the estimated value of the item. [7:22] Kyle: And I think there was a good assumption there, like a hypothesis that like, well, the description of the item should tell you enough about the item that like a model that understands a lot about the world should be able to guess how much it’s worth. But in practice, what they found is that it actually didn’t. It didn’t capture. enough information, there was enough randomness or reasons why people weren’t putting the right thing in the value box anyway, there actually was not enough information in that training data. [7:46] Kyle: Whereas if you’re going with a prompted flow and you’re trying to get that working first, you can find out very quickly, like, okay, I just don’t have the data here. You know, the model’s not able to figure this out. This point is not a universal rule. It definitely is possible to train a model that, through fine-tuning, that can have great performance on your proprietary data set or whatever, that there’s no way that a prompted model could do. [8:10] Kyle: So definitely not a hard and fast rule, but I found that if you can go in this direction, it’s going to make your life much better. So just kind of like a heuristic on that point is the so if you do find that you’re able to successfully write a prompt that is able to at least, you know, more often than not do a good job on your task, then there’s like a 90 plus percent chance that you will be able through fine tuning to improve that further on the metrics you care about. [8:39] Kyle: So whether that’s latency, quality, or cost, there’s like a very good chance you can get this working. Whereas on the other hand, if there is no way to formulate your task in such a way that a prompted model could ever work and you’re just hoping that it can learn from the data, that still can work. But you’re definitely playing in a hard mode at this point. This is now a hardcore data science project. You have to figure out, is there actually enough signal in this to solve the problem? And it just gets much more complicated. [9:08] Kyle: And in a lot of cases, it turns out that there actually isn’t that your data is not clean enough or well labeled enough or whatever to learn what you want it to. And so there’s a high chance of failure. So you want to be in this regime. You really want to be in the regime where you know it works with prompting. And then there’s a very high chance that fine tuning is going to be able to juice your returns and get you in a way better place. [9:31] Kyle: And you want to avoid being in this other regime. Okay. Okay. So, we’ve got a prompt. And now we’re going to fine tune. And okay, I guess this slide is something I just wanted to share quickly. The conceptual way I think about it, the way I encourage folks in your situation who are thinking about deployments and production to think about it is like… You’re going with the prototype. Basically, when you’re iterating fast, when you’re trying to figure out, is this even something that’s possible to do? Is this something that’s providing value? [10:01] Kyle: Is this something that, with our whole whatever app or flow or whatever it is you’re building, and you’re trying to see if it’s going to work or not, and if it’s going to provide value, just do all that part with GPT-4. Don’t worry about fine-tuning. And then once you’ve got something that actually works, that scales, that you have a sense of what people are using it for, that’s the point where you’re going to drop in fine-tuning. That’s just kind of like the general mental model that, again, not universal. [10:26] Kyle: There’s reasons and times when it makes sense to go straight to fine-tuning the model if you can’t get prompting working. But in general, this is the flow I would recommend trying to make work by default. Okay. So, we have, so let’s say we now have a prompt deployed in production. We have, you know, people using it, people prototyping, playing with it. What’s the next step? Well, you got to look at the actual data. [10:51] Kyle: And this is going to teach you so much, both on like how good a job your existing prompted model is doing, that’s super important, but also like on the types of things that people are using your product or your model. And that’s actually the part that I find personally is the most useful part of reviewing data. It just gives me a much better sense of like, okay, it just gives me a feel for what my input distribution looks like. [11:22] Kyle: And without that information, you can make assumptions about the types of tests you should write, even the types of models you think will work well or poorly based on… like, you know, just like how hard you think the task is, the types of things you think this is being used for, that can be very wrong. So you just got to look through it. There’s no magic here. The right way to look through it varies a lot. [11:50] Kyle: Very often in whatever system you are writing, you have some sort of natural UI, whether it’s a chat conversation interface, whether it’s some kind of classification system and you’re putting documents in everything. So if you do have an existing UI, then that’s probably the right place to look at it in. If not, if for whatever reason that there’s not a good way, then there are tools out there you can use. And OpenPipe is one of them, but that’ll just give you a nice formatted, okay, this is what the input looks like. [12:18] Kyle: This is what the output looks like. Let’s click through, let’s go through 20, 50 of these and just get a sense of what our data looks like. [12:26] Kyle: and you’re wanting to look at both the input and the output um you want to get a sense of the outputs any good as well of course but honestly like i said i find a lot of the value here is is just getting a really good sense of my input distribution okay so we’ve looked at our data we have a sense of what’s going on what’s next So, okay, so this next one is like, I need to explain what I mean by this. [12:51] Kyle: So you actually, once you’ve looked at this data, once you understand, okay, this data is good, now you have to like, that is the data you should use. And let me, like the failure case I see here. is there’s a lot of, there’s like a tendency where sometimes people will look through their data and they’ll be like, oh, I noticed this like particular class of data, the model I’m using in production right now, it does a bad job on. [13:15] Kyle: And so what they’ll do is they’ll go and they’ll like drop that out of their data set before they fine tune on it. Because they’re like, well, I only want to fine tune. And they’ll, you know, sometimes people have like a fancier automated way of doing this where they say, okay, well, you know, I’m going to collect user thumbs ups and thumbs down on my prompt. And then the ones that get thumbs up, I’m going to go ahead and fine tune on those. And the ones that get thumbs down, I won’t. [13:37] Kyle: And I’m not going to say that never works. Like that definitely can work. The failure mode that I see here that does happen, though, is if you’re rejecting all the places where the model is doing a bad job. there’s a real danger that there is some correlation between the things it does bad on and a specific space of your inputs or something that you actually do care about. And if you don’t fine tune on that, then you’re just going to do a bad job there. [14:04] Kyle: So the real solution here is figure out why the model is doing a bad job. Figuring out where in your input space is it doing a bad job? And then do one of several things. You can manually relabel it. You can just spin up a relabeling UI, and there’s several of them out there that are pretty good. And kind of type out what it should be. You can try and fix your instructions. Oftentimes I find that’s the best fix is as you’re manually reviewing the data, you’re like, oh, it’s doing a bad job here. [14:33] Kyle: And then you play around with it and you fix your instructions and you can get it to a place where it’s doing a much better job there. So that’s very often the right way to do it. But the main thing is, you don’t want to just drop those ones and just fine tune on the ones that it did a good job on, because there’s a very high chance that means you’re just like… chopping off big chunks of your input space. [14:50] Kyle: And then when you have your fine-tuned model that you’re deploying in production, it’s never seen this data before, and it’s also just going to do a bad job in that area. Anyway, no shortcuts there. [15:01] Kyle: Now, I’m going to contradict myself a tiny bit, which is, if what you find is the model is mostly doing a good job, and 10% of the time, but it’s kind of like a random 10% of the time, it forgets an instruction or gets something wrong or something like that, I have actually, but it’s not like super correlated in like this one area. It always gets it wrong. It’s more just like, well, sometimes GPT-4, it’s non-deterministic. It messes stuff up. once in a while. [15:29] Kyle: I have actually found that when that is the case, it actually doesn’t really hurt to train on that data. And I, this is what I’m saying right now is like very controversial. [15:38] Kyle: And I’m sure that like, you know, like I could have a great debate probably with people on this call right now about like, whether this is, you know, like how, how big of an issue this is, but I’m just saying from my own experience, what I found is like, you can very common, like basically the these models are quite smart and they’re quite good at generalization as we’re getting into these larger models. And I’m talking really, the advice I’m giving right now really is for like, you know, I’m talking about like LMs. I’m talking like… [16:03] Kyle: for 8 billion plus parameter models. I’m not talking about the really small ones. But for these big ones, I actually find that the training process itself, to some extent, does a form of regularization or normalization, where even if the input data is not perfect, as long as it is pretty good on average, you can actually very commonly see the model outperform even the model that was used to train it originally because of that sort of regularization effect. And so we see that very commonly on our platform. [16:35] Kyle: We have lots of users who are training their fine-tuned models directly on GPT-4 outputs, and then they’ll actually run evaluations between GPT-4 and their fine-tuned model and consistently see for a lot of tasks that their fine-tuned model is doing better than GPT-4. And again, I think this comes down to that regularization effect where GPT-4 will just like, you know, 10% of the time make a dumb error and as part of the training process. It sees those, but it sees the 90% good ones and it learns, you know, that basically the goodness. [17:08] Kyle: So anyway, all this to say, don’t stress too much if there’s like a few bad examples. I think in many cases, you can get great results anyway, and it may not be worth the effort of like manually reviewing all whatever 5,000 or 10,000 examples you have. Okay. So let’s say we’ve got, you know, we’re pretty happy. We’ve reviewed our data. We’re like, okay, you know, at least like 90% of these look good. This is basically doing what I want my model to do. [17:37] Kyle: Okay, so next, before we actually do our training, very important, you have to pull out a test set. Okay? And this is not news to anyone who has a background in machine learning, but I do actually see this as something that sometimes people don’t do. Or often I’ll see people who have a test set which is kind of like weirdly constructed. So they’ll like pull out like, I don’t know, they’ll come up with like 10. [18:03] Kyle: random examples because they saw these are the ones that prompted poorly on, or these are ones that like, for whatever reason, you know, a customer complained about this. So, so they have like a way of constructing a test set, um, that is not representative of the input data, I guess, is the way I would put this. [18:18] Participant 2: Um, [18:19] Kyle: and I think that’s totally fine. Like, I think having a test set like that of like specific cases that, you know, are weird corner cases and you want to make sure it’s doing well on like, that’s, that’s a, that’s actually great. There’s nothing wrong with that. [18:29] Kyle: The problem is if you are testing on that exclusively and you are not also having a test set, which is just like basically like a random sub sample of your inputs, then you can be like you can you can have you can think you’re doing well and really not be doing well. Like you really want to have a test set, which is just like, hey, grab 5%, grab 10% of my data at random. Don’t train on it and and use that as the input. So Highly recommend doing that. [18:58] Kyle: This is just kind of standard stuff for machine learning. But again, something I see people that don’t have a lot of experience fine tuning maybe not realizing is really important. Okay. So now let’s talk about the actual model that you should be fine tuning. So one nice thing is if you’ve got, and I know that you’ve had, you know, like you’ve already had a chat with Wing from Axolotl. [19:23] Kyle: A really nice thing about Axolotl or HuggingFace’s SFT train or things like that is, and just the HuggingFace transformers ecosystem in general, is there are a lot of… It’s pretty easy if you get a fine-tuning pipeline set up for one model to throw in several different ones and run them side by side. And as long as your dataset isn’t huge, the cost is not prohibitive. We’re talking maybe a few dollars in many cases per fine-tuning run. And so that’s great because it makes it a lot easier to kind of like try different things. [19:53] Kyle: But kind of as like a rule of thumb or a place to start. So this is a chart I put together actually for a different presentation. But I think it’s kind of representative of the ecosystem and some of the models that people are fine tuning commonly. And so the sense here, so this is actually based on real data. What this is normalized to is the number of training examples it took to match a certain performance threshold. [20:24] Kyle: So basically, for the specific test that this is, I looked at like, okay, how good does GPT-4 do on this? And then for each of these other models, like, how many training examples did I have to add until it was able to… match GPT-4’s performance on this specific task? And the answer to that question is task dependent, but this is kind of like, this gives you an overview. [20:45] Kyle: What I find is, you know, in general, if you’ve got like a few dozen examples, like you can very often get like Lama 370 billion to match GPT-4 on your example. And then as you start getting to smaller models, it does take more training data. It takes a wider input of training data. And again, this is very task dependent. There are some tasks where like, it doesn’t matter how much training you do, you’re never going to reach GPT-4 performance. [21:09] Kyle: I actually find that that’s like definitely the exception for most things I see people running in production. I find that like these fine-tune models actually usually can pretty easily match GPT-4’s performance if you have a good training set. The place where I actually see most people coming in when they’re actually doing this in production, really the sweet spot, is this 8 billion, 7 billion, 8 billion parameter model. And so things like LAMA3, 8 billion, Mistral 7 billion. [21:37] Kyle: I see that as like a really nice sweet spot because the amount of training data you need to get good results out of that is not overwhelming. You know, if you have like a thousand examples or so, you can typically get really good performance out of this. And at the same time, and so if you’re running in production, like we were talking about earlier anyway, hopefully you do have a few thousand examples that you’ve gotten that have gone through GPT-4 anyway. So hopefully you’re in a good spot. And the cost savings are really significant. [22:06] Kyle: So actually this, I think, yeah, I didn’t update this with the latest inference costs you can get online, but this should actually be lower. The cost you’re seeing per token for like a Lama 3, a billion is somewhere in the 15 or 20 cents per million tokens. versus GPT-4, even GPT-4.0 is, I think, about 50 times that. So it’s a huge savings you’re getting there. And you can get really good performance in that, you know, 1000 to 5000 example range. So that’s where I often recommend people go. [22:36] Kyle: But again, these training runs, the nice thing is they’re really cheap to do, especially if you’re in that like, you know, less 5000 or less example thing. And so why, you know, you can try a 70 billion model and see if it works. does better. You can even go smaller. You can try like a 5.3 and see how it does. And like, ultimately, like, it’s a pretty easy test to run. But this is probably where my default would be for a lot of tasks, at least. Okay, so now we’re going to talk about evaluations. [23:04] Kyle: Evaluations obviously are a huge subject all on their own. And I’m not going to go super deep into this. I think probably there’s other segments of the course where you’re going to be talking about this more. But I do think there’s some there’s like an interesting framing here, which is which is not one that I hear people talk about that much. And I think it’s pretty important because in my mind, there’s actually two different kinds of evaluations, both of which are really important. And so the first one are fast evaluations. [23:31] Kyle: And fast evaluations are ones that you can run, like, in your training loop or even if you’re doing prompt engineering, as you’re updating the prompt, these are evaluations you can just update the prompt, run, and then immediately see, did I do a good job or not? So these should be relatively fast to run. They should be relatively cheap to run and not require a lot of outside input to get good results. The Where I personally have found a really good sweet spot for this category, these fast evaluations, is using LLM as judge. [24:03] Kyle: And so that’s kind of like the default. That’s where I would start is basically like asking GPD4 or asking like a jury of like GPD4 and Clod3 and maybe another model or something to say, okay, this is the task, the input I had. And then these are two different outputs from two different models. And there’s some tricks here. You have to randomize the order because there’s a slight preference for the former one. And if you’re using the same model as judge and as one of the entries, it has a preference for itself. [24:32] Kyle: So there’s a little bit of subtlety here. I think there’s good libraries out there that can help you do this that are built in. We also, on OpenPipe, this is the default evaluation we give people. But the main point is… These things are cheap enough to run if you’re running it against, say, 50 or 100 examples that you can just like you can update your prompt and rerun it, or you can fine tune a new model and rerun it and really quickly get a sense, okay, is this like plausibly doing, helping me or not? [24:59] Kyle: And I think that having something really fast like that, that you can run quickly and get a sense, okay, am I on the right track or not, is super critical because otherwise, you know, if you get to, and I’m going to talk about the other kind of evaluation in just a moment, but like basically if you’ve got like these slower evaluations that require you to run a production, your feedback cycle is so slow. that it’s just a lot harder to make progress and to get where you need to go. [25:22] Kyle: So I really recommend investing in fast evaluations. You know, I think element as judges, right, is great. If you’re doing, there’s also like for specific tasks, other evaluations that can make a lot of sense. Like if you’re doing like a classification task or something, then you can just get a golden data set and calculate an F1 score or something like that. So anyway, but the high level point is find something that you can run fast. and then have a quick inner loop and can help you figure out you’re in the right direction. Okay. [25:52] Kyle: Now, the other kind of evaluation, which is also super important, are like outer loop evaluations, slow evaluations, production evaluations, and you can call it different things. But the idea is that this is the one that is actually measuring the final result that you care about, right? And so this is something like if you’re writing a chatbot, like a customer support chatbot, it’s like, okay, what percentage of customers came out of this feeling like… their problem was resolved. Right? [26:22] Kyle: And so these evaluations, I don’t think there’s like, I mean, there definitely is not a one size fit all. It really basically comes back to like, what is the outcome, you know, the business outcome or the product outcome that you’re trying to drive and figuring out how can I measure that? And so you really, these are really critical as well because, you know, the fast evaluations, even if a call looks better in… isolation. Maybe it’s if like a specific model output looks better in isolation. [26:54] Kyle: Maybe it is not maybe there’s there’s like, you know, like some other interaction with some other piece of the system that’s like giving you a bad result. Or maybe like the elements judge is like not perfectly accurate. And it’s not actually measuring or maybe it’s like, once you deploy to production, like you’re quantizing it or something, and there’s like some disconnect in like the actual way it’s running. So there’s all these reasons why. [27:12] Kyle: the sort of fast devalues you were doing before might not tell you, might not give you the full picture and might not be perfectly accurate. And so you really want to have that outer loop and make sure that you are going in the right direction. Okay, so I have a couple of examples here just from OpenAI with ChatGPT. In their particular case, I think they measure how often do people regenerate, how often do people give a thumbs down. [27:35] Kyle: They also have a more concrete flow, which they rarely put up, but I have seen it a few times where you ask a question, it’ll just give you two responses side by side, it’ll let you choose which one is better. And I think, again, obviously it’s really dependent. If you’re not writing a chatbot, then probably this is not the right form. [27:56] Kyle: But I do think it’s important that you think about for your specific problem, how are you actually going to measure that you’re doing a good job and that it’s driving the outcome that you actually care about. Okay. So, we’re almost getting through this. We’re to the ninth commandment out of ten. This one is really important. So, hopefully, like in this previous one, you’ve written these slow evaluations. You’re actually looking, you have some way of measuring at least somewhat objectively or repeatedly like how well you’re doing. [28:28] Kyle: Well, once you’ve deployed your fine tune model, you really have to keep running those on a constant basis. Because if you don’t, what you’re going to see is Something is going to change about the world. There’s going to be some difference in the types of users you’re getting or the things they’re sending you. Or there’s just like so much that can change out there that if you’re not continually measuring how well this is doing, like you’re going to have and like sort of the term machine learning practitioners use for this is data drift. [28:58] Kyle: which can come from a lot of sources. But the main point is, like over time, it’s very likely that your model is not going to be as well adapted to the input as it should be. And like one, so one interesting concrete example, in our case, we had a customer who was doing basically structured data extraction from these call logs, actually, transcripts. [29:23] Kyle: And they noticed that they had this eval that they were running in sort of this slow loop, and they noticed that their responses were getting worse right around January of this year, so I guess five months ago. And so it wasn’t huge. It was like, you know, I don’t remember exactly what the numbers were, but it went up from like, you know, 1% error rate to like a 5% error rate where it was like hitting their things and wasn’t working. [29:51] Kyle: And so they ended up looking into it and they shared with us what they found, which I thought was fantastic, which was the data extraction. There were a number of fields they were trying to extract from these call logs, and one of them was a specific date. So it was people calling in about, it was call logs about people that mortgages basically and trying to get the due date on their payment, whatever. So one of the things they were extracting was what is the specific date that the next payment was due? [30:18] Kyle: And what they found was because their model had been fine-tuned entirely on data from 2023, not in all cases, but in like 5% of cases, even though we were now in 2024, it would just kind of like, you know, it would get the date, like the month and day, right? But then it would like put the year as 2023 in the extracted date instead of the year of 2024. Because it was just like in every single case of the training data, it was the year was always 2023. It didn’t see any other examples. [30:46] Kyle: And I didn’t do that every time. It was smart enough in most cases to figure out, okay, we should be pulling out the 2024. But anyway, that was starting to happen. So all they had to do was retrain a model with a few extra, I don’t remember how they put it in, they put in 10 or 12 2024 examples, and that was plenty to clear it up. So anyway, just an example. You never know where this stuff is coming from, but you do have to keep measuring to see if things get worse. And, okay. [31:15] Kyle: And so finally, the last one is I framed these as commandments. I tried to give disclaimers as I was going through, but I think it’s really important to realize that the most important thing is that you understand your problem, that you understand your data, you understand what you’re trying to solve, and then you figure out what the right way is to solve it. So I think the flow I described is really, really helpful. I think there are other ways to do it that can also be effective. [31:42] Kyle: But I do think that this is a good place to start, especially if you haven’t done this before and you want to maximize the chances that you’re able to do it successfully. So anyway, it’s like Pirates of the Caribbean, right? Where the pirate says, well, the pirate code is not really a code, right? It’s a guideline. Anyway, same feeling.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Fine-tuning when you've already deployed LLMs in prod"
    ]
  },
  {
    "objectID": "education/fine_tuning/napkin_math.html#chapters",
    "href": "education/fine_tuning/napkin_math.html#chapters",
    "title": "Napkin Math For Fine Tuning",
    "section": "Chapters",
    "text": "Chapters\n01:23 About Johno and AnswerAI\nJohno shares his background and his work at AnswerAI, an applied R&D lab focusing on the societal benefits of AI.\n03:18 Plan for the Talk\nJohno outlines the structure of the talk, including objectives, running experiments, and live napkin math to estimate memory use.\n04:40 Training and Fine-Tuning Loop\nDescription of the training loop: feeding data through a model, measuring accuracy, updating the model, and repeating the process.\n09:05 Hardware Considerations\nDiscussion on the different hardware components (CPU, GPU, RAM) and how they affect training performance.\n12:28 Tricks for Efficient Training\nOverview of various techniques to optimize training efficiency, including LoRa, quantization, and CPU offloading.\n13:12 Full Fine-Tuning\nDescribes the parameters and memory involved with full fine-tuning.\n18:14 LoRA\nDetailed explanation of full fine-tuning versus parameter-efficient fine-tuning techniques like LoRa.\n21:04 Quantization and Memory Savings\nDiscussion on quantization methods to reduce memory usage and enable training of larger models.\n23:10 Combining Techniques\nCombining different techniques like quantization and LoRa to maximize training efficiency.\n22:55 Running Experiments\nImportance of running controlled experiments to understand the impact of various training parameters.\n25:46 CPU Offloading\nHow CPU offloading works and the tradeoffs.\n28:31 Real-World Example\nDemo of memory optimization and problem-solving during model training, with code. This also includes pragmatic ways to profile your code.\n45:44 Case Study: QLoRA + FSDP\nDiscussion of QLoRA with FSDP, along with a discussion of tradeoffs.\n54:25 Recap / Conclusion\nJohno summarizes the key points of his talk.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Napkin Math For Fine Tuning"
    ]
  },
  {
    "objectID": "education/fine_tuning/napkin_math.html#resources",
    "href": "education/fine_tuning/napkin_math.html#resources",
    "title": "Napkin Math For Fine Tuning",
    "section": "Resources",
    "text": "Resources\nLinks to resources mentioned in the talk:\n\nJohnowhitaker.dev &lt;&lt; Personal website for Johno Whitaker.\nFSDP+QLoRA Benchmarks &lt;&lt; Johno’s (and others) benchmarks for FSDP+QLoRA used in an example\nTransformers Issue 25572 &lt;&lt; Someone showing math for activations etc\nTalk Slides &lt;&lt; Talk slides\nPyTorch Tutorial on Optimizer Step in Backward &lt;&lt; More use of memory viz plus an under-rated technique",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Napkin Math For Fine Tuning"
    ]
  },
  {
    "objectID": "education/fine_tuning/napkin_math.html#slides",
    "href": "education/fine_tuning/napkin_math.html#slides",
    "title": "Napkin Math For Fine Tuning",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Napkin Math For Fine Tuning"
    ]
  },
  {
    "objectID": "education/fine_tuning/napkin_math.html#full-transcript",
    "href": "education/fine_tuning/napkin_math.html#full-transcript",
    "title": "Napkin Math For Fine Tuning",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Johno: All right, so welcome everybody. The talk is titled Napkin Math for Fine Tuning. The goal here is to answer a number of different related questions that often come up when you’re talking about training, and especially with a lot of people getting into training models for the first time via fine-tuning these big existing models. What affects the performance? How do I make this better or worse? Why is this running out of memory? Or why is this taking so long? There’s some questions already in the Q&A. What if we’re over the limit around GPU? [0:33] Johno: What are the things that we can turn on to bring us under? you know, how do we, how do we like reason about these different parameters? Because if you’ve looked at the axolotl config files or anything like that, you realize there are so many knobs to tweak and it can be tricky to get a mental model of what those different things will do. So that’s the goal in this talk is to kind of get a feel for the space. What are the different types of fine tuning? [0:55] Johno: What are the different things that we can tweak and how does that affect how much memory it uses, how much computation it has to do, how fast or slow it is, how cheap it is, et cetera. Cool. Okay, so these links I’ve also posted in the channel. These are some of the things we’re going to reference. I guess, yeah, for those who are curious about me, there’s a site there if you want all the info. I currently work at AnswerAI, which is an applied R&D. [1:23] Johno: research and development lab for AI, trying to figure out what these models are useful for, what things like fine-tuning are actually useful for, and also how can we make them more accessible, easier, as a public benefit corporation. So the goal is to try and find societally beneficial uses of AI, which is quite a big challenge and I think an open question. [1:44] Johno: Okay, so the good news for this talk, we understand how these models work from it, like not in how do they learn things, that’s still quite an open question, but at least in how do we produce an answer, right? We have some known mathematical operations, we have some known transforms of the data, we put the data through these different operations and we get an output. We do understand this. We can do this maths and we can also experiment and find things out. [2:08] Johno: The bad news is it’s never as simple as you think it’s going to be. The paper might be different from their code, might be different from the Hugging Face implementation. The implementations are changing and shifting over time. The maths can get very tricky. It’s easy to forget things. I always feel as soon as I start reaching into this that I’m very quickly into a depth where oh, wow, I thought I had this down. I thought I understood X. There’s always something deeper. [2:35] Johno: There’s always the little asterisk that says, oh, actually in a multi-GPU system, we keep two copies of this buffer around just in case. You know, there’s always something that you don’t know. And I want to make this clear from the outset. This is like, I will try and be useful in this talk. We’re doing napkin math, so it will be hand wavy. I make no claims that anything I do or say here is 100% accurate. We’re just going to do our best. [2:59] Johno: And the whole talk is going to be this process of trying to guesstimate, trying to get there without having to do necessarily all the perfect nitty gritty steps. All right. Cool. So it’s going to be okay. We’re going to get through even though there’s those hard bits. So, what is the plan? If you’re just joining, we’re talking about napkin math for fine tuning, we’re going to go through what the objectives are, that’s the intro that we’ve just done, so that’s what, 14 and a bit percent of the way through already. [3:30] Johno: We’re going to talk about what happens during fine tuning to get an understanding of like, okay, what are the pieces that we should be keeping an eye on and thinking about? We’re going to talk about how you can run experiments to interrogate some of these questions. And then we’ll also do obviously some napkin math to say, before I run the experiment, maybe I can try and at least make some predictions as to what I might see. [3:51] Johno: I’ll show some code as a way to run experiments that are like at a smaller scale then I run a full fine tuning run on some GPUs that I’ve rented in the cloud like how do you test out things more locally again we’ll do more more napkin math I’m going to try and do some like live mathing We’ll see how well that goes. And then we’ll dive into maybe like another case study that I’ve done recently, just as a way to hopefully surface more and more questions around some of the nitty gritty details. [4:21] Johno: And then like I said, hopefully lots of time for questions. Cool. Does that sound good? Sounds great. Fantastic. Okay. Yeah. So, yeah. Oh, sorry. I just said I’m really excited. Oh, fantastic. Cool. Okay. So this is the loop that we’re going to be talking about. And training and fine-tuning, it’s sort of the same operation in a sense, except that fine-tuning, we’re starting with something that’s already been trained for a little while. What we do is we take some data and we feed it through a model and we get an answer. [5:01] Johno: And then we measure how good that answer is, and we try and update the model, hopefully to make a better answer in the future. Right, then we take some more data, we feed it through, we get an answer, we compare it to the true answer, we update the model, and we repeat and repeat. And in the case of language models, which is what this course is focusing on, usually the correct answer is what word actually came next. And the predictions are the probabilities for what word might come next. [5:27] Johno: So we have some fine tuning data set and we’re saying, oh, you know, I feed my instruction and my response through the model. And at each token, it’s predicting, well, what’s the next token that I look at, especially for the response? What tokens that I predict versus which ones that I actually want? That’s going to be the thing I measure. And that’s going to be what I update with, right? So this is our loose map. And we’re doing this not just in some sort of virtual space, but we’re doing this on actual physical hardware. [5:55] Johno: You usually have a CPU with some CPU RAM. You have hopefully a GPU with some GPU RAM. Ideally, multiple GPUs, each with their own, potentially, a little bit of GPU RAM. Maybe you have multiple computers, each of which have multiple GPUs. So this hardware that we’re running on is going to start to influence a lot of the questions that we’re asking in conjunction with what’s actually going on in that training loop. Okay, so now this is that same loop as before, just annotate it a little bit. [6:23] Johno: What are some of the things we need to keep an eye on when we talk about what makes fine-tuning faster or slower? Why is it difficult? Why do we need lots of GPUs? Can’t I just do it with one? So the thing is, some data that we’re loading from disk or from the internet or from somewhere, that takes up some space, usually not too much in the grand scheme of things. We want to feed that through a model. And now each of these operations that we’re doing, like attention, we’re feeding it through these feed-forward networks. [6:51] Johno: Everything here is maths. Lots and lots of operations. So this is crunching a lot of numbers. This takes time. The GPU has a number of different cores in it, you can imagine. They’re each going as fast as they can. But still, there’s just a lot of numerical operations. You’ve heard flops, floating point operations. There’s a lot of those we need to do. Also, the parameters of the model… take up space as well. [7:16] Johno: So we have the data, but we also need the matrices that we’re multiplying that data against, and then the results are getting multiplied with more matrices. So the model takes up a lot of space too. And then since we want to train this model, we’re also keeping track of something called gradients. Like, okay, if I were to tweak this parameter, how would that affect the output of this layer? And then how would that affect the output of the next layer all the way down to the final prediction? [7:41] Johno: So I’m feeding data through the model, that’s taking a lot of computation. The model is taking up a lot of space, restoring gradients, they’re taking up a lot of space. Eventually I get an answer. So we compare it to the right answer. And then we now want to say, okay, how do I update the model? Again, this is a lot of crunching of numbers to figure out what those gradients are, how I need to update these parameters. And then if you’re using an optimizer to update the parameters, there’s more stuff to store there. [8:08] Johno: So the point of the slide is to look a little confusing. I see someone’s very helpfully annotated the flow. Yeah, we’re still going circular, right? This is still the same as this one. Feed some data through a model, get an answer, measure how good it is, update, right? looping cycle. And here we’re just saying at various points in the cycle, there’s things that involve a lot of computation and there’s things that take up a lot of space. And so what I want you to take from this, keep an eye on those two values, right? [8:34] Johno: There’s things that take number crunching and there’s things that hold memory. So, yeah, keep these two aspects in your mind. These are going to be like two sides of the coin when you talk about performance, et cetera. People in the chat are saying, oh, there should be errors in that slide. Napkin man, who has time for the errors? Okay, so that’s the framing that we want to keep in mind. We’re holding things in memory and we’re shuffling data around and we’re doing operations. Okay, so why do I say shuffling data around? [9:09] Johno: Remember I mentioned this is all running on actual computers. And what that means is that there’s a whole bunch of different types of memory. And I don’t want to get too deep into the weeds on this, but for example, on your physical CPU die, right? This is like an actual piece of silicon with lots of little tiny wires and circuits etched into it. You have some memory that is right next to the CPU. It is like very, very short actual physical distance. The transfer times between those two are almost instantaneous. [9:43] Johno: Then you also have those sticks of RAM that you put in your desktop over to the side. Those are also very fast, right? Things that are in RAM, we usually think of that as fast memory compared to something like stored on your hard drive. So every little piece of this chain, we have different chunks of memory at different stages, and the copying back and forth is going to take more or less time. Same thing on the GPU. On the GPU, you have some RAM that’s actually on the physical GPU die, usually not very much. [10:10] Johno: You have some other RAM blocks usually right next to it that are still really, really fast, but a little slower. You can share memory across GPUs, but then you’re not having to communicate via NVLink or via your PCIe lens on your motherboard. And so if we’re copying data back and forth, like say I’m copying data to the GPU so that I can do some computations. If I’m copying that data from a spinning hard drive. that’s going to take a very long time. If I’m copying it from an SSD, it’s going to be faster. [10:37] Johno: If I’m copying it from RAM, like on the CPU, it’s going to be faster, et cetera, et cetera, et cetera. So we have this hierarchy, and we want to be as aware as we can that putting things in slower memory is going to mean it takes longer to copy them across. And if we have to copy them across every time we go through that loop, that’s going to slow things down. So that’s a big piece of the performance puzzle to keep an eye out on. And again, when we talk about… [11:02] Johno: GPU rich scenarios where you have multiple nodes and each node has multiple GPUs. The network link between two nodes might be like 10 or 100 times slower than the inter-GPU communication, which might again be like 10 times slower than the high bandwidth memory on the GPU. Okay, so that’s a lot of like background, but these are the pieces that we want to be thinking about when we’re talking about what makes something fast. Okay, I should check the chats and questions in case I’m… glossing over things too fast. Someone says my voice sounds AI generated. [11:38] Johno: That’s great. Thank you. Also apologies. I mean, hopefully the recording, you can put it at half speed or something like that. I’ve been told I do talk quite fast. Okay. Some specific questions that we’ll get into at some point. Cool. All right. So let’s plot on. Okay. So the goal in training is we want to keep this GPU fit. And so if I said, hey, I’ve got a brand new idea, my hard drive has a terabyte of space. [12:08] Johno: So why can’t I train a massive model just by keeping the whole model on the hard drive and then layer by layer, I’ll copy it onto my GPU, I’ll do my operations and then I’ll copy it back onto the hard drive. That’s going to be a lot slower than if I could say, okay, I can fit my model in the GPU round. Okay. So maybe we’ve got lots of tricks that we can do to keep things closer to the metal. [12:35] Johno: Maybe we should first switch to the napkin and look at an example of like, yeah, some different types of fine tuning and how much memory they’re using. Hamel, do you have questions or other things that you’ve seen that we should look at before we start putting this theory into practice? No, I mean, I think this outline looks good to me. All right, cool. So let’s switch to the napkin and let’s try and doodle an actual example. Okay, and so we’re going to start with, I saw a question, different types of fine tuning. [13:15] Johno: Let’s start with full fine tuning. So every parameter of the model I am wanting to update. And so on this napkin here, what I’m going to try and do is I’m going to try and keep size somewhat representative of how much memory something’s taking. So let’s imagine we’re lucky. We have a lot of GPU RAM and our model is quite small. So I have my model. This model is represented by a number of parameters. Each parameter is some numeric value, and we’re going to use maybe high precision. We’re going to use 32 bits per parameter. [13:56] Johno: So that’s four bytes. So if my model is 100 million parameters, this is going to be 400 million. or it’s going to be 400 megabytes, right? And the reason this factor of four here, just to be clear, we’re saying we often have multiple bits per parameter, usually 8, 16, 32, right? So one byte is eight bits. If I have 32 bits, that’s four bytes. So for every parameter in this 100 million parameter model, I have four bytes of memory representing that parameter. So that’s why I said this is a 400 megabyte model. [14:38] Johno: Okay, so then I take some data and let’s imagine our data is tiny, right? Like this is a model that takes in one number and it tells you if it’s greater than or less than one, right? This is classic overparameterized machine learning. Why use a small model when a big model will do? So my data is going to be negligible. I put it on the GPU and I want to run it through this model. Now, at the start, the model has no gradients, we haven’t done any operations yet. [15:03] Johno: But when I start feeding the model, feeding the data through the model, what we’re going to end up with is for every parameter in the model, we’re also going to be storing a gradient potentially. And so then I have now a 32-bit value that is separate from the… model parameter that’s like a gradient, right? And then I get my final answer. I compare it to the true answer. Was the label correct? I call backwards. And actually, it’s the backward pass that’s filling in these gradients. Now I would like to update the model. [15:44] Johno: And if I’m using an optimizer like stochastic gradient descent, the optimization step is just looking at these gradients and then figuring out from there how to update the model. but some fancier optimizers and specifically something like Atom, which a lot of us use, they don’t just use the gradient from this particular training step. They have ways of accounting for like momentum, right? So that’s another set of parameters for momentum. They might have like a variance measure of like the last few gradients, have they all been very similar or have they been very different? [16:17] Johno: So we can end up with potentially multiple sets of numbers in the optimizer state, each of which is the same amount of parameters, same amount of storage as the model. Right, so you can see… So is it usual to have like, I mean, is it this pictorial representation? Is the optimizer taking up three to four times more space than the weights? Yes, with lots of caveats. So often people will use, you’ve heard of like maybe 8-bit Atom. [16:47] Johno: Right, was a thing where we said, okay, rather than representing these in full precision, you know, maybe I’ll have my, I’ll use smaller little 8-bit precision rather than 32-bit precision buffers to store the momentum or whatever, right? We’ll have some sort of clever filtering or clever quantization that compresses this. There’s also like different optimizers will try and say, how can we get away with less state, right? [17:19] Johno: So I have my gradients and maybe I only have one other value, or maybe what I do is I have one value for every block of gradients, you know, so for every layer maybe. So then I suddenly end up with a much smaller set of stored states. So this is a lot of the juggling that goes on. But in general, for the high-performing optimizers that everyone uses, you do end up with, yeah, my model takes up 400 megabytes. The gradients take up 400 megabytes. The optimizer state takes up 400 or 800 megabytes as well. [17:49] Johno: So it is a lot of overhead. What we’ll see is that this changes when we come to the kind of LoRa LLM fine-tuning that we’re talking about. Boom. Okay, so this is one example here. When we’re talking about language model fine-tuning, we are in a slightly different situation, right? Because we usually have a very big model, but often we’re using something like LoRa specifically because I don’t have space on this GPU to fit four copies of this model, right? I just actually don’t have enough space. [18:27] Johno: So one of the tricks at our disposal is to say, well, why don’t I keep most of this model frozen? In other words, I’m not going to be updating them. And if I’m not updating them, I don’t need gradients and I don’t need optimizer state. right? But what I’m going to do is maybe just some layers, or maybe I’ll add a few extra parameters, right? [18:48] Johno: Like some small subset, 1% of the weights I’m going to add for every big, you know, 1000 by 1000 matrix, I’m going to add a 1000 by 32 matrix, something like that, right? So much smaller subset of the weights. And these are the ones that I’m going to be keeping track of gradients. And these are the ones that I’m going to be optimizing. So now, even though I don’t have room for four copies of this whole network, that’s fine because I only need one copy. I can feed my data through. [19:19] Johno: So I get some data, I feed it through, get my answer, and now my gradients are only for these trainable parameters. My optimizer state is only for these trainable parameters. And so I’ve still got some room here. So that’s one of the reasons why LoRa was so popular is it’s saying suddenly… you don’t need 4x your model size to train. You need just enough room for your model plus 4x your trainable parameters, which are usually like 1% of the model size. All right, so if my model… [19:51] Johno: Just for the audience a bit, I see people trying to take notes and write down an outline. I guess maybe at the end, maybe you can enumerate right down on whatever all the different components. And then we can just pay attention to this. I don’t know. I think the collaborative note taking, some people find helpful. And that’s also nice for me because I’m going to be like, what I should be doing is redrawing this diagram multiple times. So I should have. Maybe made a new one for the lower. [20:20] Johno: But I think it might be easiest if I can just erase things, scribble over things. So we might not end up with much of an artifact here. So if people feel like they can take their own notes, that’s even better. Okay. Maybe it’s useful to actually write down, just say the name on the side. Like, hey, yeah. All the components. Okay. So I mentioned in the slides, we have lots of tricks at our disposal to try and beat this situation where we’re… [20:47] Johno: running out of memory, we’re having to put things in slower memory, and that’s slowing everything down. Okay, so lower was one, only training some parameters, right? So I’ve got fewer parameters to train, smaller sets of gradients to keep track of, smaller optimizer state. So let’s look at another one that’s quite popular these days, right? Here’s my GPU. If this is how much the model would take in 32-bit, we can say, okay… can I represent that with fewer bits per parameter? So we call this quantization. [21:22] Johno: And so where we would have had this big model, I can now say, well, rather than using 32 bits to represent every parameter, can’t I get away with some smaller number? So 8 bits, suddenly I’m using already a quarter of the size versus 32 bits. And if you’re really crafty, you can compress that down even further to something like 4 bits per parameter. [21:46] Johno: So now I have my frozen model, the weights are stored in this quantized state, we’re not going to go into what quantization is or all the nitty gritty of how that’s done, but it just means that these take up much less room. And so suddenly, from 60% of my GPU being taken up by these 32-bit weights, we have gone from 32 bits to 4 bits. So that’s an 8x reduction. So now we’re at… 5, 6, 7, 8% of the GPU. Or we can train a 5 or 10 times larger model. You can combine these techniques. [22:22] Johno: Let me just put a Q for quantization. So you can keep the base model quantized and then you can have trainable lower layers that are still in full precision. But because they’re so small, The total size taken is a lot less than if you had a full model in 16-bit. I now have a full model in 4-bit and some lower layers in 16-bit, but the lower layers maybe add up now to 10% of what the quantized model is. So this is another big trick we can do to get more space to do more things. [22:55] Johno: So if you’re trying to train, like here’s a very actionable thing. Oh, I’m trying to train a model, it just doesn’t fit on my GPUs. It’s like, okay, maybe try out LoRa, right? So now you’re only training a much smaller set of parameters. See if that fits. Okay, cool. This is great. I want to go to an even bigger model and now it doesn’t fit on my GPU even if I’m just doing LoRa. Like, okay, maybe consider quantization. So do look up QLoRa, right? There’s a load in 4-bit equals true thing in Axolotl. [23:22] Johno: A lot of other trainers will support this. And so now you’re saying, okay, I’m quantizing my model and I’m putting it on and we can go from there. There’s a question when I do View Lower, is both weights and gradients quantized or only one of them? Is there pros and cons of choosing full precision of gradients over weights? So yes, what we usually do is we just quantize the frozen base weights and then the lower parameters we still keep in higher precision and the gradients we still keep in higher precision. [23:51] Johno: So, um, There’s a few reasons. One, we can get away with it, right? Because the lower parameters are still some small subset of the whole. But two, it’s very tricky because what we’re usually doing during training is we’re making very small updates to the weights. And if you have only a four-bit value and you’re trying to train that directly, You might calculate your gradient and then your update might be 0.001 in some direction. But because I’ve quantized the value so aggressively, the possible values I can represent with four bits might be 0 or 0.2. [24:23] Johno: And so if I’m saying, okay, I’m going to try and add 0.001 to 0. Okay, it’s 0.001 now, but then I’m quantizing it so it still stays zero. We don’t actually get an update. There are tricks like the 8-bit optimizer does some various tricks to try and get effectively higher precision with something called the Kalman filter and all these clever tricks. But in general, you want your trainable parameters and higher precision. You want the base weights in low precision if you can get away with it. And all of this is usually found via experimentation. [24:52] Johno: So we tried 8-bit training that was very popular for a while. Then people found like, hey, as long as we’re doing lower and the lower is still in 16-bit, we can get away with… 4-bit, 3-bit, even 2-bit compression of the main bass weights. But at below 4-bits, you start to see really some performance degradation. So that’s kind of like the sweet spot. Maybe for some models, it’s 6-bits. For some models, it’s 2-bits, whatever. But yeah, this is the sort of thinking that people are doing. Like, okay, where can I keep full precision? [25:22] Johno: There’s some layers that we never quantize down, or at least we keep the activations in high precision, things like the position embeddings, layer norm. But for the most part, it’s like wherever we can, we want to compress it as much as we want. But for the things that are being trained, we still keep them in high precision. Okay, so these are some of the tricks. There’s more. If we look back at my slide, there’s a few others I listed. CPU offloading. [25:48] Johno: If we get to the stage where we’re saying, look, here’s my model, and here’s my GPU. Oh, I can quantize my model. So here’s my quantized model. It’s still not going to fit. I don’t need to keep gradients. I don’t need to do other things. So at this point, we can say, well, I have my CPU RAM. And it’s, you know, CPU RAM is cheap. I have 256 gigs or 128 gigs in my machine, even though my GPU only has 32 gigs. So I’ll kind of like declare bankruptcy. [26:19] Johno: And what I’ll do is I’ll keep the GPU like empty for calculations. And I’ll only put one layer at a time of the model. And then I’ll do some operations and then I’ll store back the gradients, the weights and everything on the CPU. I’ll bring the next layer on. I’ll do some stuff on the GPU. I’ll put it back so we can get even more size in terms of models, but there’s a trade off. There’s some copying. Okay. So this is all to do with memory. [26:49] Johno: Specifically, I’ve been talking about the weights and the gradients and things. There’s a few other considerations to start thinking about now, and maybe we’ll jump into the code shortly and then switch back to the napkin. But one really good question is, at what context length do activations become significant enough for us to start thinking about? And that is an excellent question because this first explanation here, this is what you’ll see in a lot of places online. But it definitely doesn’t tell the whole story. And so let’s go back, let’s draw another GPU and a model. [27:25] Johno: And we can go whatever, here’s my gradients, say. Before I said, oh, here’s my data, and we’re going to feed it through. Now if we’re feeding a lot of data, like I have a really large batch size or a really long sequence, suddenly the size of that starts to matter. And every layer in this model. I’m storing the outputs of that layer to be fed into the next layer. And I often need to keep those around to be able to calculate the gradients. [27:53] Johno: And so what you end up with is then, okay, I’ve got my input data. It’s probably still quite small relative to the model size. But for every layer, I’m keeping around these output activations so that I can calculate the gradients. And these really start to matter. These start to add up. And suddenly, you might see a situation where, like if any of you have tried long context length training, this starts to be a problem. So let’s switch over into… Well, actually, first of all, I’ll show you how you can run some experiments like this. [28:22] Johno: And maybe I should have started with this before we went straight to the napkin. But then I’ll look at some code and show some concrete examples of the stuff we’re talking about. Okay, so one way to run experiments is to… do two versions of your training, right? If you follow the modal, getting started with axolotl docs, and I did this, this was my first time using it, but you get a nice configuration file, you can change the batch size, right? Which on an AppCon, that’s going to be changing like maybe how much data we have. [28:54] Johno: You know, there’s one sample, another sample. So if I have a higher or lower batch size, there’s more data. You can run that through and you can see how long it takes. Another way is to go directly into code and to try and look at these individual steps more close to the metal, rather than having to rely on multiple minutes of training on some GPU in the cloud. It’s sometimes nice to have a smaller example. So that’s the notebook that I prepared. We’re just going to run through… [29:24] Johno: doing one cycle effectively, or not even, like we’re not even going to worry about Optimizer, just feeding some data through the model and looking at the gradients and starting to see like, how do we understand a question like this? How does context length come in? How does batch size come in? When does that start to matter? Okay, so we have a notebook here. We’re going to be using a few different things. One, PyTorch has some nice built-in memory tracking options. And so we can print out… the memory allocated and the memory reserved. [29:57] Johno: Memory reserved is what you’ll see in something like the weights and biases, memory usage logs or NVIDIA SMI. But sometimes it’s like we’ve put stuff in memory. We’ve actually deleted it or we’re not using it at the moment or PyTorch has put it there ready for the next computation. But if we didn’t have enough space, it wouldn’t really matter and we could just load that as needed. So memory allocated is maybe a slightly better measure of how much you can get away with if you have 24 gigabytes of GPU RAM. [30:26] Johno: and you can do something that memory allocated is 23 point something, right? It’ll be a squeeze, but you might fit in. Memory reserved might show 24 gigs used, but actually only 18 might be allocated. It’s a small difference, but these are two measures of like GPU RAM used. Okay, so we can do something like load up a model. In this case, TinyLama, it’s a very small, well, still over a billion parameters. So it’s kind of funny to say small, but in the days we live in, that is small. [30:53] Johno: We can load it in 4-bit, so this is doing the quantization. We can set up LoRa, so this is now making sure the base model is frozen, but we have these trainable parameters that are a small subset of the total. We can create a batch of data, and with data, usually you’re thinking about, wait, where is the text? This is the language model. Each token of text gets transformed into a number, and so that’s why I’m just using random integers here. [31:22] Johno: And then we have a list of them that is 12,000 tokens long for each example in the batch, and in this case batch size is one. I’m going to feed that data through my model, and then I’m going to calculate the gradients, and then I’m going to print the memory usage. And if I run this, we’ll see there’s some amount of memory used. Okay, so this is where we can start to do some napkin math, right? So first it might be to say, what if the data was really, really small? And that ran too fast. [31:58] Johno: I was going to say, make your predictions how much space is going to be here. But you can see this is pretty small. It’s a small model and it’s quantized. As we increase this. we’ll see that suddenly the memory usage actually is quite a lot bigger. And so you can play with this. In fact, I put a little exercise there. I may have left the solution in the notebook that I shared, but just try this out. [32:22] Johno: Plot your memory usage versus context link or see what happens when I say, okay, what if I double the batch size here? So let’s do that. Double the batch size. Suddenly I have a higher memory. So you can see here, this is a lot easier than like, I kick off a training run on modal, I wait for it to go, I take time for the training to finish, and then I go look on weights and biases and I see which finished. Here we have this very like concrete way of checking things out. [32:52] Johno: So I could do, for example, load in 8-bit, I think this is a thing that one can just say. And see what that does. Cool. So I’ll get a different memory usage there. If I didn’t load the model with any quantization at all, right? So we’re just loading it now. Sorry, where is the print memory stats thing coming from again? That’s just up here. So this is just like a nice wrapper around torch.cuda.maxMemoryAllocated and maxMemoryReserved, which I think used to be called maxMemoryCached. [33:31] Johno: So if we load it without any quantization, I’m expecting those numbers to be even slightly higher. Yeah, suddenly we’re up to 8 gigs instead of 6. So this is a little thing for you to go play with. I’d love for you to explore, like, what if I turn on CPU offloading, turn on and off gradient checkpointing? As always, there’s complication. Question about that print thing. Yeah, yeah. Does that account for all of the GPU memory that you might care about? [33:56] Johno: So I’ve tried using this before, and I’ve found, like, yeah, like, discrepancy between, like, what weights and biases logs and what that prints out. I’m just curious if you’ve experienced anything like that before. This will give you a pretty good idea, except, I mean, maybe look at the higher one, just to be safe. Aim to not fully saturate your memory because sometimes during training, if you’ve got a bit of extra, it can do things like prefetching the next layer or whatever. It’s sometimes helpful to have a little bit of headroom. [34:28] Johno: And then also, as soon as you’re talking about a distributed setup, I’ve got multiple GPUs or I’ve got multiple nodes, there’s things like if my weights are spread out over the GPUs, PyTorch needs to be able to combine them and prepare them in ahead of doing the computation. So sometimes you need a little bit of extra overhead there. [34:46] Johno: But in general, this is a really good way to track, you know, if I have a 24 gigabyte GPU and then this is saying, you know, 20 gigabytes, then I probably can’t do my training on a 12 gig GPU, right? It’s a pretty good approximation. Okay, so that’s one way to like get insight. If you really want to dig deeper, there are some other tools. So… Here is a cool little function that’s maybe not as documented as it should be. [35:13] Johno: If you turn on record memory history, PyTorch will log exactly what is taking up memory over time. So if I run this cell, it’s going to take a little while, and then it’s going to spit out a pickle file, which if I go to this PyTorch memory site, sorry, just a sec, I’ll need to find this file. Yeah, here we go. I can drag this on. This is now over time, the total memory usage. You can see here on the left, figures in gigabytes. [35:49] Johno: And so this is a really instructive way of looking at things because you can see here, this like base usage that was there at the start, that’s the model weights. The model weights are already on the GPU quantized in this example. So they’re pretty small. And then as the data starts going through the model. We’re building up more and more memory that’s being used by the activations. We get to the end, there’s a little spike during the cross-entropy calculation and the final language modeling hit. And then in backwards, we’re now calculating gradients. [36:24] Johno: So you’ll see that these gradients are appearing. But once you’ve calculated the gradients, we can kind of let go of the activations and so we’re reducing the memory overall. This will look very different if you’ve got gradient checkpointing turned off. It’ll look very different if you’ve got quantizers unquantized. Different batch sizes, you’ll see a lot of model training that we’re doing. The base usage of the weights might be most of it. And then you’ll see a little up and down as you get more activations and things. [36:50] Johno: But it’s a very, very fun way to get an idea for, hey, this is what’s actually going on. And if there’s some big spike, you can click on that. And you can try and pass this big list of this is what’s going on. occasionally, if you’re lucky, you can then figure out what is actually causing that and what you need to do. So very underrated tool. And I’m putting it in there mostly for like, this is like, you might only use this if you’re desperate, but it is useful to know. [37:16] Johno: And it’s also cool for like, if you look at the PyTorch tutorials, some of them have started to incorporate this to say, okay, here’s what’s actually going on. You can see, oh, we’re using this new special technique that does the updates. with the optimizer as it’s doing the backward pass. And so then we don’t have to like store these gradients and then do the gradient up. So there’s all these different little tricks. So this is a way to get a much deeper insight into that. I have a question about this. [37:43] Johno: So if you do like prefetching and stuff like this, does this like smooth out into more of a plateau-ish looking thing? Or how does it… Yeah. Sorry, prefetching? Oh, like of your data loader and you’re trying to shove it through the model? Not really, because the data might already be on the GPU here, but it’s all the intermediate activations that stack up. I see. And so maybe this is a good bit of napkin math we can do. [38:17] Johno: One of the links I shared was to a PyTorch issue where someone asked, what’s the deal with this activation memory on… on this model and whatever. So here’s what the model looks like. We’ve got some number of layers. They each have, there’s a hidden dimension. It is possible, and maybe my talk was misleading because I’m not talking much about this kind of actual math, but it is possible to go and do some calculations and figure it out. Specifically, if we go to the slides, right, they’ll be here. [38:53] Johno: In the slides, we had a link to some issue, this one. And the person gives a formula, right? So there are ways to calculate, okay, every attention layer is going to take this type of input, produce this output, and then it’s going to multiply that with that and take that output, and then it’s going to do these operations. So we can take this formula and we can just run this for our model. So let me go back to tldraw. I’ll keep this on a different screen. [39:24] Johno: Okay, so they said activation memory is equal to S, B, H, 34 plus 5AS over H. A lot of these, if you go read the paper that they link, these are all terms that are defined. S is the length of our sequence, which in our case was 1,000 tokens, I believe. Batch size is 1, so we don’t need to worry about that. Hidden dimension is 2048. This is the activation memory per layer. We said there was 22 layers. What else do we need? I think the number of attention heads is what A is. [40:04] Johno: I think that’s 8. Okay, so let’s run this calculation. We have… 1000 times 1 times 2k, right? So this is 2 million-ish times by 34 plus 5. I said a was 8, so that’s sequence length, which is 1k. h is 2k, so this is 4. So that is 20. This is 54-ish, call it 50. So this is about 100 megabytes. And you can see, like, I’m not worrying about this multiple bits. I’m kind of rounding off everything that I can. So for 22 layers, this is… Have I done this right? Yes, so this is… [41:06] Johno: 2.2 gigabytes? Is that right? No. Right. Because there’s floating bit. I feel like this might need to be multiplied because you’re doing everything in 16-bit. Oof. No. Hmm. Okay. Because on our plot, right, we didn’t have We didn’t have about 2 gigabytes, we had about 4 gigabytes. I should probably have rehearsed this. And on that plot, I guess like, okay, that plot is showing there’s a reserved amount. And does that stay flat throughout the whole? Oh, like this base thing here? Is that what is reserved? It’s more like these are the model weights. [42:00] Johno: So the model weights… You had two numbers, right? Like the higher one and the lower one. Yeah, this is active memory. So this is the slightly lower value of the two. This is things that are actually active. And so, for example, as these intermediates are being thrown away, they might not be immediately cleared out of the cache or whatever. So they might still be reported as lingering around in the reserved value, but not in the active value. Can you talk about an example where you looked at this graph and it helped you? [42:31] Johno: Like, you know, get unstuck on something or something? Yeah, sure. I mean, one… I found some bugs because the spike was suddenly extremely high due to an incorrect implementation. I’ve also seen cases where, okay, so now we have like maybe some optimizer state or some gradients or something like that. Those were larger than they should have been because I hadn’t frozen all the weights I should have frozen. If you call loss.backwards and then you do your update, but then you don’t delete the loss, some gradients might stick around rather than being forcibly cleared. [43:07] Johno: And so then your second training step might actually take slightly more memory. That’s been me before. And this graph was really helpful to log the memory over several steps to check that it wasn’t just continuously escalating. That was pretty helpful. What else? Oh, and then in finding, there was a thing where in… One of the recent versions of Transformers, I think 4.37 or something like that, HuggingFace changed their implementation. They did some optimization to make things faster for inference with KB caching and whatnot. [43:41] Johno: But what it meant was that they weren’t actually using the right efficient kernel. They were using the efficient attention rather than the flash attention. But efficient attention means like compute efficiency. But for training, it was now using a lot more memory. and so we could come in and see before and after like his version you know 0.36 and here’s version 0.39, which of the pieces of this graph are larger? [44:05] Johno: This is not the interactive one, but if you hover, you can see which chunks suddenly went from 100 megabytes to two gigabytes or whatever on longer sequences. And we could say that’s the culprit, see which kernel was being called. Wait, that looks like it’s from a different attention kernel to the one that we expected. Go and find the bug, go and, please change it back to the flash attention for training. Yeah, I see. Does it take a lot of practice to read the logs when you hover over those things and you get those long logs? [44:35] Johno: Yeah, I would say don’t even worry too much about that. This is more to just get a general picture. I’ve got some memory allocated. I’ve got some memory that’s going up and down. If I change the sequence length to be smaller, the base is not going to change, but this little spike will be lower. And so this starts to give you a feel for where you can have these tradeoffs of, oh, if my model is taking most of the space and I really don’t have much space for the… [44:59] Johno: the actual batch data, then I’m reduced to doing batch size of one on short sequences. That’s not ideal. I’ve just realized we’ve gone longer than I wanted on this. I should have maybe just… left this as an exercise for the reader. I’ve shown how you can use a smaller model here. It’s my fault, sorry. No, no, it’s totally on me. What we should do is go back to this links page and as a, well, okay. So let me jump in. We have another speaker. [45:34] Johno: Right on the hour, so we’re not going to be able to go over. We’re not going to be able to go over. Okay, so I will tell people, maybe just go check out this link. [45:44] Johno: The reason that I’m talking about being able to estimate memory usage and things like that is that, especially for a GPU-poor situation, where maybe you don’t have the fastest interconnect between your GPUs or you don’t have that much GPU memory, If you can go, like every cycle, every time I’m loading the model weights from CPU onto GPU or something like that, that’s taking a lot of time. Then I do some computation and then I have to load the next layer and that takes some time. Then I do some computation. So that data transfer takes time. [46:22] Johno: which means that if I can find any way to do more computation before I then have to copy data across, that’s a good thing. And so this example here, this 3090 basement rig, you’ll notice that all the tricks in the book, quantization, using QLoRa rather than just LoRa, anything that we could do to use a larger batch size resulted in quite a significant speed up. right? Because now I’m doing fewer cycles overall because I can do 32 samples at once versus eight or something like that, right? [46:56] Johno: And this in general is going to hold true for a lot of training where you’re memory bandwidth constrained on slower machines, cheaper machines. And so then you’re really trying to think like, yeah, how can I optimize this? How can I maximize either the sequence length or the batch size that I can fit? And so all of these tricks come into play. [47:16] Johno: But then if you go and read through this post later on, we try some different machines and one that stood out was once you get to an H100 with the super fast like SXM is like the proprietary server version of NVLink, the communication between the GPUs is so fast, and the GPUs have so much memory, that you can keep the model weights in that fast memory. [47:38] Johno: And even if they’re spread across multiple GPUs, you can load them so fast that you haven’t even finished doing the computations for the last layer when the next layer is already loaded. And so suddenly you’re not memory bound at all, you’re compute bound. And in that case, if you do a batch size of eight versus a batch size of 12, you’re doing fewer steps, but you still have to crunch the same total number of numbers. And so the time doesn’t actually change that much. [48:05] Johno: And so in the benchmarking example, where we were looking at the axolotl version, where I said, hey, here’s a way to run experiments. Using the batch size from 16 to 32. it didn’t actually change the runtime that much, right? Sorry, from 32 to 64. And quantizing, it didn’t really change the runtime that much either. And the reason is, okay, quantized weights might be faster to copy from the CPU to the GPU, but that almost doesn’t matter because it’s all about like, how fast can you crunch those numbers? [48:37] Johno: So the like dark red part of my diagram. Whereas if this was on a slow machine or a machine with good GPUs but slow interconnect, it really does matter being able to fit a larger batch size so you can do fewer total batches, so you can do fewer total loads. So there’s this kind of juggling trade-off that one ends up doing. Okay, so I should see… There’s lots of rabbit holes we could go down further. [49:01] Johno: I should see what questions are in the chat that I can help rather than trying to finish everything I wanted to cover. Okay, does CPU offloading make any practical sense for training or only for inference? So I found it helps specifically in a case like mine. There’s no way I can fit a 70 billion parameter model on one GPU. And even though I have maybe several 3090s, if I spread the model weights over those GPUs, there’s very little overhead left for the actual data and the activations and so on. [49:33] Johno: And so even though it’s slower to copy the model weights from CPU, because I can fit a much larger batch. I only have to copy the model weights from the CPU once to then process like 32 samples versus having to process one or two samples at a time because there’s so little overhead. So yeah, CPU offloading does actually give you a reasonable speed up often if you’re in this case where you really just need more room. [49:59] Johno: As soon as you’ve got more capacity, you’re training smaller models or you’ve got 80 gig H100s at your disposal, then definitely not. So my recommendation actually, if you check out the bottom of that benchmarking post, there’s like a sequence of steps. Start with the default, right? None of these optimizations turned on. And then slowly go through, like turn on quantization and then see if you can increase the batch size as much as you can and then check, did that actually give you an improvement? Right? [50:25] Johno: If you’re in a FSDP scenario where you’ve got multiple GPUs, start with just data parallel, no sharding. Then, see if the sharding gives you an advantage. Then see if the full sharding and cross nodes, just like there’s a nice little sequence there that Kerem wrote up to say, yep, start with the basics, lower, no quantization, just vanilla data parallel if you’re on multi-GPUs. And then slowly add, if you find that you actually really have so little overhead that you’re needing a very small batch size, you can slowly start to add in. [50:56] Johno: Quantization, and then maybe CPU offloading, you know, definitely have gradient checkpointing, all these little tricks that I list, just turn them on one by one until you find like the sweet spot where okay, at this point, like I can now go fast. Could I share the notebook? I’ve put it in the discord, we’ll probably also then put it up as a gist or something like that and share that. Sweet spot with quantization between compression and accuracy. Yeah, it seems to be especially Quantization plus adapters seems to be a really nice thing. [51:24] Johno: I think if you’re just doing quantization below maybe six or four bits, you start to see a drop. But if you can then correct for that with a lower that you train, you can go, it seems like maybe two bits is potentially doable. But four bits for me is like a nice default that’s just, okay, if I’m training four bit plus a lower, I can usually get the same performance I’d get without any quantization. And then as long as you’re keeping the lower in high precision. Yeah, that’s pretty nice. [51:54] Johno: Some discussion around gradient accumulation steps and micro batch size. Gradient accumulation is where if I want to do, say I wanted to target a batch size of 32, but I can only fit a batch size of eight on my GPU. What I can do is I can run four batches before I do an update step. And so then it’s effectively the same as having a batch size of 32 from the learning dynamics perspective. But I’ve been able to do it in four micro-batches. [52:26] Johno: So this is useful if you’re trying to target a specific, like, oh, I want to match what someone else did, but they had an A100 and I only have a 3090. And yeah, that’s a useful way if you’re… If you’re training at batch size 1 or 2, you probably want to be using gradient accumulation to get that to a slightly larger value. But you don’t need to go crazy. You don’t need to target a batch size of 1000. 32, 64, 16, these are all reasonable values, especially just for a little bit of lower fine tuning. [52:57] Johno: What is the x-axis on the plot? I think that’s probably the memory profile. That’s time. So that’s over time as we do more computation. You can see forward pass, backward pass. That’s kind of like the flow. So the data is going layer by layer through that transformer, and then we’re back propagating those gradients back. If you have multiple lowers, do they update the same parameters or do they each update a different subset? You could target different ones, I guess. Usually, if I’ve applied several lowers, each lower specifies which layers it’s targeting. [53:32] Johno: Maybe it’s the up-projection matrix, the down-projection matrix, and the attention matrices. So you can look in the lower config. If people talk about having multiple lowers applied, they’re usually updating the same weights. Any other questions that you guys wanted to raise to the top? I’m looking at most highly uploaded on… I think we got this top one. I will also… I’ll keep an eye on the Discord. Sorry, I said I would be able to multitask. I totally couldn’t. But I will try and answer questions in that channel even going forward. [54:20] Johno: But maybe to summarize, like to step back and summarize, let me stop sharing my screen. Um… Napkin math for fine tuning. We’ve said we’re trying to juggle shuffling data around as little as possible and trying to get through all the computations we need to do this training. And so things that take up space in memory, the model weights, the gradients, et cetera, tricks that we have at our disposal. Lower means we only need gradients for a small subset of the weights. Quantization means we can store those weights with fewer bits. [54:52] Johno: Experimentation is key to see where these trade-offs are. You can measure the amount of memory allocated. You can go and try and tweak these things and see experimentally how does this work. As your batch size or your context length increases, you need more space for activations, intermediate values. And so suddenly you’re not just dealing with the model and optimize the state. You suddenly have this extra component that scales as you go longer sequences or longer batches. [55:16] Johno: So if you’re finding you’re running out of memory and you really and you can reduce your context length, that’s fine. If you can’t, then you need to start looking at the other places that you can save memory, like quantizing the weights, like offloading them to the CPU. It’s possible to calculate all of these things, although my one attempt to actually show that maybe was off by a factor of two, because I forgot to account for the data storage type. But it’s often like… [55:43] Johno: rather than trying to calculate from a, here’s this big formula I found, calculating from a, look, I measured, and with a single layer, if I do the forward and backward pass, this is how much the activations took, this is how much the weights took. I can kind of guess, okay, for a 7 billion parameter model in 16-bit, that’s 14 gigs of memory. Then I need, the lowers are 1% of that, but I need, you know, activation there. Okay, cool, that’s maybe another gigabyte. [56:08] Johno: And then for every, like, thousand token I add to my sequence length, I need another gigabyte of memory, you can start to quite quickly get a feel for, okay, I can probably go from the one that I’m currently training at, I can probably bump my sequence length up to here before I need to think about buying a more expensive GPU or renting out. So this is the space that we’re operating in. [56:30] Johno: Lots of parameters to tweak, but hopefully this talk has given you a bit of a feel for where the key things are coming from, why people even care about things like LoRa, things like quantization. tricks like CPU offloading. Yeah, I hope that’s given you some context. I will be on the Discord for further follow-up.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Napkin Math For Fine Tuning"
    ]
  },
  {
    "objectID": "education/fine_tuning/mistral_ft_sophia.html#chapters",
    "href": "education/fine_tuning/mistral_ft_sophia.html#chapters",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nSophia Yang introduces herself and provides an overview of the talk, which will cover Mistral models, their fine-tuning API, and demos.\n0:35 Mistral’s History and Model Offerings\nSophia discusses Mistral’s history, from their founding to the release of various models, including open-source and enterprise-grade models, as well as specialized models like CodeStraw.\n02:52 Customization and Fine-Tuning\nMistral recently released a fine-tuning codebase and API, allowing users to customize their models using LoRa fine-tuning. Sophia compares the performance of LoRa fine-tuning to full fine-tuning.\n04:22 Prompting vs. Fine-Tuning\nSophia discusses the advantages and use cases for prompting and fine-tuning, emphasizing the importance of considering prompting before fine-tuning for specific tasks.\n05:35 Fine-Tuning Demos\nSophia demonstrates how to use fine-tuned models shared by colleagues, as well as models fine-tuned on specific datasets like research paper abstracts and medical chatbots.\n10:57 Developer Examples and Real-World Use Cases\nSophia showcases real-world examples of startups and developers using Mistral’s fine-tuning API for various applications, such as information retrieval, medical domain, and legal co-pilots.\n12:09 Using Mistral’s Fine-Tuning API\nSophia walks through an end-to-end example of using Mistral’s Fine-Tuning API on a custom dataset, including data preparation, uploading, creating fine-tuning jobs, and using the fine-tuned model.\n19:10 Open-Source Fine-Tuning with Mistral\nSophia demonstrates how to fine-tune Mistral models using their open-source codebase, including installing dependencies, preparing data, and running the training process locally.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Best Practices For Fine Tuning Mistral"
    ]
  },
  {
    "objectID": "education/fine_tuning/mistral_ft_sophia.html#resources",
    "href": "education/fine_tuning/mistral_ft_sophia.html#resources",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Resources",
    "text": "Resources\nLinks to resources mentioned in the talk:\n\nmistral-finetune is a light-weight codebase that enables memory-efficient and performant finetuning of Mistral’s models.: mistral-finetune\nExample notebook\nFine-Tuning guide\nSpaces in the prompt templates for different versions of Mistral: tweet\nMistral Inference guide.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Best Practices For Fine Tuning Mistral"
    ]
  },
  {
    "objectID": "education/fine_tuning/mistral_ft_sophia.html#full-transcript",
    "href": "education/fine_tuning/mistral_ft_sophia.html#full-transcript",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:01] Sophia Yang: Okay, cool. Yeah. Yeah. Thank you so much everyone for joining this course and this talk. I’m super excited to be here. My name is Sophia Young. I lead developer relations at Mistral. So, yeah, sorry, it’s June already. Online of this talk, this talk is we’re going to talk about an overview of Mistral models. We’re going to talk about our fine-tuning API that we just released today. And also we have an open source fine-tuning code base that you can play with. And then I will show you some demos. Some brief history about Mistral. [0:40] Sophia Yang: I hope all of you know some of Mistral, but if you don’t, we are a Paris based team of more than 50 people. We were founded about a year ago and in September last year we released our first model, Maestro 7B. And then December we released Maestro 8x7B with a lot of good feedback on both models. And we also released our first commercial model, Maestro Medium, and also the platform where you can use our model on the platform through our API. And then February this year we released Mr. Small and Mr. Large. Mr. [1:20] Sophia Yang: Large is our flagship model with advanced reasoning, multilingual capabilities, function calling, all the good things. And Lachat is our conversational AI interface. It’s completely free. So if you’d like to talk to our models, please sign up and use Lachat. And in April this year, we released, at the time it was the best open source model, A times 22B. It was really good. [1:49] Sophia Yang: And just last week, we released CodeStraw, which is a specialized model trained on 80 plus languages, programming languages, and you can use with various VS Code plugins to generate code and talk with your code. And here’s another view of our model offerings. We have three open source models, Apache 2 license, which means you can use it freely for your personal use cases or commercial use cases. We have two enterprise grade models, Mr. Small and Mr. Large. Yeah, so for your most sophisticated needs, Mr. Large is really, really good. [2:33] Sophia Yang: It supports multilingual function calling, really good at RAG, And now we support fine-tuning Mistral-small and Mistral-7b. And again, we have specialized Postal for coding, and we have an embedded model. So we really care about customization. A lot of people asking about, like, how do you fine-tune Mistral? A lot of people want to have a recipe or want to use our API to fine-tune our model. So about… Two weeks ago, we released Mr. FineTune, which is a fine-tuning code base everyone can use to fine-tune our open-source models. [3:18] Sophia Yang: Then just two hours ago, we announced a fine-tuning API so you can customize your Mr. Model using our fine-tuning API directly. And yeah, so the technology we use is LoRa fine tuning. Since this is probably the end of the course, you probably already know a lot about LoRa. It’s very efficient. It’s very performant. So we did some analysis on the comparison between Mr. LoRa fine tuning and a full fine tuning on Mr. 7B and Mr. Small. So they have really similar performance, as you can see here. [4:00] Sophia Yang: MR7B, lower fine tuning on this benchmark is 0.9 and the full fine tuning is 0.91. So very, very close. Note that this benchmark is an internal benchmark normalized to this MR small number. So just some note there. And then before I show you some demos, just some thoughts on prompting and fine tuning. You probably already learned about this. So before you start fine tuning, you should always think about maybe you can just do prompting. So with prompting, your model can work out of the box, doesn’t require any data or training to make it work. [4:44] Sophia Yang: So it’s really easy and it can be easily updated for new workflows or prototyping. With fine-tuning, sometimes it works a lot better than prompting. It can work better than a larger model, faster and cheaper, because it doesn’t require a very long prompt. So for specific use cases with a large model, if you have a sophisticated prompt, you can make it work. But with small fine-tuned models, you can just fine-tune it on specific behavior that doesn’t require a long prompt. [5:21] Sophia Yang: So, yeah, again, it’s better alignment with the task of interest because it’s specifically trained on different tasks and you can teach new facts and information. Okay, so now I want to show you some demos, basically how to use our FineTune API and also how to use, excuse me, our Mr. FineTune, the open source code base. specifically, I want to show you four demos. Demo of how can you use a fine-tuned model that may be fine-tuned by yourself or by someone else. Demo of some developer examples, like real-world use cases. [6:07] Sophia Yang: And also the API work through and the Mistral fine-tune walkthrough. So let me move this bar. Okay, so in this notebook, first of all, we need to install Mistral API. This is the latest version, that was released today. So if you want to use our fine-tuning features, make sure you have the latest version. And in this notebook, I’m actually using three fine-tuned models shared by my coworker. [6:49] Sophia Yang: So if you are in an organization, you want to use models created by your coworker, or you want to share the fine-tuned model you yourself created, you can use the model from the same organization with different sets of options. So it’s really easy for me to use. a model that my coworker has fine-tuned today. So for the first example, it was trained on title abstract peers from archive. So basically if you input a title of a research paper, this model will generate a abstract for you. [7:33] Sophia Yang: So if I input fine-tuning is all you need, it will give us some… similarly okay abstract for this paper title. And then just another fun example, the croissant convolution. We made it up, obviously. So the abstract is like we propose novel convolution there, the croissant convolution. So it’s just a fun example for people to see how the finding link could work to play with. And then I have another example, Mroy is all you need. And again, it will output the abstract. [8:13] Sophia Yang: So I’m or a research paper because we trained on the title and abstract from the research papers. And another example, here’s another model. Note that The model name always have this structure. It’s fine-tuned. It’s fine-tuned on OpenMistral7b, our 7b model. So it will always have this name here, and it will have some random strings for the fine-tuned version. So for the medical chatbot, we actually trained on this hanging face data set of AI medical chatbot data. And then… [8:56] Sophia Yang: Here, as you can see, we ask it some questions and it will answer like it was a chatbot. And another example, so those two examples are from actual data. So here the data we get from archive and here’s the data we got from Hugging Face. But what if you don’t have the data? You can actually generate data from a larger model. Sometimes we want to mimic the behavior of a larger model like Mr. Large because we know Mr. Large behaves really well and we want to do some knowledge distillation or knowledge transfer. [9:34] Sophia Yang: from a larger model. So in this case, if you want to see how exactly we generated the data, you can go to our docs. And in this example, we have this prompt, you are a news article stylist following the economist style guide. So basically, we want to rewrite the news as the economist news article. And then we use Mr. Large to generate the revised news or like different revisions of how the news can be revised. And we give it some guidelines, basically. So So in this example, we use Mr. [10:18] Sophia Yang: Large to generate our data, and then we give it a news article, right? And then we use the fine-tuned data. It can generate a new piece of news article that may sound more like economist news. And then if you give it some prompting, it can also generate a set of revisions it proposed for you to change, how you can change the style of the news to make it sound better. Or more like economist. Yeah, so that’s. That’s some quick demonstrations of how the fine-tune model can look like. [10:57] Sophia Yang: In our docs, we have some developer examples of real-world use cases that have been using our fine-tuning API. For example, we have FOSFO. They are a startup using our fine-tune model for RAC, for Internet retrieval, and you can see the details here. We have examples of RAC for medical domain, financial conversation assistant, legal co-pilot. So they are all using fine-tuned Mr. Models with our fine-tuning API. And in these examples, you can see the specific data. You can see the evaluation training steps and how the benchmark results look like. [11:47] Sophia Yang: Yeah, so the fine-tuned with just small is better than the not fine-tuned with just small. Not surprising. Yeah, and then different results. So yeah, if you’re interested, you can check out our developer examples for some real-world use cases of fine-tuned model. So, Yeah, so in this next section, next demo, there are so many demos I want to show you today because it’s very exciting. In this next section, I want to show you an end-to-end example of how you can use Mistral Fine Tuning API on your own and for your own dataset. [12:33] Sophia Yang: Of course, we need to install Mistral AI. Make sure it’s or some numbers larger than that after probably after today. And we also need to install Pandas. The first step is, of course, we need to prepare our data set. Yeah. So whenever you want to do some fine tuning, the first step is always the data. In this simple example, we. read data from a parquet file that’s hosted on Honeyface is the UltraChat data. [13:10] Sophia Yang: We are only reading one parquet file because this data is quite large, and we don’t want to spend too much money on it. So you can feel free to change the data to your own use cases. We split the data into training and evaluation here, and then we save the data locally into the JSON-IL format. This format is actually needed for using our API. And note that here are the sizes for the training data and evaluation data. So there is a size limit here for our API. For training data, it’s limited at 512 megabytes. [13:49] Sophia Yang: So you can have multiple files feeding to your training, your fine-tuning pipeline. But each file needs to be under 512 megabytes. For your evaluation dataset, it needs to be less than one megabytes. Yeah, so a lot of times the data on Hugging Face is not greatly formatted for the training purposes. So we have a script to help you reformat your data. This script is not, maybe not robust in all of the use cases, but in our use cases it works. [14:35] Sophia Yang: So if your use case is more sophisticated, you might want to change the script to your own use case accordingly. So in this example, we basically just skipped several examples because they’re not not right formatted. So if we take a look at one of the examples, one of the issues here is the assistant message is empty. So we can’t really train based on this kind of data. So we need to make sure the data is right formatted. Otherwise, it’s going to be difficult. [15:09] Participant 1: I have a quick question about this. Is there like a format validator that y’all have? I see that you have a thing to convert it to format, but I’m curious about validation. [15:19] Sophia Yang: That’s a great question. We do have a validator script as well. That was in my next notebook. [15:26] Participant 1: Oh, sorry about that. [15:28] Sophia Yang: No, no, no. That’s a great question. So we do have a validation data script from the Mr. Find2 repo. Also, whenever you upload the model to our server, it will validate the data for you. So if your data is not right, it will tell you why it’s not right and ask you to change it. [15:54] Participant 1: Excellent. [15:55] Sophia Yang: Yeah, thanks for the question. Yeah, so yeah, so the next step is to upload the data set with the files create function and then you can just define the file name and the actual file here and we’re uploading the file and then we can see the ID of the file, the purpose of the file. The purpose right now is just fine too, maybe there will be some other purposes later, but right now it’s just fine too. And then to create a fine tuning job, you will need the IDs of the files we just uploaded. [16:32] Sophia Yang: And of course, you need to define the model you want to fine tune. Right now we only support minstrel7b and minstrelsmall, but in the future we might add more models here. And then you can try different hyperparameters. And in this example, I only ran 10 steps just to have it faster. But if you want to increase your steps, actually, you probably should increase your steps if you’re doing something serious. Yeah. So make sure you can change those different configurations here. [17:10] Sophia Yang: And then we can find the job ID and then we can use, and then we can like do different things like listing the jobs that we have. I have quite a few jobs that I ran. And we can retrieve a job based on the job ID to see if it’s successful or not. And then you can see we have some metrics here, training loss, validation loss, token accuracy. Because we only ran 10 steps, you only see this once. If you run it 100 steps, you will see it 10 times. [17:48] Sophia Yang: So every 10 step or every 10% of the step, you will see the metrics. So you can see if you are making progress or not making progress. And finally, with the fine-tuned model, when the model is, the job is successful, you will see the fine-tuned model get populated from your retrieved jobs, and then you can call this model and then ask any questions you want. So, so that’s, that’s how you can use a fine-tuned model. It’s exactly the same syntax as what if you, if you’re using our normal, non-fine-tuned models. [18:31] Sophia Yang: Okay, and finally, if you want to use weight and biases, you can add your weight and biases credentials here. Yeah, you will need your API key and everything. Just to show you how that might look like. Basically, you can check your losses, your perplexity score, your learning rate and everything. So. Yep, so that’s how you can run the fine-tuning through our API. I hope it’s clear. Any questions? Okay, cool. [19:11] Sophia Yang: So this last demo I want to show you is what if you want to just use our open source code base to fine tune MISDRAW7B or other MISDRAW models. I’m actually running this in Colab Pro Plus account, so it’s possible to fine tune a model in Colab. So in this example, we… I just git clone this repo, the MrFinding repo, because it’s not a package, it’s a repository. And in this repo, we need to install all the required packages. And of course we need to download our model because we’re doing everything basically locally. [19:58] Sophia Yang: And we’re downloading this model from the Mistral website and we’re downloading 7BB3 model here. And then… Same thing as we have seen before, we’re preparing the dataset. We’re using the exactly the same data as before reading the parquet file, splitting into training and evaluation, save the data locally. And then. Same as before, we reformat our data. Everything, the evaluation data looks pretty good. And afterwards, you can verify if your data is correctly formatted or not with our evaluation data script. [20:40] Sophia Yang: So yeah, so it will, this, this actually will take some time because it’s evaluating, yeah, each record. And then you can start training. The important thing here is actually this config file, right? This basically is telling the model, telling the LoRa how you want to fine-tune your model and different, where is the path of everything. So we have the data file. You want to define the path of your instruct data, your evaluation data. This is the training data, your evaluation data. We want to define the path of your model. [21:26] Sophia Yang: You might need to define or just leave it as default, those hyperparameters. We recommend using a sequence length of 32K, but because I’m using a Colab as the memory is limited, I ran into auto memory issue a lot. So I decreased this number to 8,000. But if you have a better GPU, you should use 82. [21:54] Sophia Yang: a thousand and then you can you can define all the other fun stuff right okay yeah and then with this one line of code we can start training and fine-tune our model um and then in uh the result as a result you can see the checkpoint of of your LoRa result here. This is where we can use to in our inference as our fine tuning model. And to run our inference, we have a package called Mr. Inference to help you run all of our open source models and also all the fine tuning models. [22:40] Sophia Yang: So basically, In this, in Mistral inference, you need to define the tokenizer, which is in the, which you are downloading from the Mistral, Mistral fine tune file. We use the v3 tokenizer because it’s a v3 model. And then we need to define the model that’s reading from the models folder we downloaded. And then we need to load LoRa from the checkpoint that we have, we just saved from the fine tuning process. And then we can run check completions and get the result. So that’s basically how you can run Mr. Fine-Tuning with Mr. [23:21] Sophia Yang: Fine-Tune the code base. And finally, I want to share some exciting news that since we just released our Fine-Tuning API, we are hosting a Fine-Tuning Hackathon starting today to June 30th. Yeah, so please feel free to check out our hackathon and you can submit from this Google form and yeah, very exciting. Looking forward to see what people are building. Thank you so much. [24:00] Participant 1: It’s very cool. What’s your favorite, like, so the very end you did like kind of like training like an open model, not using API or whatever, using Torch Run. Is that your preferred way to fine tune? I prefer the open models. [24:23] Sophia Yang: You can fine tune Mr7b with our API as well. I would recommend using our API just because you don’t need a GPU. It’s so much easier. You will not run into out of memory issues, hopefully.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Best Practices For Fine Tuning Mistral"
    ]
  },
  {
    "objectID": "education/fine_tuning/daniel.html#chapters",
    "href": "education/fine_tuning/daniel.html#chapters",
    "title": "Creating, curating, and cleaning data for LLMs",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nDaniel Van Strien and David Berenstein introduce themselves and provide an overview of their talk. They discuss datasets in the context of Large Language Models (LLMs) and briefly outline the features available in the Huggingface datasets.\n02:31 Reusing Existing Datasets\nHuggingface offers a wide range of datasets that are tailored to specific domains and tasks, though their relevance to your specific use case may vary. They provide various tools for searching, viewing, and exploring datasets.\n07:14 Creating Your Own Datasets\nDatasets can be created by restructuring existing data, incorporating user feedback to tailor preferences, utilizing internal data sources, or generating synthetic data. The discussion includes preprocessing requirements essential for training LLMs.\n09:04 Dataset Preparation\nDaniel explains the importance of formatting datasets to meet specific requirements for training LLMs, emphasizing scoping and planning based on user needs.\n11:09 Supervised Fine-Tuning Datasets\nThese datasets consist of question-answer pairs used to fine-tune models for specific tasks, facilitating the mapping of high-level concepts to data.\n12:56 Direct Preference Optimization (DPO) Dataset\nPairs inputs with preferred and rejected responses to guide models in generating desired outputs using ground truth and suboptimal examples.\n14:43 Kahneman Tversky Optimization (KTO) Datasets\nThese datasets feature binary feedback (thumbs up or thumbs down) on model responses, easily collected from user interactions in existing systems.\n15:47 Spin and Orpo as Alternatives to DPO\nSpin generates synthetic data from minimal initial datasets to reduce data requirements, while Orpo streamlines training by skipping the fine-tuning step, employing a format similar to DPO.\n17:56 Synthetic Data\nDavid discusses how LLMs generate synthetic datasets, enhancing model quality and complexity through prompts, completions, and AI-generated feedback for refining preferences.\n20:25 Issues with Synthetic Data\nDavid highlights concerns such as hallucinations, toxicity, and stereotypes in models trained with synthetic data, potentially stemming from biases in the data-generating models.\n21:18 Instruction-Based Dataset Evaluation\nModels complete instructions evaluated by GPT-4 for criteria like truthfulness and helpfulness, with simplification to an overall rating to reduce costs. Human review reveals coding errors, stressing the need for validation.\n24:20 Considerations in Synthetic Data Creation\nEfficient scaling requires avoiding vendor lock-in, ensuring fault tolerance, and generating structured data formats like JSON, highlighting the complexity of the process.\n25:17 Outlines Package\nProduces structured text generation with JSON output, optimizing token sampling for efficiency and accuracy to reduce inference time.\n26:10 DSPy Package\nFocuses on programming prompts for LLMs, optimizing prompts and model weights through multiple API calls to improve prediction accuracy.\n27:09 Distilabel Framework\nUses a directed graph structure to generate synthetic data and AI feedback, enabling scalable and parallel execution for efficient data processing.\n28:19 Improving Data Quality\nDavid discusses the iterative process of dataset improvement, emphasizing evaluations of diversity, quality, and quantity, where better data means higher quality rather than simply more data.\n29:57 Data Improvement Strategies\nDeduplication and custom techniques like hashing and rule-based approaches using regex can enhance data quality.\n31:53 Advanced Techniques for Data Cleaning\nUtilizing zero-shot models for initial topic predictions, classifiers for precise filtering, or LLMs for rationale-backed decisions, alongside intuitive text descriptive tools for straightforward data analysis.\n32:27 Tools for Annotators\nDavid showcases various annotation tools available, ranging from pre-made interfaces to custom Gradio setups and robust tools like Lilac and Argilla.\n41:41 Example Dataset Walkthrough\nDaniel walks through example DPO and KTO datasets, detailing the approach taken during dataset creation.\n45:00 Case Study: LLM Summarizer\nDaniel discusses the pipeline for a summarizer he’s developing, including preparations for the preference data pipeline.\n50:48 Data Preparation Repository\nDaniel shares a repository containing notebooks covering the topics discussed in the talk.\n51:42 Resources and Conclusion\nDaniel briefly discusses using Huggingface course credits and highlights additional resources on data duplication strategies and synthetic data generation.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Creating, curating, and cleaning data for LLMs"
    ]
  },
  {
    "objectID": "education/fine_tuning/daniel.html#resources",
    "href": "education/fine_tuning/daniel.html#resources",
    "title": "Creating, curating, and cleaning data for LLMs",
    "section": "Resources",
    "text": "Resources\nLinks to resources mentioned in the talk:\n\nArgilla is a collaboration platform for AI engineers and domain experts that strive for quality, time-to-value, and ownership.\nHuggingface DPO datasets\nLilacML: Search, quantify and edit data for LLMs\nHuggingface Security - Malware\nClamAV doesn’t scan a file if the extension is JSON\nSynthetic data: Anthropic’s CAI, from fine-tuning to pretraining, OpenAI’s Superalignment, tips, types, and open examples: LLM Synthetic Data\nAlpaca: A Strong, Replicable Instruction-Following Model\nNotus-7B: Data Curation and Open Science go a long way in shaping AI’s future\nStructured Text Generation Outlines",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Creating, curating, and cleaning data for LLMs"
    ]
  },
  {
    "objectID": "education/fine_tuning/daniel.html#full-transcript",
    "href": "education/fine_tuning/daniel.html#full-transcript",
    "title": "Creating, curating, and cleaning data for LLMs",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:05] Daniel: So the plan in this session is to kind of do a little bit of a high level overview, focusing on this topic of creating, curating and cleaning data. So I think this is already been discussed quite a lot in the course and I think it’s come up in various points. So there’s probably some stuff that we’ll say that you’ll already be familiar with, but I think the idea is to hopefully… give you some ideas for how to approach building datasets for fine-tuning large language models. [0:42] Daniel: So I’ll just quickly introduce myself and then let David introduce himself and then we can kind of dive in. So my name’s Daniel Van Streen. I work as a machine learning librarian at Hugging Face. I can talk more about what that means at the end of the session if anyone’s interested. But my background, as the kind of name implies, is very much in libraries. [1:05] Daniel: So I kind of fell into machine learning and large language models in the same way probably a lot of other people did via the Fast AI course that I took years ago now. And. [1:19] Daniel: But I think the library background is kind of nice for this topic because one thing that you will do a lot in libraries is look at lots and lots of data and think about how to organise data and how to structure things in a systematic way and I think that is a big part of working with data in the context of large language models so I think those kind of skills can be quite useful even though they’re a little bit outside of what you’d expect for people working in this domain. [1:48] Daniel: David, do you want to do a quick intro and then kind of dive in? [1:52] David: Sure. So I’m David. I’m doing ML and DevRel at Arjela. And Arjela is this data collaboration platform for AI engineers and domain experts, for the ones that aren’t familiar with it. And yeah, I kind of started off with NLP and these kind of things by initially my master’s and then working in private intelligence, working on knowledge graphs and also custom domain NLP models. And that’s also, I think, where data quality and model quality is very, very important. So yeah, that’s kind of my background. [2:25] David: And I will be covering the synthetic data part and hopefully give some pointers on that for you guys. [2:34] Daniel: Okay, that sounds good. So I’ll jump in then. So basically the kind of structure of this presentation is to start from like the ideal case in which you might already find some existing data and then work to the probably more realistic case that you’re going to have to build some of your own data for fine tuning if you’re not working in a topic that’s already kind of well worked on. And then you can start to build some of So I’ll just talk very quickly about some ways in which you might find existing data. [3:03] Daniel: So obviously I’m going to pitch Hugging Face as a good place to look for datasets. And I think that the question is a little bit what kind of datasets you need to look for. So the thing with Hugging Face, there’s quite a diversity of datasets that are shared on the hub, but a lot of the ones that will be trending and very visible tend to be. focus on like a very kind of specific type of use case. [3:29] Daniel: So an example of that here is this fine web data set, which probably quite a lot of you’ve seen do the rounds on social media. So this is a kind of data set focused pre-training large language models. So though it’s like a very interesting data set, it’s probably not something you’re going to find particularly useful unless you’re actually training based models yourself. [3:54] Daniel: The other thing that I would say is that there are a lot of research datasets that are associated with papers that can be really interesting, but I think there’s also a growing number of community contributed datasets that are made by people doing slightly more kind of bespoke and sometimes weird stuff. But those datasets can actually be really valuable both for using directly, but also for getting a bit of a better sense of how people are approaching building datasets. [4:25] Daniel: So one of the ways I would suggest if you’re not already familiar with finding datasets on the hub is to use this kind of tags feature. So I think that’s not super well used at the moment, but there’s kind of growing usage of it. And particularly for things like DPO datasets, it can be a really useful way of finding datasets that kind of match a specific format. And I’ll talk a little bit more about DPO in a little while. And there is also this full text search. [4:55] Daniel: So if people have done a kind of bad job of naming their datasets, this can often be quite useful way of finding datasets because it’s looking in the full dataset carter to find things. And then once you found the dataset, ideally someone would have documented it really well and explained exactly what it’s for and what the limitations are and how it was made, but that often doesn’t happen. But one thing that I think is really nice with the hub is that you have this datasets viewer that gives you a kind of preview of the dataset. [5:27] Daniel: And I think that can be a really good way of doing this kind of vibe check on whether a dataset is going to be useful for your application or not. So there’s a kind of example of what that might look like here. So there’s this dataset for doing function calling. As you can see, it’s very badly documented. There’s no information needed. I’m not trying to pick out on this person, but it’s just quite common that people leave that documentation until later. [5:57] Daniel: One of the things that you get in this preview is some kind of metadata about the dataset itself. And in this case you have this conversation which is this kind of chat ML format a lot of people are familiar with, where you have basically a list of dictionaries. And one of the things that might be interesting for you to know is like how long those conversations are and what the distribution of those looks like, depending on what kind of domain you’re working in. [6:22] Daniel: You might expect the kind of chats to be very short or you might expect to have longer chats. And then having a sense of what this looks like can be quite useful. But the other thing that you can then start to do is to dive into, okay, like what are the actual conversations in there? So you can look at actual example rows. And one of the things I noticed with this data set, even though some of the chat’s messages are quite long. [6:47] Daniel: Some of the responses or the kind of final turns are just basically the person being like, oh, thank you. And then the model being, oh, you’re very welcome. So probably that kind of response is not actually that useful. So if a lot of the longer messages are like that, then maybe you can’t kind of see this as a good data set for those kind of long conversation. Yeah, fine tuning use cases. So I said that was probably the ideal scenario that you find something that you can either adapt or kind of directly use. [7:20] Daniel: But I think in practice, often you’re going to have to create your own data set. And I looked a little bit through the Discord and I think people are like tend to be quite creative about this already. But I think one of the things that. [7:35] Daniel: probably is like a little bit underused still is just to adapt existing datasets for large language model fine tuning so there’s a bunch of datasets from kind of classic NLP that can be restructured or reformatted to work with LLM fine tuning and particularly if you’re already an organisation that’s kind of been using machine learning for a while probably you have some of those datasets already around that you potentially could adapt. The other part is like whether you already have some feedback from users. So there’s a lot of discussion about preferences and preference data sets. [8:10] Daniel: And you can set about creating those very deliberately, either by using human annotations or LLM judges. But quite often you already have some indications, either very direct, so people have like a thumbs up or a thumbs down. But there might be other ways in which you can kind of get at. this question of like was a user satisfied with a particular interaction and that might be a source of preference data that you can kind of create without actually having to to regather all that preference data manually. [8:43] Daniel: And then the kind of final one is this synthetic data which I think can go hand in hand with these other bits but I think it’s also very powerful for kind of jumpstarting the process of building these datasets. I won’t kind of labour this point too much, but I think there’s also often quite a lot of work to get your data into a kind of format you can actually use for large language model training. And I think there’s some nice tooling for this, but often it will be geared at a very specific use case. [9:21] Daniel: But yeah, one thing I wanted to kind of mention about that already is that I think a lot of the work that you do in prepping this data might also end up being very useful for when you get to the stage of deploying a language model in production. So I think you want to have that in mind, the kind of format that you’re gathering this data from should be quite close to what you’re going to actually be using that language model for in practice. So some of this kind of pre-processing. [9:50] Daniel: code and work will have a lot of overlap with what you’re actually going to be deploying later on. So jumping a little bit more into the kinds of data sets you need for fine tuning. So I think in some of the earlier lessons, this was already discussed a little bit. But I think in terms of the kind of, I guess the question of like what needs to be in your data set, and I know that’s a little bit vague, but I think one of the… [10:21] Daniel: The kind of considerations which I think is slightly different when you fine-tuning for a more specific use case is being okay with the kind of model losing certain abilities. So often when you’re kind of fine-tuning chat models, which I think is what a lot of the literature ends up being about, you want to make sure that you have a very diverse data set. and that it kind of captures all the things that users might discuss. [10:46] Daniel: But in practice, quite often the kind of scope of your problem or the application you’re trying to develop is much more narrow. And then I think you don’t have to worry about the diversity of the data in quite the same way. And you have to really think about diversity in terms of what your large language model is actually likely to get as input. And I’ll talk a little bit about these common dataset genres. So unfortunately, it’s a little bit the Wild West in terms of like the actual way in which datasets are structured. [11:22] Daniel: So there’s a lot of variation in this, but often there’s some kind of similarity. So for the supervised fine tuning. You have this kind of question and answer response, and it probably sounds quite obvious, but I think for some of these fine tuning use cases and also when you’re thinking about reinforcement learning and some of those algorithms for a lot of people, I think they find it easier to understand, okay, what does the data set for training using this particular algorithm look like? [11:55] Daniel: And then they can kind of map a little bit more at a high level what the process actually looks like. possible ways that they could get to a dataset that matches this kind of format. So very kind of briefly, I won’t talk about this reinforcement learning bit too much. But if you’re kind of following this literature at all, there’s a lot of algorithms being published. Some are kind of iterative improvements on existing ones and some are quite different. [12:29] Daniel: But what I think seen over the last year or so is that a lot of these algorithms end up using very similar data set structures. So I think that’s one of the kind of nice things in a way that a lot of the existing data sets can either be kind of lightly adapted or use as they are without having to. do a lot of work to reformat things. So one example of this kind of algorithm that became very popular and has been discussed earlier is this direct reference optimization. [13:02] Daniel: And kind of going back to this idea of some people finding this kind of expression of algorithms not that helpful, I think it can then be really useful to go back to what does the model actually expect when it’s being trained using this particular approach. And this direct preference optimization, I think is really nice because it kind of maps quite well to how you kind of want to nudge a model when you’re doing fine tuning. So you have some input and that could be whatever your task is related to. [13:39] Daniel: It could be kind of natural language. It could be code. It could be a bunch of different things. And then basically you want to nudge your model more to this chosen and rejected. [13:49] Daniel: and the reason I kind of mentioned this is I think one of the nice things I’ve seen quite a lot in the past year or so since this algorithm became really popular is like really interesting and creative ways for people to actually come up with where are these chosen and rejected so one way you could do it obviously is to like manually write a good example and then write a bad example but that’s really time consuming and tedious so people have done other things so for example with this chosen If you already have some kind [14:19] Daniel: of ground truth or gold standard data that a human has generated, often that can serve as a really good value for this chosen. And then, for example, generating this rejected response from a model that you’ve kind of evaluated that does an OK job, but isn’t quite there in terms of what you would like the response to look like. And then the final kind of algorithm that I’ll mention in terms of datasets is this KTO algorithm. [14:49] Daniel: And the nice thing about this is in contrast to DPO, where you need this kind of two preference pairs of chosen and rejected with KTO. You basically just have a response from the model and then this binary preference, so basically a thumbs up or a thumbs down. And that’s something that can be quite easy to collect. I think in general, people are quite happy to say, I don’t like this or I do like this in an existing system. [15:18] Daniel: But again, you might also have other kind of creative ways in which you might intuitively be able to understand, okay, well, this person didn’t click this link or didn’t do something else in the system. So probably that was a thumbs down and they did do something that kind of implies a thumbs up. So I think this could be a really useful approach to gathering data if you already have something in the wild that can kind of be interacted with by some kind of end user. [15:48] Daniel: And then the final kind of one of these algorithms in terms of data sets that I’ll mention is SPIN and ORPO. So SPIN is a kind of iterative approach. So a lot of the kind of approaches to doing this reinforcement learning have been focused on trying to reduce the amount of data you require, because that’s kind of one of the big bottlenecks. [16:12] Daniel: And the idea with SPIN is that you basically go through these different stages with some starting data and then you kind of synthetically generate new responses and then kind of build a data set on top of that without actually having to have such a large data set initially. And this ORPO algorithm, again, is a little bit about efficiency. So ORPO… algorithm basically expects the data to be in the same format as the DPO. So you have this input and then a response that’s chosen, a response that’s rejected. [16:49] Daniel: But the difference here is that it doesn’t rely on you having done this self-supervised fine-tuning step. So you can kind of more quickly go directly from a base model to actually doing this alignment without having to kind of train in two steps, which is often what you have to do with these other approaches. So that can be both nice in terms of not having to duplicate data for doing the fine tuning part and then the preference part. [17:16] Daniel: But also it means that the actual model training is a little bit more lightweight because you’re going to do that in two stages. And I’ll hand over to David, do you want me to hand over the slides to you as well? [17:30] David: Yeah, please. So I believe you have to stop sharing for a while and then I can. I think you need to disable it in Zoom itself. Yeah, perfect. So just to kind of get back to kind of what Daniel already highlighted is that in the end, often you don’t have your perfect data set. And eventually you actually want to work towards like the perfect data set that you might have. So that’s a way to actually do that is through synthetic data. [18:13] David: And synthetic data is data that’s generated by LLMs and is commonly used for fine tuning LLMs. So you can actually start your synthetic data. for example, by generating prompts from scratch, by generating completions based on prompts. And normally you basically prompt an LLM along with some initial contexts, for example, in order to also do rephrasing. [18:36] David: And some of these algorithms that do rephrasing of prompts to make them more higher quality or higher complexity or more nuanced, so to say, in order to ensure a higher quality prompt or completion are the evil complexity and the evil quality prompts. And then there’s also this special kind of AI feedback or synthetic data, and that’s actually judging. [18:58] David: judging synthetic data and that’s to assign scores to prompts and completions or to do preference ranking or to actually provide a rationale or critique along with your initial score that kind of explains as to why it came up with why the model actually produced the score so we actually would use a prompt template prompting a model okay please provide a score for this prompt and the associated response And can you also provide a rationale along with that? [19:30] David: And that would be kind of the prompt template, so to say, that people apply for these kinds of synthetic data things. So one of the earlier models that was actually generated, trained on synthetic data was the alpaca-7b model. And what they did was actually prompt the text-to-pinchy model along with the self-instruct research paper. So it was a prompt template that was actually taking some seed instructions, so some initial seed data, and actually was prompted to rewrite that seed data to better instructions or more diverse instructions. [20:05] David: So they started up with 175 instructions, then eventually ended up with 52,000 instructions and actually did supervised fine tuning along with the LAMA 7B model that Meta initially released. So that’s, I would say, amazing. You can actually use LLM data to train LLMs and then everything is solved. But as you might expect, that’s not really the case. So if you actually look into their report about synthetic data and the self-instruct and how the model actually performed, you can see that there’s a lot of hallucinations, toxicity and stereotypes within the model. [20:41] David: It might be due to the fact that they actually used the Meta model, which wasn’t trained optimally. It might be due to the fact that they actually used the DexDaVinci model from OpenAI. which might contain some bias, but in the end, it would have probably been better to actually look at the data and see what’s in there. So one of the examples from a completionist is like, what’s the optimal seed? Why is 42 the optimal seed? [21:08] David: And the model actually starts hallucinating that 42 is the optimal seed and that it can be used for any neural network training. So another example would be maybe using more complex pipelines, more better models, better data. And that’s actually what they tried to do for the ILTA feedback paper, where they initially sourced instructions. So a lot of both synthetic and human generated instructions from an instruction pool and actually asked, prompted different models to actually provide completions for each one of these instructions. And each one of these completions was actually judged by GPT-4. [21:47] David: based on four different criteria. So instruction following, truthfulness, honesty, and helpfulness. And also one overall criteria, just asking or prompting the model whether overall the completion was correct according to the initial input prompt. An issue, a good thing to take away is that whenever you do this judging, what we’ve seen is that each of these individual criteria when you’re focusing on instruction following truthfulness, et cetera, highly correlates on average, so to say, with one overall rating. [22:21] David: So if you actually want to reduce some costs and reduce some compute, you can also just apply one overall rating, also given the fact that you probably need some human involvement at the end. So, also this didn’t turn out to work. [22:38] David: So when we actually started uploading this data in our UI and actually looking at the data by human judgment, we actually saw that some data was sourced from benchmarks, that there was like this weird thing happening where there were some forms that completely or some responses that completely didn’t make sense and still ended up getting a very high score. And we kind of investigated that and investigated that and looked into the code and it was apparently a coding error leading the, for the scores to be converted from a one to a 10. [23:13] David: So all of these scores that were actually a 10 could have been, been a one. So that kind of messed up the entire dataset. There was like incomplete ratings due to the fact that open AI API calls filled. And there were some ties in the data that were still being processed as. chosen and rejected pairs, even though if the responses would get the same rating, you wouldn’t expect them to have one preference over another because they would be in a tie. [23:43] David: And that’s actually what we kind of also showed that whenever you spend some time, look at your data, get involved and work towards your higher quality data set, that you can actually train a better model. And initially the Hugging Face team published the alignment handbook saying that this awesome Zephyr model and made the entire research reproducible. And then we did that exact reproduction, but then instead we used the clean Ultra feedback set. And that actually led to a model that performed better on the benchmarks and also in intuitively putting to human judgment performed better. [24:21] David: So then apparently you need better data, better synthetic data, but whenever you actually want to work towards that, there’s a lot of things that come to mind. So initially you can kind of start spamming ChatGPT with random requests. But if you actually want to scale that or make it more cost efficient, want to avoid vendor lock-in licenses, you might not be able to be allowed to use all of the different models for actually generating synthetic data to find your models. [24:50] David: some fault tolerance that might arise as you have seen in the ultra feedback data set and things like like structural generation where you might want to just output JSON output responses in a JSON format. So overall, there’s just a lot of complexity involved and actually starting to do this. And yeah, I’ve chosen a set of tools that you might kind of look into whenever you start working with your own synthetic data. And one of them is the outlines package for structured text generation. And it’s going to actually produce structured text generation based on… [25:25] David: like prompts for LLMs based on Regex, JSON, Pydentic models that everyone loves and uses, and also actually functions to be able to do a function calling. And this actually relies on the modified, by modifying the sampling of tokens. And yeah, this is a very accurate approach to actually take. And it actually also reduces the inference time due to the fact that there’s a limited… number of tokens that you can actually sample from. And one of the interesting blog posts from Hacking Value actually dives a bit deeper into this topic. [26:02] David: Another package is DSPy that probably everyone is familiar with, but it’s focused on programming, not prompting, so to say. And this actually lets you define a DSPy signature that you use for actually calling an LLM, where you kind of program the expectations that you might have for your model. And then whenever you… kind of optimize this for your specific use case. The package underneath tries to optimize the prompt and optimize or fine-tune the model weight slightly, so that then eventually you end up with a higher or more accurate prediction. [26:42] David: And one thing that’s Hamel actually showed in one of his blog posts is that, yeah, under the hood, the model or the DSPy package actually makes hundreds of calls to the OpenAI API when it tries to optimize these prompts due to the fact that it’s gathering good few shot examples for your use case. And that actually is quite costly. And that’s an interesting blog post where Hamel kind of goes over some of these tools as well. So I recommend reading that too. [27:13] David: And the last one is a DC label and it’s a synthetic framework for synthetic data generation and AI feedback. And it relies on a direct basically graph structure. And it’s more of a pipelining framework where we kind of worked on serializable pipelines, uh, cast intermediate results and actually included structure generation as well, based on the outline speculates and everything is execute executable in parallel. So whenever you. really want to scale this up, then you can actually do that. [27:47] David: And it’s kind of like application development, like the things like Lama Index or Deepset or some of these tools, but then really focused on the dataset engineer. And one of the interesting things that I’ve seen come by recently was also this blog post about the rise of the dataset engineer. So not really having AI engineers or these kind of… people anymore, but really people that focus on data and data quality. And it’s an interesting read as well. [28:17] David: So when you’re working on improving your data, it’s like an iterative process where you have your original dataset and you add some things to assess diversity, the dataset quantity, dataset quality. You do the duplication, filtering, and these kind of things. And you throw some human feedback, but also AI feedback in the mix to make it more intuitive and more smart. So what you can actually ensure or what you can think of for improving your dataset quality is that there’s enough quality, but enough quantity, but more isn’t always better. [28:56] David: So it’s also fine to throw away data that’s maybe very present within your dataset that you might be able to deduplicate or also reduce, simply reduce to lesser quality examples within your dataset. So one example, what Some examples that we’ve gathered from actual public reproductions of these initial fine-tuning techniques that Daniel highlighted earlier is that for SFT, we’ve seen great results according to benchmarks with 10K input samples for ORPO, 7K, DPO, 3K, and spin as low as 2K data examples. So apparently with… [29:36] David: lot less data and a lot of models are being fine-tuned, you can actually do a pretty good job at achieving high benchmark standards, but probably also to actually have some good human intuitive models. And whenever you do think about data duplication, there’s a lot of research being done in duplication, and often the Duplication pipelines aren’t very reproducible and not very well documented. But as we’ve seen with the FIMERAP dataset that’s recently been published by Huggingface, they actually published the entire pipeline and made it publicly available to the Data12 library. [30:22] David: And what you can think about when actually doing deduplication is applying intuitive rules like applying existing metadata that you might use for filtering your data, for filtering out some irrelevant examples, for, I don’t know, doing something like topic-wise deduplication, where you might want the most representative examples per topic. You can also think about creating your own custom metadata or your own custom features. [30:48] David: and doing like intuitive, simpler things like hashing of the strings or doing like filtering based on embedding similarity and then actually grabbing the exemplars from like the embedded data points that are most representative for your data. When you’re doing rule-based data, it can be, yes, rule-based cleaning of your data can be as simple as just writing a Regex query. [31:16] David: So, for example, querying as a large language model, if you don’t want to have that in your data set, normally some models used to respond like this or using the word Delve or using like these quotation marks that might indicate that the model is going to kind of hallucinate random references, some random URLs that you might want. to at least change within your dataset. And all of these things are like very intuitive, human understandable rules that you can apply to in order to improve your dataset quality. [31:54] David: And on top of that, what you can also do is apply these more advanced techniques or more advanced specifications or embeddings or these kind of things where you quite easily can get a zero-shot model out of Hugging Phase that’s performing quite well and decide some topics that you deem relevant for your dataset and actually start predicting, making initial predictions for these topics and actually use that as some initial ways to filter your data as well. And these classifiers can also be very, very cheap. So you can actually use one of the zero-shot models. [32:32] David: You can use a set-fit model after annotating only a couple of examples. And if you actually want to go more expensive, you can look at LLMs as judges, juries, or rationalizers where you also have this rationale, like the explanation as to why they provide this score. And if you want to go a bit more simple and a bit more intuitive and explainable, then Something like text descriptives, like the package where I provide the URL, can provide a very easy out-of-the-box analysis of your data. And all of these methods that I went over. [33:10] David: are actually intended to be used according to our vision, our view, along with human annotation. And this really helps normally to make human annotation more fun, more engaging and also more effective. So that’s kind of where you want to be. So maybe somewhere in the middle where you can either choose for a very basic, simple, intuitive approach to using Google sheets or like a completely customized approach that really works for you. [33:42] David: But for, I would say for, for a lot of the people, something in between where you might use like some, some already out of the box tools or web apps really works and really differs per person, what you, what you want and what you prefer. So one of the custom tools that I’ve seen come by to kind of play around with your data and kind of see what’s in your data is bulk annotation from Vincent Warmerdam. [34:11] David: And it’s like a custom tool that he built with Bokey and Pandas and some JavaScript to tie everything together where you embed all of your text and you can kind of explore and kind of play around with it. So this is what it looks like. You can do this lasso movement, see what’s within that specific cluster, and then move from there to annotating and actually saving your initial results. [34:38] Participant 3: By the way, I have a question about this. I saw this too, and I thought it was pretty cool. Does the Argila have something like this, a specific functionality? [34:47] David: Yeah, we actually have something like this where you might be able to attach vectors to your records and you can do semantic search, but we don’t have this interactive kind of annotation thing. It’s a thing that we’ve considered. I believe Snorkel has something similar to it, but we’ve not gotten around to actually implementing this. An alternative that you might consider is like using something like notebooks where you use notebooks to initially annotate your data. [35:15] David: And I think everything in terms of notebook annotation was a request from from Hamel to kind of highlight this is based on IP annotations. And then from that, there’s a lot of branches kind of looking roughly the same and giving the same overview. But this is also fully customizable where after. you’ve filled out one annotation, you can actually do some callbacks and actually do some post-processing. [35:40] David: So you might be able to do some active learning and actually start gathering data for fine-tuning set with classifier on the fly, so to say, and then run inference with that classification model. Another thing that’s kind of like a little bit more out of the box is like creating web apps with radio, with Streamlit, with Shiny, with R Shiny. And you can actually use like all of their components and tools to directly out of a box have a very intuitive basic UI. And this can also be fully customized normally. [36:16] David: So there’s one UI of the Somers NLP hackathon that someone created that we normally host together with Hugging Face. And this is the input text where after that someone is able to kind of correct or say, okay, this is the right translation or wrong translation can also be used for like the KTO example that Daniel mentioned earlier, where you might give a thumbs up, thumbs down and kind of go through your records one by one. [36:40] David: And this is once again, like a nice, nice example, but with Gradios, Streamlit and Shiny, you can really customize everything as far as you might want to do that. And if you actually want something a bit more out of the box, there’s also these fully fledged cool solutions with a lot of integrated features. One of them is Lilac. And Lilac kind of has this dataset overview thing where whenever you select the dataset, you can actually select different topics and you can actually see some of the main clusters within your data. And it’s pretty, pretty cool. [37:15] David: And on top of that, they also have this semantic search feature that I previously. mentioned. [37:23] Participant 3: What’s about Lilac, by the way? Do you recommend using Argila with Lilac for the different features, using them concurrently? What’s the stack that you like, personally? [37:32] David: For me, it’s kind of a bit of a mix of both. I think it really depends on what you prefer, what you like. I find the Lilac UI a bit less intuitive, personally. For example, for Within Argylla, there’s less, I think, features included and the UX, UI is a bit more intuitive for me. Things like spacey exposure are played around with, but it really depends on what you want. [38:03] David: Probably if you, as an AI engineer, look at the API that we have or the SDK that we have compared to the one with Lilac, if you as a domain expert or a labeler. [38:13] David: kind of go into into lilac and find their features and ui ux uh better then yeah then go for that but i think it’s the the same tool inherently covering the same topic having a lot of the the same or similar features and [38:29] Participant 3: for me it really depends on uh on that kind of thing well you daniel do you have a do you have an opinion or do you have a stack that you like [38:38] Daniel: Yeah, I mean, I think one thing is like at what point in your kind of process you’re trying to look at your data. So I think there’s this like initial kind of vibes check where you’re trying to look at quite a lot of data very quickly and maybe looking at a few examples carefully, but not necessarily like actually fixing things yet. So I think it’s good to kind of separate the kind of annotation mindset from the like, I’m trying to just understand this data overall. [39:05] Daniel: And I think for that, Lilac can be quite nice because you have these different kind of visualizations. And I think. The other thing that’s super crude, but if you can get your data into some kind of data frame, even in Google Colab and compute some features like string length and token counts and very basic crude features, it gives you these plot suggestions, which are actually sometimes okay. And it’s just a really lazy way of being like, hey, there’s this one really random outlier. [39:34] Daniel: So it’s a little bit crude, but I think it can be quite a quick way of finding where something’s going wrong. And then I think after that, kind of trying to dive in a little bit more detail and actually poke around the individual responses. [39:48] David: Yeah. And I think with the nice thing from Lilac already more full-fledged tools is that you don’t need to think about what you want, so to say. So of course they have a lot of these highly opinionated features building and based on that you… I think a lot of people will get away with having like some nice out of the box toolkits. There’s of course always people that are going to want to have some custom thing built on top of some custom features. [40:18] David: And then I think it’s if you really value that and it’s also worth spending time on that and actually building some of these custom tools or some of these custom annotation flows out of the box. I think both Lilac and Arjela are open source as well. Yeah. So. This was the other example that I had, Arjela, same kind of idea, having a dataset overview. You can log in and play with a demo as with Lilac. And also you have this like semantic search kind of thing. [40:51] David: You have your content that you might load into your UI, and you can actually start annotating and doing bulk annotation and all of these kind of things. So it’s same, same, but different. I have a preference for Arjela, of course, and I’m more familiar with that. But I guess for everyone, it’s up to them to decide which UI and API and workflows they prefer. Yeah. So maybe I’ll hand it over to Daniel again. Yeah. Thanks, Daniel. Thanks, Daniel. Thanks, Daniel. Thanks, Daniel. [41:36] Daniel: Okay can you see that? Yep. So I’m just going to talk through some example datasets and this might seem a little bit weird, like why I’m talking about these very particular datasets, but I actually think you can sometimes learn very little, but sometimes learn a lot from looking at some examples of datasets people in the community have built. Particularly for approaches to generating these datasets in kind of creative ways that doesn’t just rely on either throwing loads of API credits at the problem or loads of human effort. [42:11] Daniel: So I think that also goes back to this thing I was saying earlier about DPO datasets. Why they became so kind of popular in the community is because this idea of generating a chosen and rejected pair can be done in like a lot of creative ways. So there’s an example here of this dataset that’s trying to make models, large language models, better at writing. [42:36] Daniel: And the approach they take is basically to kind of take some existing books written by humans and then prompt a large language model to summarize those and then prompt another model to rewrite based on that summary. So, you know, right. novel or chapter of a novel based on this kind of summary. And then they choose the kind of the original human generated response as the chosen, the model generated response as the rejected. [43:07] Daniel: So that might not apply to your use case, but I think it’s an example of like how you can get at these things without having to just rely on either human data or model data. Here’s another example. So this one is actually kind of a vision model, but it has a kind of similar interest in that it’s focusing on kind of actually generating this data flywheel. So trying to get this thumbs up, thumbs down. So, I mean, it really depends. [43:43] Daniel: what kind of context you’re working in but I think getting these thumbs up thumbs down ratings can be quite useful and the other thing that I’ve found from doing a lot of annotation so I think it’s worth thinking a little bit about the kind of ergonomics so not just of the tool but also what is the task that you’re actually trying to do when you’re annotating and how does that work better so you Third, these kind of preference data sets, depending on what you’re trying to do, sometimes it can be really easy to say that’s [44:15] Daniel: a good response or that’s a bad response. Other times it’s really helpful to have these two generations next to each other and say, okay, this one is better than this one, because without having a comparison, it’s quite difficult to actually say which model response is better. So I think when you’re doing this human annotation, I would give like quite a lot of thought to those kind of questions, which I think people kind of don’t really think about that much. But I think you can actually. [44:44] Daniel: Yeah, I think it can be quite important for both your experience of doing the annotation and how pleasant and easy you find it. But I also have no evidence for this, but I suspect it actually influences the results of the annotations you get quite a bit, depending on how you set up the task. So I think it’s worth giving a bit of thought. And I think what maybe we’ll do is just give like a very high level overview of this kind of project I’m actually working on for this course. So that is. [45:14] Daniel: basically trying to build a large language model summarizer and hopefully a small large language model summarizer so it can actually be deployed easily. Then we’ll take a dataset card for a dataset hosted on the hub and turn this kind of long markdown into a too long, didn’t read summary that you can kind of very quickly look at and be like, what is this dataset about? What is it for? This is just like a very kind of high level overview of the steps that are kind of taken to kind of create that data set. [45:47] Daniel: So you have, as I kind of mentioned at the start, a bunch of processing that you do with the markdown, and that will also be processing that you’ll carry over to the actual deployment. So in the markdown, you have a lot of stuff that you probably don’t even pass to a large language model because it’s just like formatting or content. It’s not very informative. You might want to move some of this YAML and then you kind of get this summary. [46:13] Daniel: And then in this particular case, the approach I’m trying to take is to build this preference data set. And there’s different ways of approaching this, but this is what I’m kind of using this distilled label pipeline for. And this kind of looks a little bit crazy, but I guess the overall workflow, which is actually not that difficult to define, is that you load this data set that has a bunch of these data set cards and you do this kind of initial filtering. You format this prompt. [46:46] Daniel: And then in this case, I’m trying to use open models. So kind of compare these three different summaries and then use ultra feedback, which is a kind of approach to doing this language model judging. And in this case, I’m using Lama3 for that. And I guess the kind of thing I want to say about this pipeline, I think with synthetic data generation, it really depends on what you’re trying to do. [47:12] Daniel: Sometimes just like having a prompt and a bunch of inputs and being like, right, just call that API a bunch of time with this prompt will work well. And sometimes it’s kind of more complicated pipeline can work better. But the thing I found quite nice about this is that. [47:28] Daniel: at each of the kind of steps you kind of maybe increase the number of responses you’re generating so maybe initially when you’re comparing these different models you kind of do like a hundred or something and just kind of very quickly start looking through the responses and I guess at that point what you’re thinking about which is kind of similar to this point that you’re often doing when you’re kind of thinking about using prompting versus fine-tuning as being questioning about okay can I just improve this via the prompt so you might already done a little bit [47:59] Daniel: playing around but when you kind of get a few more responses you think is there something that I’m getting in the response that can just be fixing this prompt step and you kind of iterate on this pipeline but each of these steps might actually remain uh each like the kind of workflow might remain quite static but you’re kind of fiddling around with each of these steps you And the other thing that I will say in this kind of particular pipeline, I’m sending all of these responses to Arjela. [48:26] Daniel: So you’re kind of looking at two things when you’re doing that. So you’re looking at the model responses. and seeing how they look. But the other thing that I’m trying to look at when I’m kind of building this pipeline is this LM judge part, because I’m basically assuming with this workflow that this judge is actually going to be able to detect which summaries I’m going to like better. So one of the things that I kind of do, and there’s a kind of notebook in a GitHub that I’ll share afterwards, but… [48:58] Daniel: basically rate a bunch of examples and then look at the ratings that the model gives and whether there’s some like average better rating but also like how often do I change the rating from the model and like how big is that difference maybe if it’s like only one score different then that’s not so bad but if it’s consistently getting bad ratings then that’s maybe something to worry about in terms of whether it’s reflecting what I want the LLM to do. [49:29] Daniel: And the other thing I would say about this part, and I’m probably a little bit overly obsessed with this idea, but I think when you’re looking at these ratings and the responses, like thinking about heuristics that you can use to kind of capture, is this response good or bad? Like maybe it’s too complicated to judge via a simple rule, but quite often there might be patterns that you start to see. [49:54] Daniel: So once you have like even 100 ratings or maybe even like 20, you can start being like, okay, do I always prefer the one that’s shorter or the one that’s longer or is it like random? Because if it’s just always a shorter one, then maybe you can already say, I just need to like cap the kind of maximum tokens or do something a little bit more manual. Or I can just basically say all the short responses are always better. [50:17] Daniel: and then skip this kind of LM judge part because this is the kind of expensive part but it’s also like slightly brittle because you in this case like put quite a lot of faith as you kind of scale this in this consistently working well and if you can use some of these simple rules either instead of or as well as the the kind of judge to check okay is this continuing to work well I think that could be quite useful. [50:47] Daniel: So because we’re running out of time, I thought I’d just quickly talk through this repo that we created. So there’s some notebooks in here that kind of give some examples of like different approaches to deduplication, how to kind of do these kind of database checks. And then an example of using Distil label to generate one of these synthetic pipelines. And the other thing which I have to admit is a work in progress is trying to basically put all the work I’m doing for this particular project in here. [51:18] Daniel: I’m not saying that’s necessarily a good example of what to do, but I will at least try and share it openly and kind of communicate what I’m trying to do there. And then, yeah, I’ll maybe just quickly point to these other resources that we have here. So one thing I just wanted to mention is that with the course credits that you have from Hugging Face, you can use that for spaces, but you can also use it for inference endpoints. [51:47] Daniel: So I think between all the course credits, you could actually generate quite a lot of synthetic data. And I think it’s quite interesting to see kind of what people can come up with that. And then beyond the kind of notebooks for this session, there’s also a few other resources here that might be interesting, including this blog post about FineWeb, which talks a little bit about their approach to deduplication. [52:12] Daniel: And it is kind of for a slightly different use case, but I think there’s some kind of nice lessons in there that might still be kind of relevant. you know, doing like a really aggressive deduplication doesn’t always work better. And there’s these kind of subtle heuristics, which unfortunately, like often you have to actually train a model to see what the impact is. But if someone else has done that, that’s quite useful to see if there’s something you could pick up there. So yeah, I think maybe go ahead. [52:40] Participant 3: Sorry. [52:41] Daniel: No, sorry. I was just rambling to a close. [52:44] Participant 3: No worries. We have about two minutes left. There isn’t too much time for questions, but I’ll just throw one out there. Aminah asked, how would you recommend generating synthetic data if we’re fine tuning on proprietary data sets and human annotation is expensive? [53:09] David: Proprietary data sets that someone else creates, like a vendor of some sort? [53:18] Participant 3: Yeah, I mean, I don’t really know what he means, but maybe it’s, yeah, like fine-tuning on, I don’t even know if the word proprietary really is. I mean, just like your own company state. Just like, let’s go that way. [53:31] David: I guess it’s kind of the same as whenever you would fine-tune your own model versus using a proprietary model. It’s about data ownership, privacy, about the fact of you. value these kind of things in your company, if you need these things in your company, and if you really want to be the owner of your data and your model. And those are, I think, the main takeaways for this kind of trade off, both for models, but also for your data. [54:08] Participant 3: Sounds good. We can probably take the rest of the questions to the Discord. Maybe y’all, if you want, David and Daniel, if you have time, you might want to peruse it. There’s a channel dedicated specifically to this talk where people are in. But yeah, thanks a lot. [54:28] Daniel: This was a very good talk. And just to say also, I’m overly excited about people building data sets. So I’m happy to try and help out. I might not have the answer, but I’m happy to brainstorm with people if they’re interested in building a data set as part of this and sharing it, then definitely try and give them a hand with that. [54:47] David: Yeah. Great. All right. [54:50] Daniel: Thanks a lot. Thanks. [54:53] Participant 3: All right. Thanks for having us. [54:54] Daniel: Bye. Bye. Bye.",
    "crumbs": [
      "Educational Resources",
      "Fine-Tuning",
      "Creating, curating, and cleaning data for LLMs"
    ]
  },
  {
    "objectID": "education/rag/jo.html#chapters",
    "href": "education/rag/jo.html#chapters",
    "title": "Back to Basics for RAG",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction and Background\n01:19 RAG and Labeling with Retrieval\n03:31 Evaluating Information Retrieval Systems\n05:54 Evaluating Document Relevance\n08:22 Metrics for Retrieval System Performance\n10:11 Reciprocal Rank and Industry Metrics\n12:41 Using Large Language Models for Judging Relevance\n14:32 Microsoft’s Research on LLMs for Evaluation\n17:04 Representational Approaches for Efficient Retrieval\n19:14 Sparse and Dense Representations\n22:27 Importance of Chunking for High Precision Search\n25:55 Comparison of Retrieval Models\n27:53 Real World Retrieval: Beyond Text Similarity\n29:10 Summary and Key Takeaways\n31:07 Resources and Closing Remarks",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Back to Basics for RAG"
    ]
  },
  {
    "objectID": "education/rag/jo.html#slides",
    "href": "education/rag/jo.html#slides",
    "title": "Back to Basics for RAG",
    "section": "Slides",
    "text": "Slides\nDownload PDF file.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Back to Basics for RAG"
    ]
  },
  {
    "objectID": "education/rag/jo.html#resources",
    "href": "education/rag/jo.html#resources",
    "title": "Back to Basics for RAG",
    "section": "Resources",
    "text": "Resources\n\nJo’s Twitter\nJo’s blog\nTalk: Boosting Ranking Performance w/Minimal Supervision",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Back to Basics for RAG"
    ]
  },
  {
    "objectID": "education/rag/jo.html#full-transcript",
    "href": "education/rag/jo.html#full-transcript",
    "title": "Back to Basics for RAG",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:10] Dan: Hey, everyone. Hey, Joe. Hey, Hamill. [0:38] Jo: Hello. [0:40] Hamel: Really excited about this talk. This is also the last one of the conference. So it’s very special. [0:47] Jo: Thank you so much for inviting me. Yeah. It’s fantastic. Yeah, it’s amazing to see all the interest in your course. I’m in the lineup of speakers, so I was really honored when you asked me to join. That’s amazing. [1:05] Hamel: No, I’m really honored to have you join. So, yeah, it’s great. I think your perspectives on RAG are very good. So I’m excited about you sharing it more widely. [1:21] Jo: Thank you. [1:25] Dan: We usually start at five after the hour or after the hour. So we’ve got another five minutes. We have… 23 people who are watching or listening to us now. I bet we’ll end up probably hard to say with this being the last one, but I’m guessing we’ll end up probably around 100. [1:48] Jo: Yeah. Sounds great. [1:52] Hamel: I’m trying to think. Is there a Discord channel for this talk? [1:58] Dan: Yeah, I just made one. So it is. I posted a link in Burgum Rag. And I just posted a link to it in general. [2:16] Hamel: They don’t sort it alphabetically. They were sorted by some kind of… Okay, see you here. [2:24] Dan: Could be chronologically based on when it was created. I was thinking about the same thing. That’s not bad. Who even knows? Early on, Joe, I asked in these five minutes when some people are waiting, what they want to hear us talk about. And the popular response was war stories. So either war stories or what’s something that in your coding has not worked or gone wrong in the last week? [3:33] Jo: In the last week? [3:35] Hamel: Uh-huh. [3:39] Jo: No, I don’t have a lot of war stories for this week. I’ve been trying out some new techniques for evaluating search results. So I’ll share some of those results in this talk. Yeah. So you make some interesting findings and then you also do some mistakes. Use Copilot a lot and its autocompletions are basically a couple of years ago. Some of the OpenAI APIs are changing and so, yeah. Not that interesting though. [4:16] Dan: Yeah, you know, with Copilot, with RAG, you do a lot of metadata filtering so that you try and get more recent results. And it feels to me that with large language models more broadly, It’d be nice to do something so that it tries to auto-complete with newer results rather than older ones. Yeah. You could imagine when you calculate loss functions, there’s a weight involved, and that weight is a function of when the training data is from. It’d be nice if it was something like that. [4:48] Jo: Yeah, it’s also interesting from, I think… the existing technologies like SQL databases, the completions are pretty good, both from ChatGPT and general language models because they have a good, it’s a lot of that data in their training data basically. But if you have a new product with some new APIs, the code completions don’t work that well. That’s why we at Vespa, we also try to. build our own RAG solution on search Vespa AI to help people use Vespa. [5:27] Jo: And that’s one of the things that’s been frustrating with these language models is that they are quite familiar with Elastic because Elasticsearch has been around for quite some time, but Vespa is newer in the public domain. So people are getting better completions for Elasticsearch than Vespa. Awesome. I have to do something about that. [5:49] Dan: Yeah. [6:19] Jo: Yeah, I see some great questions already. So that’s fantastic. So I’m planning on, I’m not sure how much time, because there were quite a few invites, but I’m hoping to spend a half an hour talking and that we could have an open session. So drop your questions. That’s awesome. So we can get the discussion going. And there’s a lot of things to be excited about in search, and I’ll cover some of them, and especially around evaluations. [6:56] Jo: So some major bulk in this talk will be about setting up your own evaluations so that you can actually make changes and iterate on search and actually measuring. the impact of that. And it doesn’t need to be very fancy to have something that you can actually iterate on. And thankfully, large language models can also help us there, thanks to recent advances. So I think that’s interesting. So I’ll try to share my presentation, see if everything is working well. [7:51] Dan: Yeah, we can see it. [7:52] Jo: You can see it? Okay. Okay. Zoom is so much better than Meet. [8:11] Hamel: Yeah, I agree with that. [8:14] Dan: Yeah, it’s a I guess Google has fortunately, at one point, yeah, they have like 10 different solutions for Yeah, I think they’ve probably consolidated them. But they haven’t used that to make them dramatically better. [8:31] Jo: Yeah, because in meat when you run percent, like everything just disappears. Okay, here [8:40] Dan: I have the full view. [8:42] Jo: Yeah. That’s an improvement. [8:46] Dan: All right. We’re five after. We’ve got about 100 people. If you want to wait another minute or two, that’s great. But otherwise, I think you can start anytime. [8:56] Jo: Yeah, sure. I can just get started. Yeah, so thank you for having me. I’ll talk about Back to Basics. And I’m Joe Christenberg. And let’s see if I can actually… Yeah. So about me, I’m a distinguished engineer. I work at Vespa AI. And I’ve been at Vespa AI for 18 years, actually. So I’ve been working in search and recommendation space for about 20 years. And Vespa AI is basically a platform, a serving platform that was recently spun out of Yahoo. We’ve been open source since 2017. [9:38] Jo: And in my spare time, I spend some time on Twitter posting memes. Yeah, and in this talk, I’ll talk about stuffing text into the language model prompt. I’ll talk about… information retrieval, the R in RAG, and most of the talk will be about evaluation of these systems of information retrieval systems, and how you can build your own evals to impress your CTO. And I’ll talk about representational approaches for information retrieval. This includes BM25, vectors, embeddings, whatnot, and some of the baselines. So RAG. [10:20] Jo: You’re all familiar with RAG, but I think it’s also interesting that you can use the kind of the whole RAG concept to stuff things into the prompt, not necessarily related to question answering or search. But for example, if we are building a labeler or a classifier, we can also use retrieval to retrieve kind of relevant examples or examples out of our training data sets, right? So that’s one way, but it’s not that often discussed that you can also use retrieval. [10:55] Jo: So let’s say you have 1 billion annotated training examples, you can actually use retrieval to retrieve relevant examples and then have the large language models reason around that and predict a label. But most are… Thinking about RAG in the context of building this kind of question answering model that you see at Google and all these chatbots and similar where you retrieve for a question, open-ended question, and then you retrieve some hopefully relevant context, and you then stuff that into the prompt. And then you have hopefully the language model will generate a grounded response. [11:48] Jo: It might not be hallucination-free, but some say that it improves the kind of accuracy of the generation step. So that’s kind of demystifying it. And working with these reference architecture, there’s some orchestration component, there’s some input, there’s some output. Hopefully you have some evaluation of that output, prompting, different language models. And then you have kind of state, which can be files, search engines, vector databases, regular databases, or even NumPy. So there’s a lot of things going on here. And There’s a lot of hype around RAG and also different methods for doing RAG. [12:31] Jo: I’m on Twitter a lot and I see all these Twitter threads. There’s a new model. components in this machinery, lots of new tricks, check out this. So it’s a lot of kind of hype. So I like to kind of try to cut through that and, you know, what’s behind this? How does this work on your data? Is this actually a model that actually have some basics or backing from research? Have you actually evaluated it on some data set? And I think… [13:08] Jo: This is, it can be, if you’re like coming into this space and you’re new to retrieval, you’re new to search and you’re new to language models and you want to build something, there’s a lot of confusing information going around. And I just saw this Twitter thread about RAG and people are losing faith in it. And you know, we removed the AI powered search. [13:33] Jo: And I think there’s been like, RAG is only about taking a language model from, for example, OpenAI, and then you use their embeddings, and then you have a magical search experience, and that’s all you need. And I think that’s naive to think that you can build a great product or a great RAG solution in that way just by using vector embeddings and the language models. Because there are the retrieval stack in this pipeline, the process of obtaining relevant information based on some query basically has been around for, like Benjamin in his talk covered, for decades. [14:16] Jo: And There are a lot of people, the brightest minds, that have actually spent a lot of time on retrieval and search, right? Because it’s so relevant across many kind of multi-billion companies like recommendation services, search like Google, Bing, and whatnot. So this has kind of always been a very hot and interesting topic. And it’s much deeper than encoding your text into one vector representation and then that’s it. But. I’ll talk about how we can evaluate these information retrieval systems. [14:54] Jo: And this kind of basically you can treat this as a more or less of a green box where you have put some data into it and you have your kind of retrieval system and you’re asking that retrieval system a question and you’re getting back a ranked list of documents. And then you can evaluate these documents and the quality of these documents with regards to relevance of how relevant they are with regards to the query. [15:25] Jo: And this is kind of independent if you’re using what kind of retrieval method you’re using or combination or hybrids or face ranking or Colbert or Spade or whatnot. You can evaluate any type of system. If it’s using NumPy or files or whatnot, it doesn’t really matter. And the basic idea of building a such system is that you take a query and you retrieve those documents, and then you can have a human annotator, for example, to judge the quality of each of the documents. And there are different ways of doing this. [16:01] Jo: We can do it by the binary judgment, saying that, OK, this document is relevant for the query or not. Or we can have a graded judgment where you say, okay, zero means that the document is irrelevant for the query, and one, it’s slightly relevant, or two is highly relevant. And we can also use this to judge the rank list that are coming out of recommendation systems or personalization and many different systems that are producing a rank list. And in information retrieval, this is going back decades. And there are a lot of researchers working on this. [16:40] Jo: And you have TRACK, which is the text retrieval conference, spans multiple different topics each year, news retrieval, all kinds of different retrieval tasks. MS Marko, maybe some of you are familiar with, which is one of the largest data sets that you can… published research on is from Bing, actually real world data, which is annotated. And a lot of these embedding models are trained on this data set. Then we have Beer from Nils Reimers et al. evaluate types of models without actually using the training data, but this is like in the zero-shot setting. [17:22] Jo: So there are many different collections and then there are metrics that can measure how well the retrieval system is actually working. So recall at K, for example, K here meaning a position in the in the ranking list. So K could be for example 10 or 20 or 100 or 1000 and it’s a metric that is focusing about, you know, you know that there are like six items that are relevant for this query. And are we actually retrieving those six relevant documents into the to the top K? [17:57] Jo: In most systems, you don’t actually know how many relevant documents there are in the collection. In a web scale, it might be millions of documents that are relevant to the query. So unless you have a really good control of your corpus, it’s really difficult to know what are the actually relevant documents in the document. But precision is much easier because we can look at those results and say, Are there any irrelevant hits in the top K? So precision is one, but it’s not really rank aware. [18:32] Jo: So it’s not bothering if the missing or irrelevant hit is placed at position one or 10. the precision at 10 would be the same. It doesn’t necessarily depend on the position. NGCG, very complicated metric, but it tries to incorporate the labels, so the graded labels, and also awareness of the rank position. If you want to look up that, you can basically go to Wikipedia, but it’s a quite advanced metric. Reciprocal rank measures are the same as the Where is the first relevant hit in the position? [19:12] Jo: So if you place the relevant hit at position 1, you have a reciprocal rank of 1. If you place the relevant hit at position 2, you have a reciprocal rank of 0.5. Then, of course, you have LGTEM, which looks good to me. Maybe the most common metric used in the industry. And of course, also in industry, you have other evaluation metrics like engagement, click, if you’re measuring what actually users are interacting with the search, dwell time or e-commerce, add to chart, all these kind of signals that you can feedback. [19:49] Jo: Of course, revenue, e-commerce, search, for example, it’s not only about the relevancy, but also you have some objectives for your business. I also like to point out that most of the benchmarks are comparing just a flat list. And then when you’re evaluating each of these queries, you get a score for each query. And then you take the average to kind of come up with an average number for the whole kind of retrieval method. But in practice, in production systems, you will see that maybe 20% of the queries actually is contributed like 80% of the volume. [20:29] Jo: So you have to think a little bit about that when you’re evaluating systems. Yeah, so, and to do better than looks good to me, you really have to measure how you’re doing. And since we have all these benchmarks, MTAB and whatnot, they don’t necessarily transfer to your domain or your use case. If you’re building a RAG application or retrieval application over code, or documentation, or a specific health domain, or products, because there are different domains, different use cases. Your data, your queries. [21:12] Jo: The solution to do better is to measure and building your own relevance to dataset. And it’s actually not that hard. If you have actually a service in production, look at what actually users are searching for, and look at the results, and put in a few hours, and judge the results. Is it actually relevant? Are you producing relevant results? And it doesn’t really need to be fancy at all. And if you don’t have traffic, if you haven’t launched with it, you obviously have played around with the product. [21:51] Jo: Or you can also ask a large language model to present in some of your content, and then you can ask it, okay, what’s a question that will be natural for a user to retrieve this kind of passage? So you can kind of bootstrap even before you have any kind of user queries. And as I said, it doesn’t need to be fancy. You can log. There are some fancy tools for doing this with user interfaces and Docker and whatnot. But a simple TSTF separated file will do the trick. Preferably, you will have like a static collection. [22:30] Jo: But maybe not everybody has the luxury that you can actually have a static collection. And the reason why you would like to have a static collection is that When you are judging the results and you’re saying that for this query, for instance, query ID 3 and the document ID 5, you say that, oh, this is a relevant one. When we are judging the kind of, or computing the metric for the query, if there’s a new document that is suddenly appearing, which is irrelevant or relevant. [22:59] Jo: it might actually change how we display things in the ranking without being able to pick it up. So that’s why you preferably have these kind of static collections. And all the information retrieval data sets, they are usually static. They don’t change so we can evaluate methods and practices over time. But you can also… in this process, use language models to judge the results. [23:32] Jo: And there’s been interesting research coming out of Microsoft and Bing team for over the last year, where they find that with some prompting techniques that they actually can have the large language models be… pretty good at judging query and passages. So given a passage that is retrieved for the query and they can ask the language model, is this relevant or not relevant? And they find that this actually correlates pretty well. [24:01] Jo: And if you find like a prompt combination that actually correlates with your data or your kind of golden data set, then you can start using this at a more massive scale. And here’s a very recent paper coming out eight days ago where they also demonstrated that this prompt could actually work very well to assess the relevancy of queries. And this can free us from having this kind of static golden data sets, because we could start instead sampling real user queries, and then ask the language model to evaluate the results. [24:43] Jo: So I think this is a very interesting direction. And we have in our RAG or Vespa RAG documentation search, I built like a small golden set with about 90 query pass judgments. And I just ran them through with this prompt or a similar prompt. And I’m getting quite good correlation between what… But I’m judging the results and GPT-4 is judging them, which is good because it means that I can now much cheaper judge more results and then potentially also use this to adjust ranking. Because when you have this kind of data set. [25:30] Jo: you can also iterate and make changes. Then you can see how it’s actually performing. Instead of saying, we changed something, we actually deployed this change that did increase the NGCG with 30 percent. This is from our documentation, Vespa documentation search, which is relevant for us. It’s our domain. You see here, semantic here is off the shelf, vector embedding models, and then there are different ways in Vespa to use hybrids. I won’t go into the details, but now I actually have numbers on it, and then when I’m making changes. So that’s… How about evaluation? [26:16] Jo: So independent of the method or technique that we’re using, we can evaluate the results coming out of the retrieval system. Now I want to talk a little bit about the representational approaches and scoring functions that can be used for efficient retrieval. And the motivation for having this kind of representational approach is that you want to try to avoid scoring all the documents in the collection. [26:45] Jo: So if you’re using, some of you might heard about Cohere re-ranking service or this kind of ranking services where you basically input the query and all the documents and they go and score everything, but then you have everything in memory, already retrieved the documents. And imagine doing that at the web scale or if you have 100 million documents, it’s not possible, right? And it’s also similar to doing a grep. [27:11] Jo: So instead, we would like to have some kind of technique for representing these documents so that we can index them so that when the query comes in, that we efficiently can retrieve over this representation and that we efficiently in sublinear time can retrieve the kind of top ranked docs. And then we can feed that into subsequent ranking faces. [27:37] Jo: And there are two primary representations, and that is the sparse representation, where we basically have the total vocabulary is kind of the whole sparse vector representation that you potentially take, but for a given query or a given document, only the words that are actually occurring in that document or in that query have a non-zero weight. And this can be efficiently retrieved over using algorithms like Weekend or MaxScore and inverted indexes. You’re familiar with Elasticsearch or other kind of keyword search technologies. They build on this. [28:15] Jo: More recently, we also have using neural or kind of embedding or sparse embedding models so that instead of having an unsupervised weight that is just based on your corpus statistics, you can also use transformer models to learn the weights of the words in the queries and the documents. And then you have dense representations, and this is where you have text embedding models, where you take some text and you encode it into this latent embedding space, and you compare queries and documents in this latent space using some kind of distance metric. [28:48] Jo: And there you can build indexes using different techniques, vector databases, different types of algorithms. And in this case, also, you can accelerate search quite significantly so that you can search even billion scale data sets in milliseconds, single credit. But the downside is that there are a lot of tradeoffs related to that the actual search is not exact. It’s an approximate search. So you might not retrieve exactly. the ones that you would do if you did a brute force search over all the vectors in the collection. [29:22] Jo: And these representations are mainly supervised through transfer learning, because you’re using typically an off-the-shelf embedding model that’s been trained on some other data, some data sets. And then you’re trying to apply that to your model. You can fine-tune it if you have relevancy data and so forth. And it’s no longer like a zero-shot or transfer learning, but it’s still like a learned representation. And I think these representations and the whole ChatGPT OpenAI, ChatGPT language model, OpenAI embeddings really opened the world of embeddings to a lot of developers. [30:03] Jo: And this stuck for quite some time and it’s still stuck, I think, because people think that this will give you a magical AI. Powered representation is not bad. And you can also use now a lot of different technologies for implementing search. Vector databases, regular databases, everybody now has a vector search report, which is great, because you can now use different or more wide landscape of different technologies to kind of solve search. But there are some challenges with these text embedding models, especially because of the way they work. [30:48] Jo: Most of them are based on a kind of encoder style transformer model where you take the input text, you tokenize it into a fixed vocabulary. And then you have previously in the pre-training stage and the fine tuning stage, you have learned representations of each of these fixed tokens. Then you feed them through the encoder network. And for each of the input tokens, you have an output vector. And then there’s a pooling step, typically averaging into a single vector representation. So this is how you represent not only one word, but a full sentence. [31:28] Jo: Or even now, with the embedding model coming out today, supporting encoding several books as one vector. But… The issue with this is that the representation becomes quite diluted when you kind of average everything into one vector, which has proven not to work that well for high precision search. So you have to have some kind of shunking mechanism in order to have a better representation for search. And this fixed vocabulary, especially for BERT-based models, you’re basing it off a vocabulary that was trained in 2018. So there are a lot of words that it doesn’t know. [32:12] Jo: So we had one issue here with a user that was searching for our recently announced support for running inference with GGF models in Vespa. And this has a lot of out-of-word, oh sorry, out-of-vocabulary words. So it gets maps to different concepts, and this might produce quite weird results when you are mapping this into the latent embedding space. And then there’s the final question is, does this actually transfer to your data, to your queries? [32:45] Jo: But the framework or the kind of evaluation routines that I talked about earlier will give you the answer to that because then you can actually test if they’re working or not. And also, I think on the baselines, it’s quite important to establish some baselines. And in the information retrieval community, the kind of de facto baseline is BM25. So BM25 is this scoring function where you tokenize the text, linguistic processing, and so forth. It’s well known, implemented in multiple mature technologies like Elastic, Vespa. Tantive, whatnot. [33:29] Jo: I think there was even a library announced today, BM25 in Python. And it builds a model, kind of model, unsupervised from your data, looking at the words that are occurring in the collection, how many times it’s occurring in the data, and how frequent the word is in the total collection. Then there’s the scoring function, and it’s very cheap, small index footprint, and most importantly you don’t have to invoke a transformer embedding model like a 7B LAMA model or something like that, which is quite expensive. [34:04] Jo: It has limitations, but it can avoid these kind of spectacular failure cases of embedding retrieval related to out-of-vocabulary words. The huge downside is that if you want to make this work in CJK languages or Turkish or different types of languages, you need to have some kind of tokenization integrated, which you will find in engines like Elasticsearch or OpenSearch or Vespa. And long context. So we did the announcement earlier this year of supporting Colbert in a specific way. I’m just including this to show you that this is a long context document. [34:48] Jo: So I think they are around 3K tokens long, and the researchers evaluated these different models. And they were presenting results about M3, which is scoring 48.9 in this diagram. And they were comparing it with OpenAI embeddings, with different types of mistrawl or different types of embedding models. And then we realized that, you know, this is actually quite easy to beat. just using a vanilla BM-25 implementation, even Lucene or Vespa or Elasticsearch or OpenSearch. [35:21] Jo: So having that kind of mindset that you can evaluate and actually see what works and remember that BM-25 can be a strong baseline, I think that’s an important takeaway. Then there’s a hybrid alternative. We see a lot of enthusiasm around that, where you can combine these representations, and it can overcome this kind of fixed vocabulary issue with regular embedding models. But it’s not also a single silver bullet reciprocal rank fusions or methods to fuse these kind of different methods. It really depends on the data and the type of queries. [36:02] Jo: But if you have built your own evals, then you don’t have to listen to me about what you should do, because you can actually evaluate and test things out, and you can iterate on it. So I think that’s really critical to be able to build a better rag to improve the quality of the retrieval phase. Yeah, and of course, I talked about long context and that the long context models, we all want to get rid of chunking. We all want to get rid of all the videos about how to chunk. [36:40] Jo: But the basic kind of short answer to this is that you do need to chunk in order to have meaningful representations of text for high precision search. So typically, like Niels Reimers, the de facto embedding expert says that if you go about 250, so 256 tokens, you’re starting to lose a lot of precision, right? There’s other use cases that you can use these embeddings for, like classification, you know, there are a lot of different things, but for high precision search, it becomes very diluted because of these pooling operations. [37:17] Jo: And also, there’s not that many great datasets that you can actually train models on and have longer text. And even if you’re chunking to have meaningful representation, it doesn’t mean that you have to split this into multiple rows in your database. There are technologies that allow you to kind of index multiple vectors per row. So that’s possible. Finally, real world rag. Not sure if you’ve seen this, but there was a huge Google leak earlier. in May, where they revealed a lot of different signals. And in the real world… [38:00] Jo: In the real-world search, it’s not only about the text similarity. It’s not only about BM25 or a single vector cosine similarity. There are things like freshness, authority, quality, page rank you heard about, and also revenue. So there are a lot of different features. And GBDT is still a simple, straightforward method. And it’s still the kind of king of tabular features where you have… specific name features and you have values for them. So combining GBDT with these kind of new neural features is quite effective when you’re starting to actually operate in the real world. [38:44] Jo: So quick summary, I think that information retrieval is more than just a single vector representation. And if you want to improve your retrieval stage, you should look at building your own evals. And please don’t ignore the BM25 baseline. And choosing some technology that has hybrid capabilities, meaning that you can have exact search for exact tokens and still have matches, and also combine the signals from text search via keywords and text search via embeddings. can avoid some of these failure modes that I talked about. And yeah, and finally, real-world search is more than text similarity. [39:37] Jo: So that’s what I had, and I’m hoping for questions. If you want to check out some resources, I do a lot of writing on the blog West by AI, so you can check that out. And if you hated it, you can tweet me at Joe Bergen on Twitter. I’m quite active there, so I appreciate if you hated it. And since then, you can mention me there. And you can also contact me on Twitter. I love getting questions. [40:08] Hamel: That’s a bold call to action if you hated it. [40:12] Jo: It is. [40:16] Hamel: We definitely have a lot of questions. I’ll just go through some of them. What kind of metadata is most valuable to put into a vector DB for doing Rack? [40:27] Jo: Yeah, if you look at only the text domain, if you’re only concerned about text, so you have no freshness component or you don’t have any authority, if you, for example, are building like a healthcare or a health, you know, users are asking health questions, you definitely want to have some kind of filtering. What’s the authoritative sources within hot health? You don’t want to drag up Reddit or things like that, right? So, and title and other metadata, of course, is, but it really depends on the use case. [40:59] Jo: If you’re like a text only use case, or if it’s like more like real world where you have different types of signals. [41:06] Hamel: Makes sense. Do you have any thoughts on calibration of different indices, of the different indices? Not only are different document indices not aligned in terms of similarity scores, but it’s also nice to have confidence scores for how likely the recommendation is to be good. [41:23] Jo: Yeah, I think it’s a very tough question. So these different methods for all these different scoring functions, you can call that, have a different distribution, different shape, different score ranges. So it’s really hard to… combine them and they’re not probabilities as well. So it’s very difficult to map them into a probability that actually this is, or filtering. People want to like, oh, I have a cosine similarity filter on 0.8, but it’s different from different types of model, but combining them. is also a learning task. [41:59] Jo: It also kind of, you need to learn the parameters and GBDT is quite good at that because you’re learning a non-linear combination of these different features. But in order to do that, then you also have to have training data. But the way I described here for doing evaluation can also help you generate training data for training ranking models. Yeah. [42:23] Hamel: So does the calibration really turn into a hyperparameter tuning exercise with your eval set or is that kind of… [42:30] Jo: Well, you could do that, right? If you don’t have any data that you can train a model on to train those parameters, you could do a hyperparameter sweep and then basically check if your eval is improving or not. But if you want to apply more like an ML technique on this, then you would… either like Google is doing, like gathering search and clicks and interactions. But now we also see more that people are actually using large language models to generate synthetic training data. [42:59] Jo: So you can distill kind of the powers of the larger models into smaller models that you can use for ranking purposes. But it’s a very broad topic, I think. So it’s very difficult to kind of deep dive. And it’s very difficult to say that, oh, you should have a cutoff. 0.8 on the vector similarity, or you can do this transformation. So there are no really great tricks to do this without having some kind of training data and at least some evaluations. [43:35] Hamel: What are your observations on the efficacy of re-rankers? And do you really recommend to use a re-ranker? [43:43] Jo: Yeah, because the great thing about re-rankers is that in the phase retrieval and ranking pipelines, you’re gradually throwing away hits using this kind of representational approach. And then you can have a gradual approach where you’re investing more compute into fewer hits and still be within your latency budget. And the great thing about re-rankers like Cohere or cross-encoders, you can deploy them in Vespa as well. is that they offer this kind of token level interaction. [44:15] Jo: because you input both the query and the document at the same time through the transformer network, and then you have token-level interactions. So you’re no longer interacting between the query and the document through this vector representation, but you’re actually feeding all the tokens of the query and the document into the method. So yeah, that definitely can help accuracy. But then it’s becoming a question about cost and latency and so forth. [44:42] Jo: yeah so a lot of trade-offs in this but if you’re only looking at accuracy and you can afford uh the additional cost yeah definitely they can help yeah [45:02] Hamel: Hey, William Horton is asking, do you have advice on combining usage data along with semantic similarity? Like if I have a number of views or some kind of metadata like that from a document? Yeah, [45:17] Jo: it goes into more of a, if you have interaction data, it becomes more of a learning to rank problem. You first need to come out with labels from those interactions because there are going to be multiple interactions and then there’s going to be add to charge and different actions will have different weights. So the standard procedure is that you convert that data into kind of a label data set, similar to what I’ve shown here in the eval. [45:43] Jo: So when you convert that to kind of a label data set, then you can train a model, for instance, a GBT model, where you can include the semantic score as well as a feature. Yeah. [45:58] Hamel: All right. Someone’s asking a question that you may not be familiar with, but I’m just going to give it a shot. It’s a reference to someone else. What are your thoughts on Jason Liu’s post about the value of generating structured summaries and reports for decision makers instead of doing RAG the way we are doing commonly done today. Have you seen that? [46:20] Jo: Are you familiar? I mean, Jason is fantastic. I love Jason. But he’s a high volume tweeter, so I don’t read everything. So I haven’t caught up on that yet. No, sorry. [46:34] Hamel: Okay, no, I don’t want to try to rehash it from my memory either. So I’ll just skip that one. What are some of your favorite advancements recently in text embedding models or other search technologies that people… I’ll just stop the question there. Yeah, what are your… [47:03] Jo: Yeah. Yeah. Yeah, I think embedding models will become better. What I do hope is that we can have models that have a larger vocabulary. So, like LAMA vocabulary, so we have a larger vocabulary. So, like BERT models, they have this old vocabulary from 2018. I think we, I would love to see a new kind of the BERT model trained on more recent data with more recent techniques, like a pre-training stage, including a larger vocabulary for tokenization. I think, as I said, that I’m not… [47:43] Jo: too hyped about increasing the context length because all the information retrieval research shows that they are not that well at generalizing a long text into a good representation for a high precision search. So I’m not so excited about the direction where you’re going with just larger and larger and larger and larger context windows for embedding models because I think it’s the wrong direction. I would rather see… [48:08] Jo: larger vocabularies and better pre-trained models like the Berta it’s still a good model for embeddings [48:18] Hamel: Someone’s asking, does query expansion of out of vocabulary words with BM25 work better at search? And I think like, just to add onto that, do you think people are going as far with classical search techniques as they should? Like things like query expansion and all kinds of other stuff that have been around for a while before, like what’s your feeling about the spectrum and like, yeah. [48:43] Jo: I think you can get really good results by starting. with PM25 and classical resorts and adding a re-ranker on top of that. You won’t get the magic if you have a single word query and there are no words in your collection. Then you might fail at recall, but you don’t get into these kind of really nasty failure modes of embedding vector search alone. And yeah, definitely there are techniques like query expansion, query understanding, and language models. They are also quite good at this. There’s a paper from Google. They did query expansions with Gemini. [49:27] Jo: pretty well, not amazingly well compared to the size of the model and the additional latency. But we have much better tools for doing query expansion and all kinds of fancy techniques now involving prompting of live language models. So definitely that too is really interesting for expansion. So that’s another way. But like in the diagram where I saw this machine and all these components and things like that. [49:53] Jo: What I’m hoping people can take away from this is that if you’re wondering about this technique, that technique, I read about this, is that if you put that into practice in a more systematic way, having your own eval, you will be able to answer those questions on your data, on your queries, without me saying that the threshold should be 0.6, which is bullshit, because I don’t know your queries or your domain or your data. So by building these evals, then you can actually iterate and get the answers. [50:28] Hamel: In one slide, you mentioned limitations in fixed vocabulary with text that is chunked poorly. How do you overcome these sort of limitations in a domain that uses a lot of jargon and that doesn’t tokenize well with an out-of-the-box model? [50:41] Jo: Yeah, then you’re out of luck with the regular embedding models. And that’s why the hybrid capability where you actually can combine the keyword search with the embedding retrieval mechanism. But the hard thing is to understand when to completely ignore the embedding results. Because embedding retrieval, no matter how far they are out in the vector space, will be retrieved, right? So when you’re asking for 10 nearest neighbors, they might not be so near, but you’re still retrieving some junk. [51:11] Jo: And then it’s important to understand that this is actually junk so that you don’t use like techniques like reciprocal rank fusion, which by some vendors is sold as the full kind of blown solution to solve all this. But then you’re just blending rubbish into something that could be reasonable from the keyword search. So currently, and the other alternative is as well that might do a little bit stop capping is to fine tuning your own embedding model. but you still have the vocabulary issues. [51:41] Jo: But if you have resources to kind of do the pre-training stage on your data with a vocabulary that is more matching up with your domain, that might work. But then you have a training job that goes from scratch. But I hear it’s a lot easier to train Bert from scratch nowadays than in 2018. So it might be a viable option for some organizations. Most of the e-commerce companies are doing this. Anyway, they’re starting in all their semantic search papers. They basically say, here’s our pipeline. We pre-trained to build a tokenizer on the whole Amazon corpus. [52:15] Jo: They don’t use BERT-based from 2018. [52:19] Hamel: That makes sense. Okay. Last question. Would you see cold BERT-based methods get around or at least improve retrieval when we’re concerned with tokenizer problems? [52:31] Jo: Yeah, so Colbert to introduce that is basically another neural method where you, instead of learning one vector representation of the full passage or the full query, you are learning token level vector representations. And this is a bit more expensive compute wise at serving time than the regular single embedding models. But. it has close to the accuracy of like the regular re-ranker, but it still also suffer from vocabulary because it still uses the same kind of vocabulary as other models. [53:11] Jo: So, but if we can get better pre-trained model that are trained with a larger vocabulary, I hope that it’s a path towards better kind of neural search with Colbert and other embedding models as well. [53:29] Hamel: Okay, great. Yeah. Yeah, that’s it. There’s certainly more questions, but we don’t want to go on for an infinite amount of time. I think we hit the more important ones. [53:40] Jo: So, yeah, thank you so much. There’s a lot of great questions. So if you want to, you know, throw them at me at Twitter, and I will try to answer as best as possible. Thank you. Thanks Joe. [53:56] Dan: Yeah, thank you. [53:57] Hamel: Yeah, great being here. [53:58] Jo: Great seeing you guys and have a great day. You too. Bye bye.",
    "crumbs": [
      "Educational Resources",
      "RAG",
      "Back to Basics for RAG"
    ]
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": " ",
    "section": "Why",
    "text": "Why\nYou should consider hiring us if:\n\nYou don’t know how to consistently and quickly improve your LLM products.\nYour LLMs are too expensive or slow.\nYou feel blind re: how good/reliable your LLMs are.\nYou are overwhelmed by tools/frameworks."
  },
  {
    "objectID": "index.html#expertise",
    "href": "index.html#expertise",
    "title": " ",
    "section": "Expertise",
    "text": "Expertise\nOur expertise includes:\n\nBuilding domain-specific evaluation systems\nOptimizing RAG\nFine-tuning\nCreating dev tools and infrastructure to help you iterate quickly on LLMs.\nLLM vendor evaluation and selection"
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": " ",
    "section": "Solutions",
    "text": "Solutions\nWe offer two tiers of services to support your goals.\n\nContact\nEmail: consulting@parlance-labs.com"
  }
]