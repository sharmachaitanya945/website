[
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": " ",
    "section": "Why",
    "text": "Why\nYou should consider hiring us if:\n\nYou don’t know how to consistently and quickly improve your LLM products.\nYour LLMs are too expensive or slow.\nYou feel blind re: how good/reliable your LLMs are.\nYou are overwhelmed by tools/frameworks."
  },
  {
    "objectID": "index.html#expertise",
    "href": "index.html#expertise",
    "title": " ",
    "section": "Expertise",
    "text": "Expertise\nOur expertise includes:\n\nBuilding domain-specific evaluation systems\nOptimizing RAG\nFine-tuning\nCreating dev tools and infrastructure to help you iterate quickly on LLMs.\nLLM vendor evaluation and selection"
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": " ",
    "section": "Solutions",
    "text": "Solutions\nWe offer two tiers of services to support your goals.\n\nContact\nEmail: consulting@parlance-labs.com"
  },
  {
    "objectID": "talks/fine_tuning/mistral_ft_sophia.html#chapters",
    "href": "talks/fine_tuning/mistral_ft_sophia.html#chapters",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nSophia Yang introduces herself and provides an overview of the talk, which will cover Mistral models, their fine-tuning API, and demos.\n0:35 Mistral’s History and Model Offerings\nSophia discusses Mistral’s history, from their founding to the release of various models, including open-source and enterprise-grade models, as well as specialized models like CodeStraw.\n02:52 Customization and Fine-Tuning\nMistral recently released a fine-tuning codebase and API, allowing users to customize their models using LoRa fine-tuning. Sophia compares the performance of LoRa fine-tuning to full fine-tuning.\n04:22 Prompting vs. Fine-Tuning\nSophia discusses the advantages and use cases for prompting and fine-tuning, emphasizing the importance of considering prompting before fine-tuning for specific tasks.\n05:35 Fine-Tuning Demos\nSophia demonstrates how to use fine-tuned models shared by colleagues, as well as models fine-tuned on specific datasets like research paper abstracts and medical chatbots.\n10:57 Developer Examples and Real-World Use Cases\nSophia showcases real-world examples of startups and developers using Mistral’s fine-tuning API for various applications, such as information retrieval, medical domain, and legal co-pilots.\n12:09 Using Mistral’s Fine-Tuning API\nSophia walks through an end-to-end example of using Mistral’s Fine-Tuning API on a custom dataset, including data preparation, uploading, creating fine-tuning jobs, and using the fine-tuned model.\n19:10 Open-Source Fine-Tuning with Mistral\nSophia demonstrates how to fine-tune Mistral models using their open-source codebase, including installing dependencies, preparing data, and running the training process locally."
  },
  {
    "objectID": "talks/fine_tuning/mistral_ft_sophia.html#resources",
    "href": "talks/fine_tuning/mistral_ft_sophia.html#resources",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Resources",
    "text": "Resources\nLinks to resources mentioned in the talk:\n\nmistral-finetune is a light-weight codebase that enables memory-efficient and performant finetuning of Mistral’s models.: mistral-finetune\nExample notebook\nFine-Tuning guide\nSpaces in the prompt templates for different versions of Mistral: tweet\nMistral Inference guide."
  },
  {
    "objectID": "talks/fine_tuning/mistral_ft_sophia.html#full-transcript",
    "href": "talks/fine_tuning/mistral_ft_sophia.html#full-transcript",
    "title": "Best Practices For Fine Tuning Mistral",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:01] Sophia Yang: Okay, cool. Yeah. Yeah. Thank you so much everyone for joining this course and this talk. I’m super excited to be here. My name is Sophia Young. I lead developer relations at Mistral. So, yeah, sorry, it’s June already. Online of this talk, this talk is we’re going to talk about an overview of Mistral models. We’re going to talk about our fine-tuning API that we just released today. And also we have an open source fine-tuning code base that you can play with. And then I will show you some demos. Some brief history about Mistral. [0:40] Sophia Yang: I hope all of you know some of Mistral, but if you don’t, we are a Paris based team of more than 50 people. We were founded about a year ago and in September last year we released our first model, Maestro 7B. And then December we released Maestro 8x7B with a lot of good feedback on both models. And we also released our first commercial model, Maestro Medium, and also the platform where you can use our model on the platform through our API. And then February this year we released Mr. Small and Mr. Large. Mr. [1:20] Sophia Yang: Large is our flagship model with advanced reasoning, multilingual capabilities, function calling, all the good things. And Lachat is our conversational AI interface. It’s completely free. So if you’d like to talk to our models, please sign up and use Lachat. And in April this year, we released, at the time it was the best open source model, A times 22B. It was really good. [1:49] Sophia Yang: And just last week, we released CodeStraw, which is a specialized model trained on 80 plus languages, programming languages, and you can use with various VS Code plugins to generate code and talk with your code. And here’s another view of our model offerings. We have three open source models, Apache 2 license, which means you can use it freely for your personal use cases or commercial use cases. We have two enterprise grade models, Mr. Small and Mr. Large. Yeah, so for your most sophisticated needs, Mr. Large is really, really good. [2:33] Sophia Yang: It supports multilingual function calling, really good at RAG, And now we support fine-tuning Mistral-small and Mistral-7b. And again, we have specialized Postal for coding, and we have an embedded model. So we really care about customization. A lot of people asking about, like, how do you fine-tune Mistral? A lot of people want to have a recipe or want to use our API to fine-tune our model. So about… Two weeks ago, we released Mr. FineTune, which is a fine-tuning code base everyone can use to fine-tune our open-source models. [3:18] Sophia Yang: Then just two hours ago, we announced a fine-tuning API so you can customize your Mr. Model using our fine-tuning API directly. And yeah, so the technology we use is LoRa fine tuning. Since this is probably the end of the course, you probably already know a lot about LoRa. It’s very efficient. It’s very performant. So we did some analysis on the comparison between Mr. LoRa fine tuning and a full fine tuning on Mr. 7B and Mr. Small. So they have really similar performance, as you can see here. [4:00] Sophia Yang: MR7B, lower fine tuning on this benchmark is 0.9 and the full fine tuning is 0.91. So very, very close. Note that this benchmark is an internal benchmark normalized to this MR small number. So just some note there. And then before I show you some demos, just some thoughts on prompting and fine tuning. You probably already learned about this. So before you start fine tuning, you should always think about maybe you can just do prompting. So with prompting, your model can work out of the box, doesn’t require any data or training to make it work. [4:44] Sophia Yang: So it’s really easy and it can be easily updated for new workflows or prototyping. With fine-tuning, sometimes it works a lot better than prompting. It can work better than a larger model, faster and cheaper, because it doesn’t require a very long prompt. So for specific use cases with a large model, if you have a sophisticated prompt, you can make it work. But with small fine-tuned models, you can just fine-tune it on specific behavior that doesn’t require a long prompt. [5:21] Sophia Yang: So, yeah, again, it’s better alignment with the task of interest because it’s specifically trained on different tasks and you can teach new facts and information. Okay, so now I want to show you some demos, basically how to use our FineTune API and also how to use, excuse me, our Mr. FineTune, the open source code base. specifically, I want to show you four demos. Demo of how can you use a fine-tuned model that may be fine-tuned by yourself or by someone else. Demo of some developer examples, like real-world use cases. [6:07] Sophia Yang: And also the API work through and the Mistral fine-tune walkthrough. So let me move this bar. Okay, so in this notebook, first of all, we need to install Mistral API. This is the latest version, that was released today. So if you want to use our fine-tuning features, make sure you have the latest version. And in this notebook, I’m actually using three fine-tuned models shared by my coworker. [6:49] Sophia Yang: So if you are in an organization, you want to use models created by your coworker, or you want to share the fine-tuned model you yourself created, you can use the model from the same organization with different sets of options. So it’s really easy for me to use. a model that my coworker has fine-tuned today. So for the first example, it was trained on title abstract peers from archive. So basically if you input a title of a research paper, this model will generate a abstract for you. [7:33] Sophia Yang: So if I input fine-tuning is all you need, it will give us some… similarly okay abstract for this paper title. And then just another fun example, the croissant convolution. We made it up, obviously. So the abstract is like we propose novel convolution there, the croissant convolution. So it’s just a fun example for people to see how the finding link could work to play with. And then I have another example, Mroy is all you need. And again, it will output the abstract. [8:13] Sophia Yang: So I’m or a research paper because we trained on the title and abstract from the research papers. And another example, here’s another model. Note that The model name always have this structure. It’s fine-tuned. It’s fine-tuned on OpenMistral7b, our 7b model. So it will always have this name here, and it will have some random strings for the fine-tuned version. So for the medical chatbot, we actually trained on this hanging face data set of AI medical chatbot data. And then… [8:56] Sophia Yang: Here, as you can see, we ask it some questions and it will answer like it was a chatbot. And another example, so those two examples are from actual data. So here the data we get from archive and here’s the data we got from Hugging Face. But what if you don’t have the data? You can actually generate data from a larger model. Sometimes we want to mimic the behavior of a larger model like Mr. Large because we know Mr. Large behaves really well and we want to do some knowledge distillation or knowledge transfer. [9:34] Sophia Yang: from a larger model. So in this case, if you want to see how exactly we generated the data, you can go to our docs. And in this example, we have this prompt, you are a news article stylist following the economist style guide. So basically, we want to rewrite the news as the economist news article. And then we use Mr. Large to generate the revised news or like different revisions of how the news can be revised. And we give it some guidelines, basically. So So in this example, we use Mr. [10:18] Sophia Yang: Large to generate our data, and then we give it a news article, right? And then we use the fine-tuned data. It can generate a new piece of news article that may sound more like economist news. And then if you give it some prompting, it can also generate a set of revisions it proposed for you to change, how you can change the style of the news to make it sound better. Or more like economist. Yeah, so that’s. That’s some quick demonstrations of how the fine-tune model can look like. [10:57] Sophia Yang: In our docs, we have some developer examples of real-world use cases that have been using our fine-tuning API. For example, we have FOSFO. They are a startup using our fine-tune model for RAC, for Internet retrieval, and you can see the details here. We have examples of RAC for medical domain, financial conversation assistant, legal co-pilot. So they are all using fine-tuned Mr. Models with our fine-tuning API. And in these examples, you can see the specific data. You can see the evaluation training steps and how the benchmark results look like. [11:47] Sophia Yang: Yeah, so the fine-tuned with just small is better than the not fine-tuned with just small. Not surprising. Yeah, and then different results. So yeah, if you’re interested, you can check out our developer examples for some real-world use cases of fine-tuned model. So, Yeah, so in this next section, next demo, there are so many demos I want to show you today because it’s very exciting. In this next section, I want to show you an end-to-end example of how you can use Mistral Fine Tuning API on your own and for your own dataset. [12:33] Sophia Yang: Of course, we need to install Mistral AI. Make sure it’s or some numbers larger than that after probably after today. And we also need to install Pandas. The first step is, of course, we need to prepare our data set. Yeah. So whenever you want to do some fine tuning, the first step is always the data. In this simple example, we. read data from a parquet file that’s hosted on Honeyface is the UltraChat data. [13:10] Sophia Yang: We are only reading one parquet file because this data is quite large, and we don’t want to spend too much money on it. So you can feel free to change the data to your own use cases. We split the data into training and evaluation here, and then we save the data locally into the JSON-IL format. This format is actually needed for using our API. And note that here are the sizes for the training data and evaluation data. So there is a size limit here for our API. For training data, it’s limited at 512 megabytes. [13:49] Sophia Yang: So you can have multiple files feeding to your training, your fine-tuning pipeline. But each file needs to be under 512 megabytes. For your evaluation dataset, it needs to be less than one megabytes. Yeah, so a lot of times the data on Hugging Face is not greatly formatted for the training purposes. So we have a script to help you reformat your data. This script is not, maybe not robust in all of the use cases, but in our use cases it works. [14:35] Sophia Yang: So if your use case is more sophisticated, you might want to change the script to your own use case accordingly. So in this example, we basically just skipped several examples because they’re not not right formatted. So if we take a look at one of the examples, one of the issues here is the assistant message is empty. So we can’t really train based on this kind of data. So we need to make sure the data is right formatted. Otherwise, it’s going to be difficult. [15:09] Participant 1: I have a quick question about this. Is there like a format validator that y’all have? I see that you have a thing to convert it to format, but I’m curious about validation. [15:19] Sophia Yang: That’s a great question. We do have a validator script as well. That was in my next notebook. [15:26] Participant 1: Oh, sorry about that. [15:28] Sophia Yang: No, no, no. That’s a great question. So we do have a validation data script from the Mr. Find2 repo. Also, whenever you upload the model to our server, it will validate the data for you. So if your data is not right, it will tell you why it’s not right and ask you to change it. [15:54] Participant 1: Excellent. [15:55] Sophia Yang: Yeah, thanks for the question. Yeah, so yeah, so the next step is to upload the data set with the files create function and then you can just define the file name and the actual file here and we’re uploading the file and then we can see the ID of the file, the purpose of the file. The purpose right now is just fine too, maybe there will be some other purposes later, but right now it’s just fine too. And then to create a fine tuning job, you will need the IDs of the files we just uploaded. [16:32] Sophia Yang: And of course, you need to define the model you want to fine tune. Right now we only support minstrel7b and minstrelsmall, but in the future we might add more models here. And then you can try different hyperparameters. And in this example, I only ran 10 steps just to have it faster. But if you want to increase your steps, actually, you probably should increase your steps if you’re doing something serious. Yeah. So make sure you can change those different configurations here. [17:10] Sophia Yang: And then we can find the job ID and then we can use, and then we can like do different things like listing the jobs that we have. I have quite a few jobs that I ran. And we can retrieve a job based on the job ID to see if it’s successful or not. And then you can see we have some metrics here, training loss, validation loss, token accuracy. Because we only ran 10 steps, you only see this once. If you run it 100 steps, you will see it 10 times. [17:48] Sophia Yang: So every 10 step or every 10% of the step, you will see the metrics. So you can see if you are making progress or not making progress. And finally, with the fine-tuned model, when the model is, the job is successful, you will see the fine-tuned model get populated from your retrieved jobs, and then you can call this model and then ask any questions you want. So, so that’s, that’s how you can use a fine-tuned model. It’s exactly the same syntax as what if you, if you’re using our normal, non-fine-tuned models. [18:31] Sophia Yang: Okay, and finally, if you want to use weight and biases, you can add your weight and biases credentials here. Yeah, you will need your API key and everything. Just to show you how that might look like. Basically, you can check your losses, your perplexity score, your learning rate and everything. So. Yep, so that’s how you can run the fine-tuning through our API. I hope it’s clear. Any questions? Okay, cool. [19:11] Sophia Yang: So this last demo I want to show you is what if you want to just use our open source code base to fine tune MISDRAW7B or other MISDRAW models. I’m actually running this in Colab Pro Plus account, so it’s possible to fine tune a model in Colab. So in this example, we… I just git clone this repo, the MrFinding repo, because it’s not a package, it’s a repository. And in this repo, we need to install all the required packages. And of course we need to download our model because we’re doing everything basically locally. [19:58] Sophia Yang: And we’re downloading this model from the Mistral website and we’re downloading 7BB3 model here. And then… Same thing as we have seen before, we’re preparing the dataset. We’re using the exactly the same data as before reading the parquet file, splitting into training and evaluation, save the data locally. And then. Same as before, we reformat our data. Everything, the evaluation data looks pretty good. And afterwards, you can verify if your data is correctly formatted or not with our evaluation data script. [20:40] Sophia Yang: So yeah, so it will, this, this actually will take some time because it’s evaluating, yeah, each record. And then you can start training. The important thing here is actually this config file, right? This basically is telling the model, telling the LoRa how you want to fine-tune your model and different, where is the path of everything. So we have the data file. You want to define the path of your instruct data, your evaluation data. This is the training data, your evaluation data. We want to define the path of your model. [21:26] Sophia Yang: You might need to define or just leave it as default, those hyperparameters. We recommend using a sequence length of 32K, but because I’m using a Colab as the memory is limited, I ran into auto memory issue a lot. So I decreased this number to 8,000. But if you have a better GPU, you should use 82. [21:54] Sophia Yang: a thousand and then you can you can define all the other fun stuff right okay yeah and then with this one line of code we can start training and fine-tune our model um and then in uh the result as a result you can see the checkpoint of of your LoRa result here. This is where we can use to in our inference as our fine tuning model. And to run our inference, we have a package called Mr. Inference to help you run all of our open source models and also all the fine tuning models. [22:40] Sophia Yang: So basically, In this, in Mistral inference, you need to define the tokenizer, which is in the, which you are downloading from the Mistral, Mistral fine tune file. We use the v3 tokenizer because it’s a v3 model. And then we need to define the model that’s reading from the models folder we downloaded. And then we need to load LoRa from the checkpoint that we have, we just saved from the fine tuning process. And then we can run check completions and get the result. So that’s basically how you can run Mr. Fine-Tuning with Mr. [23:21] Sophia Yang: Fine-Tune the code base. And finally, I want to share some exciting news that since we just released our Fine-Tuning API, we are hosting a Fine-Tuning Hackathon starting today to June 30th. Yeah, so please feel free to check out our hackathon and you can submit from this Google form and yeah, very exciting. Looking forward to see what people are building. Thank you so much. [24:00] Participant 1: It’s very cool. What’s your favorite, like, so the very end you did like kind of like training like an open model, not using API or whatever, using Torch Run. Is that your preferred way to fine tune? I prefer the open models. [24:23] Sophia Yang: You can fine tune Mr7b with our API as well. I would recommend using our API just because you don’t need a GPU. It’s so much easier. You will not run into out of memory issues, hopefully."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Hamel Husain is a machine learning engineer with over 25 years of experience. He has worked with innovative companies such as Airbnb and GitHub, which included early LLM research used by OpenAI for code understanding. He has also led and contributed to numerous popular open-source machine-learning tools."
  },
  {
    "objectID": "team.html#hamel-husain",
    "href": "team.html#hamel-husain",
    "title": "Team",
    "section": "",
    "text": "Hamel Husain is a machine learning engineer with over 25 years of experience. He has worked with innovative companies such as Airbnb and GitHub, which included early LLM research used by OpenAI for code understanding. He has also led and contributed to numerous popular open-source machine-learning tools."
  },
  {
    "objectID": "team.html#jason-liu",
    "href": "team.html#jason-liu",
    "title": "Team",
    "section": "Jason Liu",
    "text": "Jason Liu\n\nJason Liu is a distinguished machine learning consultant known for leading teams to successfully ship AI products. Jason’s technical expertise covers personalization algorithms, search optimization, synthetic data generation, and MLOps systems. His experience includes companies like Stitch Fix, where he created a recommendation framework and observability tools that handled 350 million daily requests. Additional roles have included Meta, NYU, and startups such as Limitless AI and Trunk Tools."
  },
  {
    "objectID": "team.html#dan-becker",
    "href": "team.html#dan-becker",
    "title": "Team",
    "section": "Dan Becker",
    "text": "Dan Becker\n\nDan has been working in AI since 2012, when he finished 2nd (out of 1350 teams) in a machine learning competition with a $500,000 prize. He has contributed to open source AI tools like TensorFlow and Keras, worked as a data scientist at Google and lead the product team building AI Development Tools for DataRobot. Over 100,000 people have taken his deep learning courses on Kaggle and DataCamp."
  },
  {
    "objectID": "team.html#jeremy-lewi",
    "href": "team.html#jeremy-lewi",
    "title": "Team",
    "section": "Jeremy Lewi",
    "text": "Jeremy Lewi\n\nJeremy Lewi is a Machine Learning platform engineer with over 15 years of experience and expertise in using AI to solve practical business applications. He has built platforms for YouTube, Google Cloud Platform, and Primer, enabling ML Engineers and data scientists to rapidly develop and deploy models into production. He played a pivotal role in developing systems like YouTube’s Video Recommendations and made major contributions to open-source software, including creating Kubeflow, one of the most popular OSS frameworks for ML.\nJeremy is an expert in Cloud services, Kubernetes, MLOps, LLMOps, CICD, IAC, and GitOps."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Applied AI Consulting",
    "section": "",
    "text": "We are a group of experienced AI Engineers who have built AI products at leading tech companies. We specialize in helping companies improve their LLM products. See our current and past clients here."
  },
  {
    "objectID": "services.html#about",
    "href": "services.html#about",
    "title": "Applied AI Consulting",
    "section": "",
    "text": "We are a group of experienced AI Engineers who have built AI products at leading tech companies. We specialize in helping companies improve their LLM products. See our current and past clients here."
  },
  {
    "objectID": "services.html#problems-we-solve",
    "href": "services.html#problems-we-solve",
    "title": "Applied AI Consulting",
    "section": "Problems We Solve",
    "text": "Problems We Solve\nYou should consider hiring us if …\n\nYou don’t know how to improve your LLM products systematically.\nYour LLMs need to be cheaper or faster.\nYou are overwhelmed by tools & frameworks.\nFeel lost on what to prioritize and what experiments you should run.\nNeed outside expertise to evaluate your needs and vet potential talent."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Applied AI Consulting",
    "section": "Services",
    "text": "Services\nWe offer two tiers of services to support your goals.\n\nTier 1: Strategy\nI will advise you on the following topics:\n\nLLM performance issues: (cost, quality, speed)\nCustom Evaluation Systems\nRAG, Fine-Tuning, and Prompt Engineering\nStrategy & Hiring: I will introduce you to talent, vendors, and partners in my network.\n\nI’ll give you ongoing feedback and guidance via regular meetings with your team. This will help you avoid common pitfalls and select the right tools and techniques, saving you time and money.\n\n\nTier 2: Comprehensive\nEverything in Tier 1, plus:\n\nImplementation: We will write production-ready code and/or prototypes to accelerate your AI product development.\nDomain-Specific Evaluation Systems: We will design and implement custom evaluation systems to measure the performance and reliability of your LLMs.\nHands-On Model Optimization: We will fine-tune, prompt engineer and debug models to improve performance and efficiency.\nDevelopment Tools and Infrastructure: Build custom tools and infrastructure to streamline your AI development process.\nContent and Writing: Produce written documents and blogs to communicate best practices, methodologies, and practical case studies to your stakeholders\nTeam Growth & Hiring: Work with 2-4 people individually on your team to rapidly upskill them on AI. I will also help you source and evaluate key hires.\n\n\n\nPricing\nWe work on a monthly retainer basis. The cost depends on the tier of service you choose:\n\nStrategy: $15,575/month\nComprehensive: $88,300/month\n\nContact us at consultingl@parlance-labs.com to discuss starting an engagement. You can also book a paid call if you need immediate short-form advice."
  },
  {
    "objectID": "services.html#current-past-clients",
    "href": "services.html#current-past-clients",
    "title": "Applied AI Consulting",
    "section": "Current & Past Clients",
    "text": "Current & Past Clients\nMembers of our team have worked with the following companies:\n\nLimitless AI: Limitless AI is a personal memory assistant that helps you remember, organize, and navigate your life.\nRaycast: Raycast is a blazingly fast, totally extendable launcher. It lets you complete tasks, calculate, share common links, and much more.\nTensorlake: Build Knowledge for LLMs from un-structured data\nReplicate: development of fine-tuning serverless infrastructure using axolotl and optimized LLM inference.\nWeights & Biases: Provide product guidance for evaluation, annotation, and observability. I am currently assisting with their LLM initiatives.\nHoneycomb: Currently improving the natural language query assistant through evaluation systems and fine-tuning.\nLangChain/LangSmith: provided product guidance for enterprise LLM tools.\nModal: Serverless tools for fine-tuning.\nRechat: I am working on Lucy, a conversational AI for real estate agents. You can read about my recent work on Rechat here.\nAnswer.ai: Conduct research on new LLM applications and help with product strategy.\nAxolotl: I am a core contributor to axolotl, a library for efficient fine-tuning of LLMs.\nKay.ai: Retrieve relevant context from the semantic web for your LLM apps with fully hosted embeddings.\nModal Labs: Modal specializes in cloud functions, offering a platform for running generative AI models, large-scale batch jobs, and more.\nPydantic: Pydantic provides data validation and settings management using Python type annotations, enforcing type hints at runtime with user-friendly error handling.\nPosit: Helped Posit expand into the Python AI and ML ecosystem.\nCatena: LLM infrastructure such as routing, evaluation, and orchestration."
  },
  {
    "objectID": "talks/rag/ben.html",
    "href": "talks/rag/ben.html",
    "title": "Beyond the Basics of Retrieval for Augmenting Generation",
    "section": "",
    "text": "This talk was given by Ben Clavié at the Mastering LLMs Conference."
  },
  {
    "objectID": "talks/rag/ben.html#chapters",
    "href": "talks/rag/ben.html#chapters",
    "title": "Beyond the Basics of Retrieval for Augmenting Generation",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nHamel introduces Ben Clavier, a researcher at Answer.ai with a strong background in information retrieval and the creator of the RAGatouille library.\n00:48 Ben’s Background\nBen shares his journey into AI and information retrieval, his work at Answer.ai, and the open-source libraries he maintains, including ReRankers.\n02:20 Agenda\nBen defines Retrieval-Augmented Generation (RAG), clarifies common misconceptions, and explains that RAG is not a silver bullet or an end-to-end system.\n05:01 RAG Basics and Limitations\nBen explains the basic mechanics of RAG, emphasizing that it is simply the process of stitching retrieval and generation together, and discusses common failure points.\n06:29 RAG MVP Pipeline\nBen breaks down the simple RAG pipeline, including model loading, data encoding, cosine similarity search, and obtaining relevant documents.\n07:54 Vector Databases\nBen explains the role of vector databases in handling large-scale document retrieval efficiently and their place in the RAG pipeline.\n08:46 Bi-Encoders\nBen describes bi-encoders, their efficiency in pre-computing document representations, and their role in quick query encoding and retrieval.\n11:24 Cross-Encoders and Re-Ranking\nBen introduces cross-encoders, their computational expense, and their ability to provide more accurate relevance scores by encoding query-document pairs together.\n14:38 Importance of Keyword Search\nBen highlights the enduring relevance of keyword search methods like BM25 and their role in handling specific terms and acronyms effectively.\n15:24 Integration of Full-Text Search\nBen discusses the integration of full-text search (TF-IDF) with vector search to handle detailed and specific queries better, especially in technical domains.\n16:34 TF-IDF and BM25\nBen explains TF-IDF, BM25, and their implementation in modern retrieval systems, emphasizing their effectiveness despite being older techniques.\n19:33 Combined Retrieval Approach\nBen illustrates a combined retrieval approach using both embeddings and keyword search, recommending a balanced weighting of scores.\n19:22 Metadata Filtering\nBen emphasizes the importance of metadata in filtering documents, providing examples and explaining how metadata can significantly improve retrieval relevance.\n22:37 Full Pipeline Overview\nBen presents a comprehensive RAG pipeline incorporating bi-encoders, cross-encoders, full-text search, and metadata filtering, showing how to implement these steps in code.\n26:05 Q&A Session Introduction\n26:14 Fine-Tuning Bi-Encoder and Cross-Encoder Models\nBen discusses the importance of fine-tuning bi-encoder and cross-encoder models for improved retrieval accuracy, emphasizing the need to make the bi-encoder more loose and the cross-encoder more precise.\n26:59 Combining Scores from Different Retrieval Methods\nA participant asks about combining scores from different retrieval methods. Ben explains the pros and cons of weighted averages versus taking top candidates from multiple rankers, emphasizing the importance of context and data specifics.\n29:01 The Importance of RAG as Context Lengths Get Longer\nBen reflects on how RAG may evolve or change as context lengths of LLMs get larger, but emphasizing that long context lengths are not a silver bullet.\n30:06 Chunking Strategies for Long Documents\nBen discusses effective chunking strategies for long documents, including overlapping chunks and ensuring chunks do not cut off sentences, while considering the importance of latency tolerance in production systems.\n30:56 Fine-Tuning Encoders and Advanced Retrieval with ColBERT\nBen also discusses when to fine-tune your encoders, and explains ColBERT for advanced retrieval."
  },
  {
    "objectID": "talks/rag/ben.html#slides",
    "href": "talks/rag/ben.html#slides",
    "title": "Beyond the Basics of Retrieval for Augmenting Generation",
    "section": "Slides",
    "text": "Slides\nDownload PDF file."
  },
  {
    "objectID": "talks/rag/ben.html#additional-resources",
    "href": "talks/rag/ben.html#additional-resources",
    "title": "Beyond the Basics of Retrieval for Augmenting Generation",
    "section": "Additional Resources",
    "text": "Additional Resources\nThe following resources were mentioned during the talk:\n\nEasily use and train state of the art late-interaction retrieval methods (ColBERT) in any RAG pipeline. https://github.com/bclavie/RAGatouille\nA lightweight unified API for various reranking models: https://github.com/AnswerDotAI/rerankers\nA Hackers’ Guide to Language Models: https://www.youtube.com/watch?v=jkrNMKz9pWU\nGLiNER: Generalist Model for Named Entity Recognition using Bidirectional - Transformer: https://arxiv.org/abs/2311.08526\nFine-Tuning with Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/training_overview.html\nElastic, Dense vector field type: https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html"
  },
  {
    "objectID": "talks/rag/ben.html#full-transcript",
    "href": "talks/rag/ben.html#full-transcript",
    "title": "Beyond the Basics of Retrieval for Augmenting Generation",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:01] Hamel: Ben Clavier is one of the cracked researchers who work at Answer.ai. You’ve heard from several researchers from Answer.ai already in this conference. Ben has a background in information retrieval, amongst other things, and he has an open source package called Ragatouille, which you should check out. He also comes from a deep background in information retrieval. and brings that to RAG. And he’s also one of the clearest thinkers on the topic. But yeah, I’ll hand it over to you, Ben, to kind of give more color to your background, anything that I missed. [0:45] Hamel: And yeah, we can just jump into it. [0:48] Ben: Okay, let’s go. So I think that’s pretty much the key aspect of my background. You pretty much read this slide out. So I do R&D at Ansoya with Jeremy. You’ve seen Jono in this course and there’s a lot of other awesome people. We’re a distributed R&D lab, so we do AI research and we try to be as open source as possible because we want people to use what we build. Prior to joining ANSR, I did a lot of NLP and kind of stumbled upon information retrieval because it’s very, very useful and everybody wants information retrieval. [1:20] Ben: It’s more for clarifying what information retrieval is, which I hope today’s talk will help. And yeah, so my claim to fame or claim to moderate fame at least is the Ragatool library, which makes it much easier to use a family of models called Colbert. which we will very briefly mention today, but won’t have time to go into detail. But hopefully, like, if you want to know more about that, like, do feel free to ping me on Discord. I’m generally either very responsive or you need to ping me again. Pretty much how I work. [1:50] Ben: And I also maintain the ReRankers library, which we’ll discuss in one of the later slides. And yeah, if you know me, I want to follow me. I want to hear more. But what I do is pretty much all on Twitter. I’m not on LinkedIn at all. I’m just everything go through Twitter. A lot of memes and shitposts, but some very informative stuff once in a while. So. So yeah, and let’s get started with what we’re going to talk about today. So it’s only half an hour, so we’re not going to talk about a lot. [2:20] Ben: I’m going to talk about why I think I’ll do like call retrieval basics as they should exist in your pipelines, because RAG is a very nebulous term and that will be the first slide and Hamel will be very happy about that slide, I think. But RAG is not a silver bullet. RAG is not a new thing from December 2022. RAG is not even an end-to-end system. We’ll cover that, but I think it’s very important to like ground it a bit when we talk about RAG because it means a lot of different things to different people. [2:47] Ben: Then we will cover what we call the compact MVP, which is what most people do when they are starting out with RAG. It’s actually an example from Jeremy. It’s like the simplest possible implementation of RAG, as in just using a vector search. And then the other topics are basically things that I think you should have in your rack pipeline as part of your MVP. And I’ll show that like there’s a lot of scary concepts because they’re all big walls like by encoder, cross encoder, TFIDF, BM25, filtering. [3:14] Ben: That sounds like a lot, but then I’m going to try and show it that they’re very simple concepts and you can have pretty much the same MVP by adding just 10 lines of code, by choosing like by the state of the art retrieval components in every bit. And the bonus, which I don’t think we’ll have time to cover when I try this again, was talking about Colbert because I like talking about Colbert. So I might do it at the end if we have some time, but I might not. And yeah, that’s it for the agenda. [3:40] Ben: And then I also think it’s important to have the counter agenda, which is what we won’t be talking about today, because those are just as important for RAG. But they are not what we put in the very basics. And here we’re very much about the basics. So one of them is. How to monitor and improve RAC systems because RACs are systems and they’re living systems and they’re very much things you should monitor and continuously improve on. I think Jaydon covered that quite well in his talk yesterday or last week. Yeah, last week. [4:07] Ben: So I would invite you to watch that and watch Jaydon and Dan’s upcoming course if it does materialize. Evaluations, they’re also extremely important, but we won’t talk about them at all today. I know that Joe will talk about them at length in his talk. Benchmarks and paper references. So I’ll make a lot of claims that you will just have to trust me on because I don’t want to have too many references or too many academic looking tables and this trying to keep it quite lively and airy. [4:33] Ben: I won’t give you a rundown of all the best performing models and why you should use them. I won’t talk about training, data augmentation, et cetera. And I won’t talk about all the other cool approaches like Splayed, Colbert, and details because they go beyond the basics. But those are all very important topics, so if you’re interested, do look up, there’s a lot of good resources out there. Do feel free to ask me. And with that, let’s get started with the rant, which is my favorite part. [5:01] Ben: This is a thing that Hamel has been doing on Twitter recently as part of his flame posting campaign, I’ll say, which is basically, there’s so much in AI, so much in especially the LLM world that uses worlds that are like a lot scarier than they need to be. And RUG’s probably that because to me when I hear retrieval of matter generation or RUG, it sounds like that’s an end-to-end system, that’s a very definite set of components, that’s a thing that works on its own. [5:27] Ben: And it’s not, it’s literally just doing retrieval to put stuff into your prompt context, like before your prompt or after your prompt, you want to get some context, so you’re doing retrieval. But that means that’s not an end-to-end system, despite what Jason will have you believe on his Twitter, he’s not created it, but it does make a lot of money from it. And it’s basically just the act of stitching together retrieval, so the R part of RAG and generation, so the G part of RAG. like to ground the later. [5:53] Ben: So you want your generation to be grounded to use some context. So you’re doing retrieval on the wire of documents you have and pass it to your LLM. But there’s no magic going on. It’s very much like a pipeline that take the output of model A and gives it to model B. The generation part is what’s handled by large language models and good rags and actually three different components. It’s your good retrieval pipeline. It’s a good generative model and it’s a good way of linking them up. So it can be formatting your prompt or whatever. [6:20] Ben: And it’s very important to think about it when you’re saying my rack doesn’t work. You need to be more specific like my rack doesn’t work is the same as saying my car doesn’t work. It’s like yeah, but something specific is broken. You need to figure out what is the retrieval part is the LLM struggling to make use of the context, etc. There’s a lot of failure cases there. And with that being said, let’s look at what the compact MVP is. [6:44] Ben: So that is basically what you will see, I think, if you’ve read any Medium blog post about the advent of Frag in early 2023. That’s the pipeline that everyone used. And that’s also because the easiest pipeline to put into production is very simple. You have a query. You have an embedding model. You have documents. The documents get embedded and pulled into a single vector. Then you do cosine similarity search between the vectors for your query and for the documents. And that gets you a result. That gets you a score. [7:10] Ben: And this is a bit of a teaser for an upcoming slide when I say this is called the Bayan Khodor approach, but just so you get the term in mind and I’ll define it because that’s one of those things that is like a scary term. That’s actually very, very simple when you break it down. But first, let’s look at what this actually means in code, this whole pipeline. So the first thing you want to do is load your model. [7:29] Ben: Then you get your data, you encode it, you store your vectors, and you get your query, you encode it. And then here we use NumPy, you do a cosine similarity search, eg a dot product between normalized vectors to get the most similar documents. And the documents whose embedding are similar to your query embedding is what you would consider as your relevant documents. And that’s pretty much it. Thanks. modified from something that Jeremy did to showcase how simple RAG actually is in his Hackers Guide to LLMs. [8:02] Ben: But that’s what you want to do to retrieve context in the simplest possible way. And you will have noticed that there’s no vector DB in this. This is all numpy arrays. And this is all numpy arrays because when you use vector DBs, the huge point of using a vector DB is to allow you to efficiently search through a lot of documents because what a vector DB does generally, not all of them, but most of them, wrap stuff like HNSW, IVFPQ, which are indexing types. [8:31] Ben: That allows you to do is to find and retrieve relevant documents without having to compute cosine similarity against every single document. It tries to do an approximate search of an exact search. This is not something that you need if you’re embedding 500 documents. Your CPU can do that in milliseconds. You don’t actually need a vector DB if you’re trying to go to the simplest possible stage. But if you wanted one, it would go right here on the graph, right after you embed your documents, you would put them in the vector DB. [9:00] Ben: And the second thing I think to discuss about is like this tiny graph is why am I calling embeddings by encoders? Because that step that I call by encoder, you will have seen a lot of times, but you will always see it generally called embeddings or model. And by encoder is the term that the IR literature uses to refer to that. And it’s simply because you encode things separately, like you do two encoding stages. So it’s a by encoding. And that’s used to create single vector representations where you pre-compute all your documentary presentations. [9:30] Ben: So when you’re using by encoders, you encode your documents whenever you want. Like when you’re creating your database, when you’re adding documents, those get encoded at a time that’s completely separate from intrants. And then only at intrants will you, like in the second aspect of this column, will you embed your query to compare to your pre-computed documentary presentations. So that’s really computationally efficient because at inference you’re only ever encoding one thing which is the query and everything else has been done before. And so that is part of why it’s done that quickly. [10:02] Ben: And I did want to take a slight break because I can see there are questions, but they’re not showing up on my screen. So if there are any on this quick MVP, then. [10:11] Participant 3: Yeah, let me look through some of the questions. I’m going to give you a few of them and you can decide whether you want to take them now or later. So we got one. It’s a 7,000 query, a 7,000 question and answer data set. Can I optimize RAG to accurately retrieve and quote exact answers? We’ll also effectively hand in queries that are slightly different from the original data. I think there’s actually two parts to that. So one is to quote the exact answer, which is something about the information retrieval part. [10:45] Participant 3: But it’s rather just like, what do you tell the LLM to do? But the information retrieval part is probably well. [10:56] Ben: I will actually cover how to better deal with out of context things in the upcoming slide. [11:04] Participant 3: Why do you keep going? [11:08] Ben: None of these questions can be saved right now. Perfect. The next one is if that’s very computationally efficient, there is an obvious trade-off here. And that is your documents are entirely unaware of your query and your queries are entirely unaware of your documents. [11:24] Ben: which means that you’re very very like subject to how it was trained is basically if your queries look a bit different from your training data or if like if there’s very very specific information that will be in certain documents and not other sometimes you want to know what how the query is phrased you want to know what the query is looking for when you’re encoding your document so that it can like kind of paint that representation and represent it more towards information that you’re interested in And that’s done with what we call re-ranking. [11:53] Ben: So re-ranking is another one of those scary stages that we’ll see in your pipeline. And the most common way to do re-ranking is using something that we call cross-encoder. And cross-encoder is another one of those scary words, like by encoder that you feel should be like a very advanced concept, but it’s actually very simple. This graph here represents the whole difference between them. The bi-encoder is basically this two column system that we described where documents get encoded in their corner, queries get encoded in their own corner, and they only meet very, very late. [12:20] Ben: Like you only do cosine similarity between vectors, but the documents never seen the query and vice versa. The cross-encoder is different. The cross-encoder is a model that will take your document and your query together. So you’re going to give it both your document or like a series of documents, depending on the type of model, but to keep it simple, we do it one by one. So you always give it a query document pair. And you put it through this cross-encoder model, which is effectively a classifier with a single label. [12:46] Ben: And the probability of the label being positive is what your model considers as how similar the documents are or how relevant it is. This is extremely powerful because it means that the model knows everything about what you’re looking for when it’s encoding the document. It can give you a very accurate score or at least a more accurate score. The problem is that you can see how that wouldn’t scale because it’s not very computationally realistic to compute this query document score for every single query document pair every time you want to retrieve a document. [13:15] Ben: Say you’ve got Wikipedia embedded, you’ve got, I don’t know, like 10 million paragraphs. You’re not going to compute 10 million scores. through a model for like using 300 million parameters for every single document for you. You would eventually return something and it would be a very, very relevant document, but it will also take 15 minutes, which is probably not what you want in production. [13:37] Ben: So you probably also have heard, or you might also have heard if you’re really into retrieval, or not heard at all if you’re not into retrieval of other re-ranking approaches like RankGPT or RankLLM using LLMs to rank documents has been a big thing lately. For people really into retrieval, you will know of MonoT5, et cetera. So those are not cross-encoders, but that’s not really relevant to us because the core idea is the same, and that’s basically what we always do with re-ranking in the pipeline. [14:04] Ben: you use a powerful model that is computationally expensive to score only a subset of your documents. And that’s why it’s re-ranking and not ranking, because this can only work if you give it like, I don’t know, 10, 50, not more than that document. So you always have a first stage retrieval, which here is our vector search. And then the re-ranker does the ranking for you, so it creates an ordered list. There’s a lot of ways to try those models out. [14:28] Ben: Some of them have an API base, so it’s just an API called to cohere or Jena. Some of them you run your machine. If you want to try them out, and this is basically the self-promotion moment, I do maintain at answer.ai library just called rerankers with the QR code here, where it’s basically a unified API so you can test any ranking method in your pipeline and swap them out freely. And that’s what your pipeline looks like now. [14:52] Ben: It’s the same with just that one extra step at the end where you re-rank things before getting your results. So we’ve added re-ranking, but there’s something else that’s missing here. And that’s something actually addresses the first question, at least partially, is that the semantic search via embeddings is powerful and I’m not saying don’t choose vectors. Vectors are cool, like models are cool, deep learning is cool. But it’s very, very hard if you think about it, because you’re asking your model to take, I don’t know, 512 tokens, even more if you’re doing long context. [15:24] Ben: And you’re like, okay, put all of this into this one vector. We are just using a single vector. You’ve got like, I don’t know, 384, 1024 at most floats, and that must represent all the information in this document. That’s naturally lossy. There’s no way you’re going to keep all of the information here. And what you do when you’re training on embedding is that you’re teaching the embedding to represent information that is useful in their training. [15:49] Ben: So the model doesn’t learn to represent all of the document’s information because that’s pretty much impossible since embeddings are essentially a form of compression. What the model actually learned is to replant the information that is useful to the training queries. So your training data is very, very important here. It’s like replanting the documents in a way that will help you use the queries in the way that phrase in your training data to retrieve a given document. [16:16] Ben: So when you use that on your own data, it’s likely that you’re going to be missing some information, or when you go slightly out of distribution. There’s another thing which is humans love to use keywords, especially if you’re going into the legal domain, the biomedical domain, anything specific. We have a lot of acronyms that might not even be in the training data, but we use a lot of acronyms. We use a lot of very advanced medical words. People love jargon. People love to use technical words because they’re very, very useful. [16:44] Ben: And that’s why you should, and I know it sounds like I’m talking from the 70s, because that’s actually a method from the 70s, but you should always have keyword search in your pipeline. You should always also have full text search on top of like anything that you do with vectors. And keyword search, which you can call full text search or like tfidifbm25, it’s powered by what we call tfidif. [17:06] Ben: which is a very basic NLP concept that essentially stands for term frequency, inverse document frequency, and it assigns every single word in a document or a group of words because sometimes we do them two by two, or three by three even. It gives them a weight based on how rare they are. So a word that appears everywhere like V or A has a very, very small weight and a word that’s highly specific to certain documents has a very high weight. [17:32] Ben: And the main method to use TF-IDF for retrieval is called BM25, which stands for Best Matching 25. It was invented in the 70s. It’s been updated since then, but it’s basically just been iterations of it. And you’ll often hear IR researchers say that the reason that the field’s not taken off like NLP has or Computer Vision has is because the baseline is just too good. We’re still competing with BM25, although it’s been 50 years now. Oh my god, it’s been 50 years. [18:00] Ben: Yeah, so the M25 existed for like, basically my entire lifetime before my birth, and it’s still used in production pipeline today. That’s how good it is. And the good thing is it’s just word counting with a match with like a waiting formula. So the compute time is virtually unnoticeable. Like you can add that to your pipeline, you will absolutely never fail it. [18:20] Ben: And I said I wouldn’t add anything from papers, but I feel like because I’m making a very strong claim that this method from 70 is strong, I should add a table and add the table from the bare paper, which is the retrieval part of MTEB, which is basically the main embeddings benchmark. And they compared it to a lot of models that were very popular for retrieval, like DPR and very strong vector retrievers. [18:45] Ben: And basically, you can see that unless you go into very over-trained embeddings like E5, BGE, BM25 is competitive with virtually all deep learning-based approaches, at least at the time of the paper, which was only just three years ago. We now have embeddings that are better, but we don’t have any embeddings that are better to the point where they’re not made better by being used in conjunction with BM25. [19:10] Ben: Knowing that this is how you want your pipeline to look, you’ll notice that there’s now a whole new pathway for both the query and the documents, who are on top of being encoded by the embedder, they’re also encoded by TF-IDF to get full text search, and that will help you retrieve keywords, etc. Humans use keywords in queries all the time, it’s something you should do. At the end, you will combine the scores. You can do that in a lot of ways. [19:33] Ben: I won’t go into too much details, but what a lot of people do is give a weight of 0.7 to the cosine similarity score and 0.3 to the full text hash. But I’m pretty sure we could do a whole talk for an hour on different methods of combining that. Okay. I do have five more minutes. [19:50] Ben: So the last one that you want to add to a simple pipeline, the thing that I think really completes your MVP plus plus is using metadata and using metadata filtering because academic benchmarks don’t because in academic benchmarks documents exist mostly in a vacuum like they don’t exist in the real world they’re not tied to a specific company etc when you’re using rag in production it’s very, very rare that someone comes to you and says, these documents came to me in a dream and caught them. Like they came from somewhere. They’ve been generated by a department. [20:21] Ben: They’ve been generated for a reason. They might be old Excel sheets or whatever, but they have business sense or they have in context sense. And the metadata is actually sometimes a lot more informative than the document content, especially in RAG contexts. So if you take the query here, which is, can you get me the Cruise Division financial report for Q422? There’s a lot of ways in which this can go wrong if you’re just looking at it from the semantic or even using keywords aspect. [20:52] Ben: When you say, when you see like this, the model must capture the financial report. So you, the model must figure out you want the financial report, but also cruise division Q4 and 2022 and embedding models are bad at numbers. So you might get a financial report, but maybe for another division, or maybe for the cruise division of 1998, it’s very hard to just hope that your vector will capture all of this. [21:15] Ben: But there’s another failure case, which will happen, especially with weaker LLMs, is that if you just have like top Ks and top five documents, and you retrieve the top five documents for your query. Even if your model is very good, if you just let it retrieve the top five documents, no matter what, you will end up with financial reports, at least five of them, and there’s most likely only one for Q4 22. [21:37] Ben: So at that point, you’re just passing all five to the model and being like, good luck, use the right one, which might confuse it, especially because tables can be hard, et cetera. And I’m not saying that your vector search will fail, but statistically it will. In most cases, it will fail. If it don’t fail for this query, it will fail for a similar one. But that’s actually very, very easy to mitigate. You just have to think outside of the vector and just use more traditional methods. You can use entity detection models. [22:07] Ben: One that’s very good for this is Gleaner, which is a very recent model that does basically zero-shot entity detection. You give it arbitrary entity types, so document type, time period, and the department. And this is like a live thing of Glenore. You can run the demo on the bottom, but here we just extract financial report, time period, and department. And when you generate your database for RAG, all you need to do is basically specify the time period. [22:32] Ben: So when you get an Excel sheet, you will just pass the name for it or pass the date in it and give metadata 2024 Q2, and Q4, sorry, 2022 Q2, the Q4. Okay, mixed up there. Then you just need to ensure that this is stored alongside your document. At query time, you can always pre-filter your document set to only query things that make sense. You will only query documents for this relevant time period. [22:56] Ben: You ensure that even if you give your model the wrong thing, it will at least be the right time frame so it can maybe try and make sense of it. And with this final component, this is what your pipeline looks like. You can see the new component here, which is metadata filtering, which doesn’t apply to queries. Queries go right through it. The documents get filtered by that, and we won’t perform search on documents that will not meet the metadata that we want. [23:20] Ben: And okay, I do agree that this looks a lot scarier than the friendly one at the start, which just had your embedder and then cosine similarity search and the results. It is actually not very scary. This is your full pipeline. This implements everything we’ve just talked about. It’s about 25 lines of code if you remove the commands. It does look a bit more unfriendly because there’s a lot more moving parts, I think. There’s a lot more steps, but if you want, we can just break it down a bit further. We use LensDB for this. [23:50] Ben: This is not necessarily an endorsement of LensDB as a vector DB, although I do like LensDB because it makes all of these components, which are very important, very easy to use. But I try not to take side in the vector DB wars because I’ve used WeaveYard, I’ve used Chroma, I’ve used LensDB, I’ve used Pencode, they all have their place. But I think LensDB, if you’re trying to build an MVP, is the one you should always use for MVPs right now because it has those components built in. [24:14] Ben: And here you can see just how easy it actually is. So we still load the By Encoder, just in a slightly different way, same as earlier. We define our document metadata. Here is just a string category, but it could be a timestamp, it could be just about anything. Then we encode a lot of documents just like we did previously. Here we’ve created, so it’s not an index, this is still a hard search, this is not an approximate search. Then we create a full text search index, which is generating those TF-IDF. [24:40] Ben: Why I mentioned before, we give a way to every single term in the documents. Then we load the reranker. Here we’re using the query ranker because it’s simple to use an API. And at the very end, you’ve just got your query and your search where we restrict it to the category equals films. So we will only ever search into the document that’s about a film, not about an author, not about a director. We get the top 10 results and we just have a quick ranking step. And that’s pretty much it. [25:06] Ben: We’ve taken the pipeline at the start, which only had the biancoder component to a pipeline that now has the biancoder component, metadata filtering, full text search, and a reranker at the end. So we’ve added like basically the four most important components of RetriVault into a single pipeline. And it really don’t take much more space in your code. And Yeah, that is pretty much the end of this talk. So there’s a lot more to cover in RAC. This is definitely not the full cover of RAC, but this is the most important thing. [25:36] Ben: This is what you need to know about how to make a good pipeline very quickly. All the other improvements are very, very valuable, but they have a decreasing cost effort ratio. This takes virtually no effort to put in place. Definitely worth learning about sparse methods, multi-vector methods, because they are very adapted to a lot of situations. Colbert, for instance, is very strong out of domain. Sparse is very strong in domain. [25:58] Ben: You should watch Jason’s talk about rack systems and Joe’s upcoming talk about retrieval evaluations because those are by a clear trifecta of the most important things. And yeah, any questions now? [26:12] Participant 3: Hamel and I were just messaging saying… We love this talk. Everything is presented so clearly. We’ve also got quite a few questions. [26:30] Hamel: My favorite talk so far. Not big favorites, but yeah. [26:36] Ben: Thank you. [26:37] Participant 3: Go ahead. [26:41] Hamel: Okay, questions. Did you have one that you were looking at already, Dan? I can tell it. [26:47] Participant 3: Yeah, we’ve got one that I quite like. Can the way that you fine-tune your bi-encoder model affect how you should approach fine-tuning for your cross-encoder and vice versa? [26:58] Ben: Yes. I don’t think I can give a really comprehensive answer because it will really depend on your domain, but you generally want them to be complementary. So if you’re in a situation where you’ve got the compute and the data to fine-tune both, you always want to… by encoder to be a bit more loose. Like you want it to retrieve potential candidates and then you want to trust your reranker, like your cross-encoder to actually do the filtering. [27:21] Ben: So if you’re going to use both and have full control over both, you might want to fine tune it in a way that will basically make sure that your top K candidates can be a bit more representative and trust the reranker. [27:35] Participant 3: Let me ask, this wasn’t an audience question, but a related question. You showed us where the, when you choose questions to feed into the re-ranker, that’s sort of a weighted average of what you get from the TF-IDF or BM-25 with what you get from the just simple vector search. What do you think of as the advantage or disadvantage of that over saying we’re going to take the top X from one cat from one of the rankers and the top X from the others? [28:14] Participant 3: And that way, if you think one of these is, for some questions, especially bad, you have a way of short-circuiting its influence on what gets sent to the re-ranker. [28:27] Ben: Yeah, I think that also makes complete sense. And that’s another, that’s a cop-out answer I use a lot, but that also depends a lot on your data. Like a lot of the time you want to look at what’s your actual context and how it’s actually being used. Because in some situations that actually works better, like especially if you work with biomedical data, because there’s so much like specific documents, it’s quite often the embedding won’t be that amazing on some questions. [28:52] Ben: So you just want to take the top five from both and get the re-ranker to do it, because the re-ranker is quite aware. So it’s a perfectly valid approach to combine them that way. [29:04] Participant 3: You want to pick a question, Hamel? [29:10] Hamel: Yeah, I’ve been looking through them. You guys have been… Okay, Jeremy’s asking, can we get a link to the code example? Yeah, sure. Your slides in Maven. We can also, can I share your slides in Discord as well, Ben? [29:25] Ben: Yes, please. [29:26] Hamel: Yeah. I’ll go ahead and share the slides in [29:28] Ben: Discord. And I’ll share the GitHub gist for the code examples I thought of. [29:34] Participant 3: And I’ll embed the link to the slides in Maven for people who want to talk some point deep into the future and might lose track of it in Discord. There’s a question somewhere in here I’ll find in a moment, but we got this question for Jason, the speed and then the speaker just before you, Paige Bailey said. RAG, you know, in the world of million token context lengths is not going to be as important. What’s your take on the relative importance of RAG in the future? [30:20] Ben: So I’m still very hopeful about RAG in the future. And I think I see it as some sort of like, so your LLM to me is like your CPU and your context window will be your RAM. And so like, even if you’ve got 32 gigs of RAM, nobody’s ever said, yeah, throw away your hard drive. You don’t need that. Like in a lot of contexts, you will still want to have like some sort of storage where you can retrieve the relevant documents. [30:42] Ben: Having to use a long context window is never going to be a silver bullet. Just like RAG is never a silver bullet. But I’m actually really happy because it just means I can retrieve much longer documents and get more efficient rack systems. Because to me, it’s a bit of a trade off where if you’ve got a longer context, it just means you’ve got a lot more freedom with how quick your retrieval system can be. Because if you need to use top 10 or top 15, that’s fine. You can fit them in. [31:06] Ben: Whereas when you can only fit the top three documents, you need your retrieval system to be really good, which might mean really slow. Yeah. [31:12] Participant 3: So, yeah. [31:13] Ben: So, yeah. [31:26] Participant 3: We had a question from Wade Gilliam. What are your thoughts on different chunking strategies? [31:36] Ben: I probably don’t think about chunking as much as I should. I am very hopeful for future avenues using LLMs to pre-chunk. I don’t think those work very well right now, but in my test I’ve never been impressed. Also, I do tend to use Colbert more often than Bancoders, and Colbert is a lot more resistant to chunking, so it’s something that I don’t care about as much. But generally I would try to… [32:01] Ben: So my go-to is always to chunk based on like around 300 tokens per chunk, and try to do it in a way where you never cut off a sentence in the middle, and always keep like the last 50 tokens and the next 50 tokens of the previous and next chunk. Because information overlap is very useful to give content, like please don’t be afraid to duplicate information in your chunks. [32:22] Hamel: I have a question about the buy encoder. Do you ever try to fine tune that using some kind of like label data to get that to be really good? Or do you usually kind of use that off the shelf and then use a re-ranker? And how do you usually go about it or how do you make the trade off? [32:43] Ben: So again, context dependent, but if you have data, you should always fine-tune all your encoders, be it the bi-encoder, the cross-encoder. I think Colbert, because it’s single vector, you can get away with not fine-tuning for a bit longer because it’s multi-vector, so you can get away with not fine-tuning for a bit longer. But if you have data, it’s all about like basically the resources you have. So in this talk, we’re doing an MVP, this is something you can put together in an afternoon. If your company says you have $500. [33:10] Ben: Spend 480 of that on OpenAI to generate synthetic questions and find your encoders that will always get you better results. Like always find your encoders if you can. And so, yes, so a couple of questions about fitting Colbert in and I’m using this entire executive decision to answer those. So Colbert in this pipeline, some people use it as a re-ranker, but then that’s not optimal. That’s very much when you don’t want to have to change your existing pipeline. [33:50] Ben: If you were to design a pipeline from scratch and wanted to use Colbert, you would have it instead of the BI encoder and it would perform basically the same role as the BI encoder, which is first-edge retrieval. And if you wanted to use Colbert, and especially if you don’t have the budget to fine-tune and need a re-ranking step, sometimes it can actually be better to use Colbert as a re-ranker still. Because the multi-vector approach can be better at capturing keywords, etc. But that’s very context-dependent. So ideally, you would have it as ShowByEncoder. [34:22] Participant 3: For a lot of people here who probably aren’t familiar with Colbert, Colbert, can you give the… Quick summary of it? [34:32] Ben: Yeah, sorry, I got carried away because I saw the question. So Colbert is an approach which is effectively a biancoder, but instead of cramming everything into a single vector, you represent each document as a bag of embeddings. So like, if you’ve got 100 tokens, instead of having one big 124 vector, you will have a lot of small 128 vectors, one for each token. And then you will score that at the end. You will do the same for the query. So if your query is 32 tokens, you will have 32 query token. [35:02] Ben: And for each query token, you will compare it to every token in the document and keep the highest score. And then you will sum up those highest scores and that will be the score for that given document. That’s called max similarity. And the reason that’s so powerful is not because it does very well on data it’s been trained on. You can beat it with a normal Bayer encoder, but it does very well at extrapolating to out of domain because you just give the model so much more room to replant each token in its context. [35:29] Ben: So it’s much easier if you’re in a non-familiar setting, you’ve not compressed as much information. And I do have self promotion. I do have a pretty cool Colbert thing coming out later this week to compress the Colbert space by reducing the tokens that actually needs to save by about 50 to 60% without losing any performance. So that’s a bit of a teaser, but look forward to the blog post if you’re interested. [35:57] Participant 3: And to find the blog post, you suggest people follow you on Twitter or? [36:02] Ben: Yeah, definitely follow me on Twitter. Because it was pretty much the only place where you can reliably reach me. [36:14] Hamel: Someone’s asking what are some good tools to fine tune embeddings for retrieval? Would you recommend Ragatouille or anything else? Like what’s your… [36:24] Ben: I’d recommend sentence transformers, especially with the 3.0 release recently. It’s now much, much funnier to use. It’s basically, there’s no need to reinvent the wheel. They’ve got all the basics implemented very well there, so sentence transformers. [36:44] Participant 3: Question from Divya. Can you give any pointers on how one fine-tunes their embedding model? [36:53] Ben: Sorry, can you repeat that? I could have said it. [36:55] Participant 3: Yeah. The question is, can you give any pointers or describe the flow for when you fine tune your embedding model? [37:04] Ben: Okay. So that’s probably a bit more involved than this talk, but essentially when you fine tune your embedding model, what you’ll want is queries. You need to have queries and you need your documents and you’re going to tell the model. For this given query, this document is relevant. And for this given query, this document is not relevant because sometimes there’s a triplet loss. And a triplet loss is what you will do when you have one positive document and one negative document. And you’ll kind of be teaching the model, this is useful, this is not useful. [37:32] Ben: And I’m not going to go down too much because this rabbit hole can take you quite far. But sometimes when you have triplets, you also want to use what we call hard negatives. which is you want to actually use retrieval to generate your negative examples because you want them to be quite close to what the positive example is, but not quite the right thing. Because that’s why you teach the model more, but was actually useful to answer your query. [37:57] Ben: So the workflow is probably, as always, look at your data, figure out what kind of queries your user will actually be doing. If you don’t have user queries. Go into production, write some, write some queries yourself and give that to an LLM, generate more queries and you can have a pretty solid ritual pipeline like that. [38:16] Hamel: Someone’s asking in the Discord, and I get this question all the time, is please share your thoughts on graph rag. [38:25] Ben: I have never actually done graph rag. I see this mentioned all the time, but it’s not something that has come up for me at all. So I don’t have strong thoughts about. I think it’s cool, but that’s pretty much the full extent of my knowledge. [38:49] Hamel: Someone’s asking, okay, when you have long context windows, does that allow you to do something different with RAG, like retrieve longer documents or do any other different kinds of strategies than you were able to before? Does it change anything? How you go about this? [39:09] Ben: Yeah, I think it’s a bit what I mentioned before. To me it changes two main things. One is I can use longer documents, which means I can use longer models, or I can stitch chunks together. Because sometimes if your retrieval model isn’t very good at retrieving long documents, which is often the case, you might just want, if I get a chunk from this document, give the model the full document. Like if I just get a chunk from it past the full context and you just hope the model is able to read it. [39:34] Ben: And if you’ve got a good long context model, it can. So it changes how you decide to feed the information into the model. And then the other aspect is, like I said, it changes the retrieval overhead because if you need to be very good, like I was saying, if you need only the top three documents to be relevant, you’re going to spend a lot of time and money on retrieval pipeline. If you’re like, oh, as long as my recall at 10 or my recall at 15 is good, that’s fine. [39:56] Ben: You can afford to have much lighter models and spend a lot less time and resources on retrieval. There’s a lot of diminishing returns in retrieval when getting a good recall at 10. So recall at 10 is how likely you are to retrieve the relevant document in the first 10 results. is generally very easy. Recall at 100 is very, very easy. And then recall at 5 is getting harder. And recall at 3 and recall at 1 are like the really tough ones because a lot of the training data is noisy. [40:23] Ben: So it can even be hard to know what a good recall at 1 is. So longer context makes that irrelevant. And that’s why it’s great for RUG. [40:49] Hamel: Someone’s asking, and I don’t even know what this means, what’s your view on PIDE versus React versus StepBack? [40:58] Ben: I’ve only used React out of those. And so those are like adjunct systems of function coding. It’s like to give your LLM the ability to call tools, at least React is. I don’t have strong thoughts on those in the context of retrieval, so I can’t really answer the question. Yeah, I think. I would occasionally use React from the model to be able to trigger a search itself, but I think that’s still an open area of research. And I think Griffin from AnswerIA is also in the chat and he’s very interested in that. [41:31] Ben: It’s basically how do you get a model to tell you that it doesn’t know? Because sometimes you don’t need retrieval, the model already knows. Sometimes you do need retrieval, but that’s still a very open question. Like how do you decide when to search? So no strong thoughts there yet. [41:50] Participant 3: You may or may not have a good answer for this one. Is there an end-to-end project, open-source project, that someone could look at as a way to see or evaluate the difference in result quality when they do result from just buying code or MVP and compare that to the final compact MVP++ that you showed? [42:14] Ben: No, actually, that’s a very good point. I don’t think there is one that systematically goes through every step. And that’s probably something that I would like to build at some point or find one because just like most things in retrieval, everything is kind of conventional wisdom. Like you’ve seen it piece and pieces in a lot of projects and you just know that that’s how it is. But unless you dig deep into the papers or like do it yourself, it’s quite rare to find very good resources showing that. [42:54] Participant 3: A related question, do you have a tutorial that you typically point people to on fine-tuning their encoder? [43:08] Ben: That would be the sentence transformers documentation, but it’s not the friendliest tutorial, so that’s a half answer. That’s what I would point you to, but it’s still a bit hard to get into, sadly. [43:40] Hamel: Wade is asking if you have go-to embedding models. [43:48] Ben: I think my go-to these days when I’m demoing something is the Korea one, because it’s nice to be able to work with an API. It works really well. It’s cheap. But other than that, I would just call bear if I’m using something in my own pipeline. I would use multi-vectors. But it really depends on the use case, because you would often find that some things work well for you and some things don’t. I do have strong opinions on not using… [44:16] Ben: So if you go to the MTB leaderboard, which is the embedding leaderboard right now, you’ll see a lot of LLMs as encoders. And I would advise against that because the latency is not worth it. Don’t need 7 billion parameters to encode stuff. And at least some of the early ones actually generalized worse, like Neil Schremer from Cohere had a really interesting table where the E5 mistral was worth an E5 large, despite being seven times as big. [44:47] Ben: So probably just stick to the small ones between 100 and at most a billion parameters, but that would be my only advice about that. Try all the good ones like GT, BG, E5. [45:00] Hamel: Chris Levy is asking… this question about Elasticsearch, which I also get quite a lot. So he asks, Anyone here have experience building RAG application with just keyword BM25 as a retriever at work? It makes use of Elasticsearch. And he said it’s all over the tech stack that people are already using Elasticsearch. Is there basically he’s asking, is there a way to keep using Elasticsearch with RAG that you know about or that you have encountered? Or do you mainly use like vector database like LanceDB and things like that? [45:32] Hamel: Have you tried seeing people using Elasticsearch and trying to bootstrap off of that? [45:37] Ben: Yeah, I’ve used Elasticsearch a bit and it’s perfectly possible. You do lose obviously the semantic search aspect, although I think now Elasticsearch has a vector DB offering, so you could add vectors to it. You could always plug in, you could always just do BM25 and then plug in a re-ranker at the end. That’s often, if you read papers on like cross encoders, generally the way they evaluate them is actually doing just that, like do BM25 to retrieve 50 to 100 documents and then rank them using the re-ranker. [46:07] Ben: which if you can afford to just set up your re-ranking pipeline or call the Core API is a really good way to go about it because you don’t need to embed your whole documents to sample how good it would be with deep learning because there are domains where you do not need deep learning, BM25 is still good enough in some bits and you know like I think it’s become very apparent like BM25 has never told anyone they should eat three rocks a day whereas embeddings have so [46:35] Hamel: Dimitri is asking, is it worthwhile to weigh the BM25 similarity score during the re-ranking step as well? [46:45] Ben: Probably not. You generally just want to use BM25 to retrieve candidates, but you don’t need to give those scores to your cross-encoder. [46:59] Participant 3: There’s a question. I’m going to change it slightly. Someone asks about retrieving from many documents rather than finding the best one. Maybe the tweak there is if you have a theory that information within any single document is so correlated that you actually want to try and get some diversity, are you familiar with or have you used approaches where you I specifically try in some loss function somewhere, encourage that diversity and encourage pulling from many documents rather than from one. [47:37] Ben: I have not done that myself. I know that there’s different loss methods to optimize for diversity versus clear accuracy. But I don’t think I would be able to give you a clear answer without sounding really confident about something I don’t know much about. [47:59] Participant 3: Have you used hierarchical reg? Any thoughts on it? [48:03] Ben: I have not, and I don’t think it’s very needed for the current pipelines. I think there’s a lot of other steps you can improve on. [48:18] Participant 3: Since I think we have several Answer AI people here, I don’t know if this is a question or a request, I’m eager to learn if Answer AI will come up with any books on LLM applications in the future. [48:33] Ben: I don’t think so, but never say never. Jeremy, if you want to chime in. Because I can’t make any promises because my boss is watching. [49:00] Participant 3: You see anything else to Ben, did you say that you can’t see the questions? [49:05] Ben: Yeah, they’re all blank for me. I saw one earlier, but they really show up sporadically. [49:10] Participant 3: Yeah. Not sure what’s happened with her. And I think people also cannot upvote these. So a couple of quirks today. You see any others here, Emil, that you think we should pull in? [49:25] Hamel: um no not necessarily i think like probably going to the discord is pretty good now yep tons of activity there as well um I mean, there’s infinite number of questions, so we can keep going. Okay, Lorien is asking, what’s the best strategy when chunks, when the documents, when chunks of documents don’t fit into the context window? Do you do RAG in a MapReduce style, summarize aggressively? What are the techniques that you’ve seen work most effectively? [50:25] Ben: So that’s, I think, a very broad question, because it’s like, why do they not fit? Is it because like every document is really long? Is it because you need a lot of different documents, etc, etc? So. And also another important aspect is what’s the latency tolerance? Because quite a lot of the time you can make RAG infinitely better, but users won’t stay waiting like 20 seconds for an answer. So you need to figure out like, how much time do I have? [50:52] Ben: One way that you can often see what I’ve done in production actually is retrieve the full documents but have another database that maps every document to its summary. So you will have done your LLM summarization at the previous step. You will retrieve the relevant chucks, and then you will pass the relevant summaries to the context window. But that kind of depends on your actual setting. I have another call at 10, which is in five minutes for me. So if you’ve got another final question. [51:35] Hamel: I really enjoyed this presentation. [51:39] Ben: Thank you. [51:42] Participant 3: Yeah, this was really great. Everything is super clear and well presented, so thanks so much. [51:54] Ben: Thank you. Cheers. [51:59] Participant 3: Thanks, everyone."
  },
  {
    "objectID": "talks/applications/simon_llm_cli/index.html#chapters",
    "href": "talks/applications/simon_llm_cli/index.html#chapters",
    "title": "LLMs on the command line",
    "section": "Chapters",
    "text": "Chapters\n00:00 Introduction\nSimon Willison introduces LLM - a command line tool for interacting with large language models.\n01:40 Installing and Using LLM\nSimon demonstrates how to install LLM using pip or homebrew and run prompts against OpenAI’s API. He showcases features like continuing conversations and changing default models.\n10:30 LLM Plugins\nThe LLM tool has a plugin system that allows access to various remote APIs and local models. Simon installs the Claude plugin and discusses why he considers Claude models his current favorites.\n13:14 Local Models with LLM\nSimon explores running local language models using plugins for tools like GPT4All and llama.cpp. He demonstrates the llmchat command for efficient interaction with local models.\n26:16 Writing Bash Scripts with LLM\nA practical example of creating a script to summarize Hacker News threads.\n35:01 Piping and Automating with LLM\nBy piping commands and outputs, Simon shows how to automate tasks like summarizing Hacker News threads or generating Bash commands using LLM and custom scripts.\n37:08 Web Scraping and LLM\nSimon introduces ShotScraper, a tool for browser automation and web scraping. He demonstrates how to pipe scraped data into LLM for retrieval augmented generation (RAG).\n41:13 Embeddings with LLM\nLLM has built-in support for embeddings through various plugins. Simon calculates embeddings for his blog content and performs semantic searches, showcasing how to build RAG workflows using LLM."
  },
  {
    "objectID": "talks/applications/simon_llm_cli/index.html#notes",
    "href": "talks/applications/simon_llm_cli/index.html#notes",
    "title": "LLMs on the command line",
    "section": "Notes",
    "text": "Notes\n\n\n\n\n\n\nNote\n\n\n\nThese notes were originally published by Simon Willison here\n\n\nNotes for a talk I gave at Mastering LLMs: A Conference For Developers & Data Scientists.\n\nLinks\n\nDatasette\nMy blog\nLLM\n\n\n\nGetting started\nbrew install llm # or pipx or pip\nllm keys set openai\n# paste key here\nllm \"Say hello in Spanish\"\n\n\nInstalling Claude 3\nllm install llm-claude-3\nllm keys set claude\n# Paste key here\nllm -m haiku 'Say hello from Claude Haiku'\n\n\nLocal model with llm-gpt4all\nllm install llm-gpt4all\nllm models\nllm chat -m mistral-7b-instruct-v0\n\n\nBrowsing logs with Datasette\nhttps://datasette.io/\npipx install datasette # or brew or pip\ndatasette \"$(llm logs path)\"\n# Browse at http://127.0.0.1:8001/\n\nTemplates\nllm --system 'You are a sentient cheesecake' -m gpt-4o --save cheesecake\nNow you can chat with a cheesecake:\nllm chat -t cheesecake\nMore plugins: https://llm.datasette.io/en/stable/plugins/directory.html\n\n\nllm-cmd\nHelp with shell commands. Blog entry is here: https://simonwillison.net/2024/Mar/26/llm-cmd/\n\n\nfiles-to-prompt and shot-scraper\nfiles-to-prompt is described here: https://simonwillison.net/2024/Apr/8/files-to-prompt/\nshot-scraper javascript documentation: https://shot-scraper.datasette.io/en/stable/javascript.html\nJSON output for Google search results:\nshot-scraper javascript 'https://www.google.com/search?q=nytimes+slop' '\nArray.from(\n  document.querySelectorAll(\"h3\"),\n  el =&gt; ({href: el.parentNode.href, title: el.innerText})\n)'\nThis version gets the HTML that includes the snippet summaries, then pipes it to LLM to answer a question:\nshot-scraper javascript 'https://www.google.com/search?q=nytimes+slop' '\n() =&gt; {\n    function findParentWithHveid(element) {\n        while (element && !element.hasAttribute(\"data-hveid\")) {\n            element = element.parentElement;\n        }\n        return element;\n    }\n    return Array.from(\n        document.querySelectorAll(\"h3\"),\n        el =&gt; findParentWithHveid(el).innerText\n    );\n}' | llm -s 'describe slop'\n\n\n\nHacker news summary\nhttps://til.simonwillison.net/llms/claude-hacker-news-themes describes my Hacker News summary script in detail.\n\n\nEmbeddings\nFull documentation: https://llm.datasette.io/en/stable/embeddings/index.html\nI ran this:\ncurl -O https://datasette.simonwillison.net/simonwillisonblog.db\nllm embed-multi links \\\n  -d simonwillisonblog.db \\\n  --sql 'select id, link_url, link_title, commentary from blog_blogmark' \\\n  -m 3-small --store\nThen looked for items most similar to a string like this:\nllm similar links \\\n  -d simonwillisonblog.db \\\n  -c 'things that make me angry'\n\n\nMore links\n\nCoping strategies for the serial project hoarder talk about personal productivity on different projects\nFigure out how to serve an AWS Lambda function with a Function URL from a custom subdomain as an example of how I use GitHub Issues"
  },
  {
    "objectID": "talks/applications/simon_llm_cli/index.html#full-transcript",
    "href": "talks/applications/simon_llm_cli/index.html#full-transcript",
    "title": "LLMs on the command line",
    "section": "Full Transcript",
    "text": "Full Transcript\n\n\n\n\n\n\nExpand to see transcript\n\n\n\n\n\n[0:00] Simon Willison: Hey, hey everyone, it’s great to be here. So yeah, the talk today, it’s about command line tools and large language models. And effectively the argument I want to make is that the Unix command line dating back probably 50 years now is it turns out the perfect environment to play around with this new cutting edge technology, because the Unix philosophy has always been about tools that output things that get piped into other tools as input. And that’s really what a language model is, right? [0:27] Simon Willison: An LLM is a, it’s effectively a function that you pipe a prompt to, and then you get a response back out, or you pipe a big chunk of context to, and you get a response that you can do things with. So I realized this last year and also realized that nobody had grabbed the namespace on PyPI, the Python Packaging Index, for the term LLM. So I leapt at that. I was like, okay, this is an opportunity to grab a really cool name for something. And I built this… [0:54] Simon Willison: LittleTool, which originally was just a command line tool for talking to OpenAI. So you could be in your terminal and you could type LLM, say hi in French, and that would fire it through the OpenAI API and get back response and print it to your terminal. That was all it did. And then over time, as other model providers became interesting, and as local models emerged that you could run on your computer, I realized there was an opportunity to have this tool do way more than that. [1:24] Simon Willison: So I started adding plugin support to it so you can install plugins that give you access to flawed and local mistral and all sorts of other models. There’s hundreds of models that you can access through this tool now. I’ll dive into that in a little bit more detail in a moment. First thing you need to know is how to install it. If you are Python people, that’s great. Pip install LLM works. I recommend using pip x to install it because then the dependencies end up packaged away somewhere nice. Or you can install it using homebrew. [1:54] Simon Willison: I think the one I’ve got here, yeah, this one I installed with brew install LLM. I made the mistake of running that command about half an hour ago and of course it’s homebrew so it took half an hour to install everything. So Treat with caution. But it works. And once it’s installed, you have the command. And so when you start using LLM, the default is for it to talk to OpenAI. And of course, you need an OpenAI API key for that. So I’m going to grab my API key. There’s a command you can run. [2:25] Simon Willison: LLM secrets. Is it LLM secrets? [2:30] Hugo Bowne-Anderson: Yes. [2:32] Simon Willison: Yes. No, it’s not. It’s. What is it? LLM keys. That’s it. LLM. So I can type LLM keys, set OpenAI, and then I paste in the key, and I’m done. And having done this, I’ve actually got all sorts of keys in here, but my OpenAI API key has now been made available. And that means I can run prompts. Five great names for a pet pelican. This is my favorite test prompt. And it gives me five great names for a pet pelican. So that’s fun. And that’s running over the API. [3:05] Simon Willison: And because it’s a Unix command line thing, you can do stuff with the app. So you can do things like write that to pelicans.txt. The greater than sign means take the output and run it to a file. Now I’ve got a nice permanent pelicans.txt file with my five distinctive names for a pet pelican. Another thing you can do is you can continue. If you say dash C. which stands for continue, I can say now do walruses. And it will continue that same conversation, say here are five fitting names for a pet walrus. [3:35] Simon Willison: I’m going to say justify those. Oops. And now it says why each of these names are justified for that walrus. That’s super, super, super delightful. [3:48] Hugo Bowne-Anderson: I like Gustav. [3:50] Simon Willison: Gustav, what was, let’s do LLM logs dash. Gustav, a touch of personality and grandeur, strong regal name that suits the impressive size and stature of a walrus, evoking a sense of dignity. It’s good. The justifications are quite good. This is GPT-4-0 I’m running here. You can actually say LLM models default to see what the default model is, and then you can change that as well. So if I want to… be a bit cheaper, I can set it to chat GPT. [4:24] Simon Willison: And now when I do this, oops, these are the GPT 3.5 names, which are slightly less exciting. So that’s all good and fun. But there are a whole bunch of other useful things you can do when you start working with these things in the terminal. So I’m going to… Let’s grab another model. So LLM, as I mentioned, has plugins. If you go to the LLM website, look at list of plugins, there is a plugin directory. This is all of the plugins that are currently available for the tool. And most of these are remote APIs. [5:06] Simon Willison: These are plugins for talking to Claude or Rekha or Perplexity or any scale endpoints, all of these different providers. And then there are also some local model plugins that we’ll jump into in a moment. But let’s grab Claude 3. Claude 3 is my current favorite, my favorite family of models. So I can say LLM install LLM Claude 3, and it will go ahead and install that plugin. If I type LLM plugins, it shows me the plugins that it has installed. Oh, I didn’t mean to install LLM Claude, but never mind. And if I see LLM… [5:39] Hugo Bowne-Anderson: Is Claude 3 currently your favorite? Just out of interest? [5:41] Simon Willison: Two reasons. One, Claude 3 Haiku is incredible, right? Claude 3 Haiku is cheaper than GPT 3.5. I think it’s the cheapest decent model out there. It’s better than 3.5. It has the 100,000 token limit. So you can dump a huge amount of stuff on there. And it can do images. So we’ve got, I think it’s the most exciting model that we have right now if you’re actually building because for the price, you get an enormous array of capabilities. And then Opus, I think Opus was better than GPT-4 Turbo. [6:15] Simon Willison: I think 4.0 is just about caught up for the kind of stuff that I do. But Opus is still like a really interesting model. The other thing I’ll say about Claude is the… There’s this amazing article that just came up about Claude’s personality, which talks about how they gave Claude a personality. And this is one of the most interesting essays I have read about large language models in months. Like, what they did to get Claude to behave the way it does and the way they thought about it is super fascinating. [6:45] Simon Willison: But anyway, so I’ve installed Claude. I’ve now got Claude 3 Opus, Claude 3 Sonnet and Claude 3 Haiku. LLM gives everything long names, so you can say that, say hi in Spanish, and it’ll say hola. If you say it with a flourish, it’ll add an emoji. That’s cute. Hola mi amigo. But you can also say lm-m and just the word haiku because that’s set up as a shorter alias. So if I do that, I’ll get the exact same response. Crucially, you can… [7:22] Simon Willison: This is how I spend most of my time when I’m messing around with models. install plugins for them, or often I’ll write a new plugin because it doesn’t take much effort to write new plugins for this tool. And then I can start mucking around with them in the terminal, trying out different things against them. Crucially, one of the key features of this tool is that it logs absolutely everything that you do with it. It logs that to a SQLite database. [7:47] Simon Willison: So if I type lmat logs path, it will show me the path to the SQLite database that it’s using. And I absolutely adore SQLite databases, partly because my main project, the thing I spend most of my time building, is this thing called Dataset, which is a tool for exploring data in SQLite databases. So I can actually do this. I can say Dataset that, if you put it in double quotes, it makes up that space in the file name. This is taking, this is a good command line trick. [8:18] Simon Willison: It’s taking the path to the log database and passing it to dataset. And now I’ve got a web interface where I can start browsing all of my conversations. So let’s have a look at responses. We sort by ID. Here we go. Say hi in Spanish with a flourish. Hola mi amigo. There it is. It stores the options that we used. It stores the full JSON that came back. It also organizes these things into conversation threads. So earlier we started with, we built, we had that conversation that started five great names for a pet pelican. [8:52] Simon Willison: We replied to it twice and each of those messages was logged under that same conversation idea. So we started five great names for pet pelican. Now do walruses justify those. As a tool for evaluating language models, this is incredibly powerful because I’ve got every experiment I’ve ever run through this tool. I’ve got two and a half thousand responses that I’ve run all in one place, and I can use SQL to analyze those in different ways. [9:18] Simon Willison: If I facet by them, I can see that I’ve spent the most time talking to GPT 3.5 Turbo, Cloud 3 Opus. I’ve done 334 prompts through. I’ve got all of these other ones, Gemini, Gemini Pro. Orca Mini, all of these different things that I’ve been messing around with. And I can actually search these as well. If I search for pelican, these are the six times that I’ve talked to Claude 3 Opus about pelicans. And it’s mostly… Oh, interesting. [9:46] Simon Willison: It’s mostly asking names from Pelicun, but I did at one point ask it to describe an image that was a close-up view of a Pelicun wearing a colorful party hat. The image features aren’t in the main shipped version of the software yet. That’s a feature I need to release quite soon. Basically, it lets you say LLM. like a dash I and then give it the path to an image and that will be sent as part of the prompt. So that’s kind of fun. [10:10] Simon Willison: But yeah, so if you want to be meticulous in tracking your experiments, I think this is a really great way to do that. Like having this database where I can run queries against everything I’ve ever done with them. I can try and compare the different models and the responses they gave to different prompts. That’s super, super useful. Let’s talk a little bit more about plugins. So I mentioned that we’ve got those plugins that add additional models. We also have plugins that add extra features. [10:45] Simon Willison: My favorite of those is this plugin called LLM-CMD, which basically lets you do this. If I say in the LLM-CMD, that’s installing the plugin. That gives me a new command, llmcmd. That command wasn’t there earlier. I can now say llmcmd-help, and it will tell me that this will generate and execute commands in your shell. So as an example, let’s do llm convert the file demos.md to uppercase and spit out the results. Oh, I forgot. So llmcmd. And what it does is it passes that up to, I think, GPT-4.0 is my default model right now. [11:36] Simon Willison: Gets back the command that does this thing. Why is this taking so long? Hang on. Models. Default. Let’s set that to GPT-4.0. Maybe GPT-3.5 isn’t very good at that. Anyway, when this works, it populates my shell with the command that I’m trying to run. And when I hit enter, it will run that command. But crucially, it doesn’t just run the command, because that’s a recipe for complete disaster. It lets you review that command before it goes. And I don’t know why this isn’t working. This is the one live demo I didn’t test beforehand. [12:21] Simon Willison: I will drop, I will make notes available afterwards as well. But this is a right, here we go. Here’s an animated GIF showing me, showing you exactly what happens. This is show the first three lines, show the first three lines of every file in this directory. And it’s spats out head dash N three star. And that does exactly the job. [12:41] Simon Willison: A fun thing about this is that because it’s a command here, it actually, And tab completion works as well, so you can give it file names by tab completing, and when the command is babing itself, that will do the right thing. But that’s kind of fun. It’s kind of neat to be able to build additional features into the tool, which use all of the other features of LLM, so it’ll log things to SQLite and it’ll give you access to all of those different models. [13:07] Simon Willison: But it’s a really fun playground for sort of expanding out the kind of command line features that we might want to use. Let’s talk about local models. So local models that run on your laptop are getting shockingly effective these days. And there are a bunch of LLM plugins that let you run those. One of my favorites of those is called LLM GPT-4-all. It’s a wrapper around the GPT-4-all library that Nomic put out. [13:35] Simon Willison: So Nomic have a desktop application that can run models that is accompanied by a Python library that can run models, which is very neatly designed. And so what I can do is I can install that plugin, LLM install. LLM GPT for all. This will go around fetch that. And now when I run the LLM models command, we’ve got a whole bunch of additional models. This is all of these GPT-4 ones. And you’ll note that some of these say installed. That’s because I previously installed them and they’re sat on my hard drive somewhere. [14:08] Simon Willison: I’m not going to install any new models right now because I don’t want to suck down a four gigabyte file while I’m on a Zoom call. But quite a few of these are installed, including Mistral 7b instruct. So I can grab that and I can say, lm-m Mistral, let’s do. Five great names for a pet seagull. Explanations. And I’m going to fire activity monitor for this right now. Let’s see if we can spot it doing its thing. Right now we’ve just asked a command line tool to load in a 4 gigabyte model file. [14:42] Simon Willison: There we go. It’s loading it in. It’s at 64.3 megabytes. 235 megabytes. It’s spitting out the answer. And then it goes away again. So this was actually a little bit wasteful, right? We just ran a command which loaded four gigabits of memory, of file into memory, and I think onto the GPU, ran a prompt for it, and then threw all of that away again. And it works. You know, we got our responses. And this here ran entirely on my laptop. There was no internet connection needed for this to work. [15:13] Simon Willison: But it’s a little bit annoying to have to load the model each time. So I have another command I wrote, a command I added, llm chat. And llm chat, you can feed it the ID of a model, llm chat dash m mistral 7b. And now it’s giving me a little… [15:33] Simon Willison: chat interface so I can say say hello in Spanish the first time I run this it will load the model again and now now in French and So this is if you’re working with local models This is a better way to do it because you don’t have to pay the cost of loading that model into memory every single time there’s also You can stick in pipe multi and now you can copy and paste a whole block of code into here and it translates And then you type exclamation mark end at the end. [16:08] Simon Willison: And if we’re lucky, this will now give me… Huh. Okay. Well, that didn’t. I may have hit the… I wonder if I’ve hit the context length. Yeah, something went a little bit wrong with that bit. But yeah, being able to hold things in memory is obviously really useful. There are better ways to do this. One of the best ways to do this is using the O-Lama tool, which I imagine some people here have played with already. And O-Lama is an absolutely fantastic tool. [16:50] Simon Willison: It’s a Mac, Linux, and Windows app that you can download that lets you start running local language models. And they do a much better job than I do of curating their collection of models. They have a whole team of people who are making sure that newly available models were available in that tool and work as effectively as possible. But you can use that with LLM as well. If I do LLM install LLM-o-Lama, actually… That will give me a new plugin called LLM-OLAMA. And now I can type LLM models. [17:22] Simon Willison: And now, this time, it’s giving me the OLAMA models that I have available in my machine as well. So in this case, we can do Mixtral. Let’s do LLM-M, LLM-CHAT, LLM-M Mixtral Latest. I’ll write it in Spanish. And this is now running against the Ollama server that’s running on my machine, which I think might be loading Mixtral into memory at the moment, the first time I’m calling it. Once it’s loaded into memory, it should work for following prompts without any additional overhead. Again, I spend a lot of time in an activity monitor these days. [18:00] Simon Willison: There we go. Ollama Runner has four gigabytes in residence, so you’d expect that to be doing the right thing. [18:07] Hugo Bowne-Anderson: So Simon, I just have a quick question. I love how you can use Ollama directly from LLM. I do think one of the other value props about Ollama is the ecosystem of tools built around it. Like you go to Olamas GitHub and there are all types of front ends you can use. And so I love using your LLM client, for example, with like a quick and dirty Gradio app or something like that. But I’m wondering, are there any front ends you recommend or any plugins or anything in the ecosystem to work with? [18:38] Simon Willison: I’ve not explored the Olam ecosystem in much detail. I tend to do everything on the command line and I’m perfectly happy there. But one of the features I most want to add to LLM as a plugin is a web UI. So you can type LLM web, hit enter, it starts a web server for you and gives you a slightly more… slightly more modern interface for messing around with things. And I’ve got a demo that relates to that that I’ll show in a moment. [19:04] Simon Willison: But yeah, so front-end, and actually the other front-end I spend a little bit of time with is LM Studio, which is very nice. That’s a very polished GUI front-end for working with models. There’s a lot of… [19:19] Hugo Bowne-Anderson: It’s quite around with getting two LLMs answering your same question. But there’s a mode where you can… [19:24] Simon Willison: get two or n llms if you have enough processing power to answer the same questions and compare their responses in real time yeah it’s a new feature very cool that is cool i’ve been me i’ve been planning a plugin that will let you do that with llms like llm multi dash m llama dash m something and then give it a prompt um but one of the many ideas on the on the on the backlog at the moment would be super super useful um the other one of course that people should know that if they don’t [19:53] Simon Willison: is llama file I’m going to demonstrate that right now. [19:57] Hugo Bowne-Anderson: Sorry, I’m going to shush now. [20:00] Simon Willison: This is one of the most bizarrely brilliant ways of running language models is there’s this project that’s sponsored by Mozilla called Llama File. And Llama File effectively lets you download a single file, like a single binary that’s like four gigabytes or whatever. And then that file will run, it comes with both the language model and the software that you need to run the language model. And it’s bundled together in a single file and one binary works on Windows, Mac OS, Linux and BST, which is ridiculous. What this thing does is technically impossible. [20:38] Simon Willison: You cannot have a single binary that works unmodified across multiple operating systems. But LamaFile does exactly that. It’s using a technology called Cosmopolitan. which I’ve got here we go I’ve got an article about Cosmopolitan when I dug into it just a while ago to try and figure out how this thing works astonishing project by Justin Tunney But anyway, the great thing about LamaFile is, firstly, you can just download a file and you’ve got everything that you need. And because it’s self-contained, I’m using this as my end-of-the-world backup of human knowledge. [21:16] Simon Willison: I’ve got a hard drive here, which has a bunch of LamaFiles on. If the world ends, provided I can still run any laptop and plug this hard drive in, I will have a GPT 3.5 class. language model that I can just start using. And so the one that I’ve got on there at the moment, I have, I’ve actually got a whole bunch of them, but this is the big one. I’ve got Lama 370B, which is by far, I mean, how big is that thing? That’s a 37 gigabyte file. It’s the four byte quantized version. [21:55] Simon Willison: That’s a genuinely a really, really good model. I actually started this running earlier because it takes about two minutes to load it from over USB-C from drive into memory. So this right here is that. And all I had to do was download that file, shimod7558, and then do.slash metal armor 370B and hit enter. And that’s it. And that then fires up this. In this case, it fires up a web server which loads the model and then starts running on port. Which port is it? [22:32] Simon Willison: So now if I go to localhost 8080, this right here is the default web interface for Llama. It’s all based on Llama.cpp but compiled in a special way. And so I can say, turn names for a pet pelican and hit go. And this will start firing back tokens. Oh, I spelt pelican wrong, which probably won’t matter. [22:58] Hugo Bowne-Anderson: And while this is executing, maybe I’ll also add for those who want to spin up LamaFile now you can do so as Simon has just done. One of the first models in their README they suggest playing around with, which is 4 gigs or 4.7 gigs or something, is the Lava model, which is a really cool multimodal model that you can play around with locally from one file immediately, which is actually mind blowing if you think about it for a second. [23:22] Simon Willison: It really is. Yeah. Do I have that one? Let’s see. Let’s grab. [23:30] Hugo Bowne-Anderson: With your end of the world scenario, now you’ve got me thinking about like post-apocalyptic movies where people have like old LLMs that they use to navigate the new world. [23:40] Simon Willison: Completely. Yeah, that’s exactly how I. [23:43] Hugo Bowne-Anderson: Preppers with LLMs. [23:47] Simon Willison: I’m going to. Wow. Wow. Yeah, I very much broke that one, didn’t I? [23:58] Hugo Bowne-Anderson: We’ve got a couple of questions that may be relevant as we move on. One from Alex Lee is how does LLM compare with tools like Ollama for local models? So I just want to broaden this question. I think it’s a great question. It’s the Python challenge, right? The ecosystem challenge. When somebody wants to start with a tool like this, how do they choose between the plethora? How would you encourage people to make decisions? [24:21] Simon Willison: I would say LLMs… LLM’s unique selling point is the fact that it’s scriptable and it’s a command line tool. You can script it on the command line and it can run access both the local models and the remote models. Like, that I feel is super useful. So you can try something out against a hosted like cloud-free haiku and then you can run the exact same thing against a local Lama 3 or whatever. And that, I think, is the reason to pay attention to that. I just… [24:52] Hugo Bowne-Anderson: I also, I do want to build on that. I’d say if like I’ve been using dataset for some time now, and I love local SQLite databases as well. So the integration of those three with all the dataset plugins as well, make it really, really interesting also. So I think that’s a huge selling point. [25:07] Simon Willison: So what I’ve done here, I closed Llama 370B and I have switched over to that. I switched over to that Lava demo. And if we’re lucky, look at this. Person features a person sitting in a chair with a rooster nearby. She’s a chicken, not a rooster. A white ball filled with eggs. This I think is astoundingly good for a four gigabyte file. This is a four gigabyte file. It can describe images. This is remarkably cool. And then from LLM’s point of view, if I saw LLM Lava file. And then run LM models. [25:50] Simon Willison: I’ve now got a model in here which is Lama file. So I can say LM dash Lama file. Describe chickens. This doesn’t yet. Ooh, what happened there? Error file not found. Not sure what’s going on there. I’ll dig into that one a little bit later. [26:11] Simon Willison: the joys of live demos but yeah um so we’ve got all of this stuff what are some of the things that we can start doing with it well the most exciting opportunity i think is that we can now start um we can now start writing little bash scripts writing little tools on top of this and so if i do This is a script that I’ve been running for quite a while called HN summary, which is a way of summarizing posts on Hacker News or entire conversations on Hacker News. Because Hacker News gets pretty noisy. [26:47] Simon Willison: What’s a good one of these to take a look at? Yeah, I do not have time to read 119 comments, but I’d like to know a rough overview of what’s going on here. So I can say HN-summary. And then paste in that ID. And this is giving me a summary of the themes from the hack news point. So, he’s in theme static versus dynamic linking, package management dependency, Swift cross-platform language. That totally worked. And if we look at the way this worked, it’s just a bash script which does a curl command to get the full. [27:25] Simon Willison: This is from one of the hack news APIs. If you hit this URL here. You will get back JSON of everything that’s going on in that thread as this giant terrifying nested structure. I then pipe it through the JQ command. I use ChatGP to write this because I can never remember how to use JQ. That takes that and turns it into plain text. And actually, I’ll run that right now just to show you what that looks like. There we go. [28:01] Simon Willison: So that essentially strips out all of the JSON stuff and just gives me back the names of the people and what they said. And then we pipe that into LLM-M model. The model defaults to Haiku, but you can change it to other models if you like. And then we feed it this, the dash S option. to LLM, also known as dash dash system, is the way of feeding in a system prompt. [28:26] Simon Willison: So here I’m feeding the output of that curl command, goes straight into LLM as the prompt, and then the system prompt is the thing that tells it what to do. So I’m saying summarize the themes of the opinions expressed here for each theme, output a markdown header. Let’s try that. I’m going to try that one more time, but this time I’ll use GPT-4-0. So we’re running the exact same prompt, but this time through a different model. And here we’re actually getting back quotes. [28:52] Simon Willison: So when it says, you man wizard said this, Jerry Puzzle said this about dynamic and static linking. I really like this as a mechanism of sort of summarizing conversations because the moment you ask for direct prompts, you’re not completely safe from hallucination, but you do at least have a chance of fact checking what the thing said to you. And as a general rule, models are quite good at outputting texts that they’ve just seen. So if you ask for direct quotes from the input, you’ll often get bad results. But this is really good, right? [29:20] Simon Willison: This is a pretty decent, quick way of digesting 128 comments in that giant thread. And it’s all been logged to my SQLite database. I think if I go and look in SQLite, I’ve got hundreds of hack and use threads that I’ve summarized in this way, which if I wanted to do fancy things with later, that’s probably all sorts of fun I could have with them. And again, I will. Here we go. [29:49] Simon Willison: I will share full notes later on, but there’s a TIL that I wrote up with the full notes on how I built up this script. That’s another reason I love Cloud3 Haiku, is that running this kind of thing through Cloud3 Haiku is incredibly inexpensive. It costs… And it may be a couple of cents for a long conversation. And the other model I use for this is Gemini Flash that just came out from Google. It’s also a really good long context model with a very low price per token. But yeah. Where did we go to? [30:28] Hugo Bowne-Anderson: So we have a couple of questions that maybe you want to answer now, maybe we want to leave until later and maybe you cover. And the fact that we saw some sort of formatted output leads to one of these. Is LLM compatible with tools like Instructor, Kevin asks, to generate formatted output? And there are other questions around like piping different. different things together. So I wonder if you can kind of use that as a basis to talk about how you pipe, [30:56] Simon Willison: you know? Yes, I will jump into some quite complex piping in just a moment. LLM does not yet have structured output function calling support. I am so excited about getting that in there. The thing I want to do, there are two features I care about. There’s the feature where you can like get a bunch of unstructured text and feed in a JSON schema and get back JSON. That works incredibly well. A lot of the models are really good at that now. [31:19] Simon Willison: I actually have a tool I built for dataset that uses that feature, but that’s not yet available as a command line tool. And the other thing I want to do is full-blown like tool execution where the tools themselves are plugins. Like imagine if you could install an LLM plugin that added Playwright functions, and now you can run prompts that can execute Playwright automations as part of those prompts, because it’s one of the functions that gets made available to the model. [31:46] Simon Willison: So that, I’m still gelling through exactly how that’s going to work, but I think that’s going to be enormously powerful. [31:53] Hugo Bowne-Anderson: Amazing. And on that point as well, there are some questions around evals. And if you, when using something like this, you can do evals and how that would work. [32:05] Simon Willison: So work in progress. Two months ago, I started hacking on a plugin for LLM for running evals. And it is. Very, very alpha right now. The idea is I want to be able to find my evals as YAML files and then say things like LMEVAL simple dot YML with the 40 model and the chat GPT models. I’m running the same eval against two different models and then get those results back, log them to SQLite, all of that kind of thing. This is a very, very early prototype at the moment. [32:37] Simon Willison: But it’s partly, I just, I’ve got really frustrated with how difficult it is to run evals and how little I understand about them. And when I don’t understand something, I tend to write code as my way of thinking through a problem. So I will not promise that this will turn into a generally useful thing for other people. I hope it will. At the moment, it’s a sort of R&D prototype for me to experiment with some ideas. [32:58] Hugo Bowne-Anderson: I also know the community has generated a bunch of plugins and that type of stuff for Dataset. I’m not certain about LLM, but I am wondering if people here are pretty, you know, pretty sophisticated audience here. So if people wanted to contribute or that type of thing. [33:13] Simon Willison: OK, the number one way to contribute to LLM right now is is by writing plugins for it. And I wrote a very detailed tutorial. The most exciting is the ones that enable new models. So I wrote a very detailed tutorial on exactly. how to write a plugin that exposes new models. A bunch of people have written plugins for API-based models. Those are quite easy. The local models are a little bit harder, but the documentation is here. [33:39] Simon Willison: And I mean, my dream is that someday this tool is widely enough to use that when somebody releases a new model, they build a plugin for that model themselves. That would be the ideal. But in the absence of that, it’s pretty straightforward building new models. I’m halfway through building a plugin for… the MLX Apple framework, which is getting really interesting right now. And I just this morning got to a point where I have a prototype of a plugin that can run MLX models locally, which is great. But yeah, let’s do some commands. [34:17] Simon Willison: Okay, I’ll show you a really cute thing you can do first. LLM has support for templates. So you can say things like LLM dash dash system, you are a sentient cheesecake, tell it the model, and you can save that as a template called cheesecake. Now I can say LLM chat dash T cheesecake, tell me about yourself. And it’ll say, I’m a sentient cheesecake, a delightful fusion of creamy textures. So this is, I have to admit, I built this feature. I haven’t used it as much as I expected it I would. [34:47] Simon Willison: It’s effectively LLM’s equivalent of GPT’s, of chat GPT’s. I actually got this working before GPT’s came along. And it’s kind of fun, but it’s, yeah, like I said, I don’t use it on a daily basis. And I thought I would when I built it. Let’s do some really fun stuff with piping. So I’ve got a couple of the, one of the most powerful features of LLM is that you can pipe things into it with a system prompt to have it, to then process those things further. [35:17] Simon Willison: And so you can do that by just like catting files to it. So I can say cat demos.md pipe LLM dash S summary short. And this will give me a short summary of that document that I just piped into it, which works really well. That’s really nice. Cool. A little bit longer than I wanted it to be. Of course, the joy of this is that once this is done, I can then say lm-c, no, much, much, much shorter and in haikus. [35:50] Simon Willison: And now it will write me some haikus that represent the demo that I’m giving you right now. These are sentient cheesecake, templates to save brilliant minds, cheesecake chats with us. That’s lovely. So being able to pipe things in is really powerful. I built another command called files to prompt, where the idea of this one is if you’ve got a project with multiple files in it, running files to prompt will turn those into a single prompt. [36:14] Simon Willison: And the way it does that is it outputs the name of the file and then the contents of the file and then name of the next file, contents of the file, et cetera, et cetera, et cetera. But because of this, I can now do things like suggest tests to add to this project. Oh. I’m sorry. I’m sorry. I forgot the LLM-S. Here we go. And this is now, here we go, reading all of that code and suggesting, okay, you should have tests that, oh, wow, it actually, it’s writing me sample tests. [36:46] Simon Willison: This is very, this is a very nice result. So I use this all the time. When I’m hacking on stuff on my machine, I will very frequently just. cat a whole directory of files into a prompt in one go, and use the system prompt to say, what tests should I add? Or write me some tests, or explain what this is doing, or figure out this bug. Very, very powerful way of working with the tool. But way more fun than that is another tool I built called ShotScraper. [37:14] Simon Willison: So ShotScraper is a browser automation tool which started out as a way of taking screenshots. So once you’ve got it installed, you can do ShotScraper and then the URL to a web page, and it will generate a PNG file with a screenshot of that web page. That’s great. I use that to automate screenshots in my documentation using this. But then I realized that you can do really fun things by running JavaScript from the command line. [37:38] Simon Willison: So a very simple example, if I say, shot scraper JavaScript, give it the URL to a website and then give it that string, document.title, it will load that website up in a hidden browser. It’ll execute that piece of JavaScript and it will return the result of that JavaScript directly to my terminal. So I’ve now got the title of this webpage and that’s kind of fun. Where that gets super, super fun is when you start doing much more complicated and interesting things with it. So let’s scrape Google. Google hate being scraped. We’ll do it anyway. [38:12] Simon Willison: Here is a Google search for NY Times plus slop. There’s an article in the New York Times today with a quote for me about the concept of slop in AI, which I’m quite happy about. And then so you can open that up and start looking in the. If you start looking at the HTML, you’ll see that there’s H3s for each results, and the H3s are wrapped by a link that links to that page. [38:37] Simon Willison: So what I can do is I can write a little bit of JavaScript here that finds all of the H3s on the page, and for each H3, it finds the parent link and its href, and it finds the title, and it outputs those in an array. And if I do this… This should fire up that browser. That just gave me a JSON array of links to search results on the New York Times. Now I could pipe that to LLM. So I’m gonna do pipe LLM. [39:06] Simon Willison: Actually, no, I’m gonna do a slightly more sophisticated version of this. This one goes a little bit further. It tries to get the entire… It tries to get the snippet as well, because the snippet gives you that little bit of extra context. So if I take that, and I’m just going to say dash S describe slot. And what we have just done. is we have done RAG, right? This is retrieval augmented generation against Google search results using their snippets to answer a question done as a bash one-liner effectively. [39:42] Simon Willison: Like we’re using ShotScraper to load up that web page. We’re scraping some stuff out with JavaScript. We’re piping the results into LLM, which in this case is sending it up to GPT-4.0, but I could equally tell it to send it to Claude or to run it against a local model or any of those things. And it’s a full RAG pipeline. I think that’s really fun. I do a lot of my experiments around the concept of RAG, just as these little shell scripts here. [40:09] Simon Willison: You could consider the hack and use example earlier was almost an example of RAG, but this one, because we’ve got an actual search term and a question that we’re answering, I feel like this is it. This is a very quick way to start prototyping different forms of retrieval augmented generation. [40:27] Hugo Bowne-Anderson: Let me ask though, does it use? I may have missed, does it use embeddings? [40:32] Simon Willison: Not yet, no, but I’ll get into embeddings in just a second. [40:35] Hugo Bowne-Anderson: And that’s something we decided not to talk too much about today, but it’d be sad if people didn’t find out about your embeddings. [40:42] Simon Willison: I have a closing demo I can do with embeddings. Yeah, this right here, effectively, we’re just copying and pasting these search results from the browser into the model and answering a question, but we’re doing it entirely on the command line, which means that we can hook up our own bash scripts that… automate that and pull that all together. There’s all sorts of fun bits and pieces we can do with that. But yeah, let’s… The ShotScraper JavaScript thing I’ll share later. Let’s jump into the last… Let’s jump into embedding stuff. So… [41:16] Simon Willison: If you run llm dash help, it’ll show you all of the commands that are available in the LLM family. The default command is prompt. That’s for running prompts. There are also these collections for dealing with embeddings. I would hope everyone in this course is familiar enough with embeddings now that I don’t need to dig into them in too much detail. But it’s exactly the same pattern as the language models. Embeddings are provided by plugins. There are API-based embeddings. There are local embedding models. It all works exactly the same way. [41:46] Simon Willison: So if I type LLM embed models, that will show me the models that I have installed right now. And actually, these are the open AI ones, the three small, three large, and so on. If I were to install additional plugins, is the embeddings documentation. There’s a section in the plugin directory for embedding models. So you can install sentence transformers, you can get clip running, and various other bits and pieces like that. But let’s embed something. So if I say lm embed, let’s use the OpenAI 3 small model and give it some text. [42:28] Simon Willison: It will embed that text and it will return an array of, I think, 6,000. How many is that? Like JQ length. An array of 1,536 floating point numbers. This is admittedly not very interesting or useful. There’s not a lot that we can do with that JSON array of floating point numbers right here. You can get it back in different shapes and things. You can ask for it in, I think I can say, dash, dash, X. I can say dash f hex and get back a hexadecimal blob of those. Again, not particularly useful. [43:04] Simon Willison: Where embeddings get interesting is when you calculate embeddings across a larger amount of text and then start storing them for comparison. And so we can do that in a whole bunch of different ways. There is a command called, where is it? Embed multi. Where’s my embed multi documentation gone? Here we go. The embed multi command lets you embed multiple strings in one go, and it lets you store the results of those embeddings in a SQLite database because I use SQLite databases for everything. [43:40] Simon Willison: So when I have here a SQLite database, I’m going to open it up actually using Dataset Desktop, which is my Mac OS Electron app version of Dataset. How big is that file? That’s a 129 megabyte file. Wow. Does this have embeddings in already? It does not. Okay, so this right here is a database of all of the content on my blog. And one of the things I have on my blog is I have a link blog, this thing down the side, which has 7,000 links in it. [44:16] Simon Willison: And each of those links is a title and a description and a URL, effectively. So I’ve got those here in a SQLite database, and I’m going to create embeddings for every single one of those 7,168 bookmarks. And the way I can do that is with a, well, firstly, I need to figure out a SQL query that will get me back the data I want to embed. That’s going to be select ID, link URL, link title, commentary from blog, blogmark. [44:44] Simon Willison: The way LLM works is when you give it a query like this, it treats the ID there as the unique identifier for that document, and then everything else gets piped into the embedding model. So once I’ve got that in place, I can run this command. I can say, LLM embed multi. I’m going to create a collection of embeddings called links. I’m going to do it against that Simon Wilson blog SQLite database. I’m going to run this SQL query, and I’m using that three small model. [45:11] Simon Willison: And then dash dash store causes it to store the text in the SQLite database as well. Without that, it’ll just store the IDs. So I’ve set that running, and it’s doing its thing. It’s got 7,000 items. Each of those has to be sent to the OpenAI API in this case, or if it was a local model, it would run it locally. And while that’s running, we can actually see what it’s doing by taking a look at the embeddings table. Here we go. [45:38] Simon Willison: So this table right here is being populated with the being populated by that script. We’re at one thousand two hundred rows now. I hit refresh. We’re at two thousand rows. And you can see for each one, we’ve got the content which was glued together. And then we’ve got the embedding itself, which is a big binary blob of it’s a. binary encoded version of that array of 1,500 floating point numbers. But now that we’ve got those stored, we can start doing fun things with them. I’m going to open up another. There we go. [46:20] Simon Willison: So I’ve opened up another window here so that I can say LLM similar. I’m going to look for similar items in the links collection to the text, things that make me angry. Oh, why doesn’t the, oh, because I’ve got to add the dash D. Here we go. So this right here is taking the phrase, things that make me angry, it’s embedding it, and it’s finding the most similar items in my database to that. And there’s an absolutely storming rant from somebody. There’s death threats against bloggers. There’s a bunch of things that might make me angry. [46:56] Simon Willison: This is the classic sort of embedding semantic search right here. And this is kind of cool. I now have embedding search against my blog. Let’s try something a little bit more useful. I’m going to say. Let’s do dataset plugins. So we’ll get back everything that looks like it’s a dataset plugin. There we go. And I can now pipe that into LLM itself. So I can pipe it to LLM and I’m gonna say system prompt, most interesting plugins. And here we are. Again, this is effectively another version of command line rag. [47:39] Simon Willison: I have got an embeddings database in this case. I can search it for things that are similar to things. I like this example because we’re running LLM twice. We’re doing the LLM similar command to get things out of that vector database. And then we’re biking to the LLM prompt command to summarize that data and turn it into something interesting. And so you can build a full rag-based system again as a… little command line script. I think I’ve got one of those. Log answer. Yes, there we go. [48:13] Simon Willison: This one I don’t think is working at the moment, but this is an example of what it would take. What it would take to… take a question, run a embedding search against, in this case, it’s every paragraph in my blog. I’ve got a little bit of JQ to clean that up. And then I’m piping it into… In this case, the local Lama file, but I’ve typed into other models as well to answer questions. [48:38] Simon Willison: So you can build a full RAG Q&A workflow as a bash script that runs against this local SQLite database and does everything that way. It’s worth noting that this is not a fancy vector database at all. This is a SQLite database with those embedding vectors as binary blobs. Anytime you run a search against this, it’s doing effectively it’s a… Effectively, it’s doing a brute force. It’s calculating the vector similarity difference between your input and every one of those things, and then it’s sorting by those records. [49:13] Simon Willison: I find for less than 100,000 records, it’s so fast it doesn’t matter. If you were using millions and millions of records, that’s the point where the brute force approach doesn’t work anymore, and you’ll want to use some kind of specialized vector index. There are SQLite vector indexing tools that I haven’t integrated with yet, but they’re looking really promising. You can use pinecone and things like that as well. One of my future goals for LLM is to teach it how to work with external vector indexes. [49:40] Simon Willison: Cause I feel like once you’ve got those embeddings stored, having a command that like synchronizes up your pinecone to run searches, that feels like that would be a reasonable thing to do. I realize we’re running a little bit short on time. So I’m gonna switch to questions for the rest of the section. I think I went through all of the demos that I wanted to provide. [49:59] Hugo Bowne-Anderson: Awesome. Well, thank you so much, Simon. That was illuminating as always. And there are a lot more things I want to try now. And I hope for those who have played around with LLM and these client utilities that I’ve got a lot more ideas about how to do so. And for those who haven’t, please jump in and let us know on Discord or whatever, like what type of stuff you, what type of fun you get to have. [50:20] Hugo Bowne-Anderson: Question wise, I haven’t been able to rank all of them some reason with respect to upvotes, unfortunately, this time. There was one, and I don’t know if you mentioned this, there was one quickly about Hugging Face hub models. Are there plugins? [50:37] Simon Willison: No, there is not. And that’s because I am GPU poor. I’m running on a Mac. Most of the Hugging Face models appear to need an NVIDIA GPU. If you have an NVIDIA GPU and want to write the LLM Hugging Face plugin, I think it would be quite a straightforward plugin to write, and it would be enormously useful. So yeah, that’s open right now for somebody to do that. Same thing with, is it VLLX or something? [51:05] Simon Willison: There’s a few different serving technologies that I haven’t dug into because I don’t have an NVIDIA GPU to play with on a daily basis. But yeah, the Hugging Face Models thing would be fantastic. [51:15] Hugo Bowne-Anderson: Awesome. And how about some of the serverless inference stuff? Is there a way we can use LLM to ping those in? [51:23] Simon Willison: Do you mean like the Cloudflare ones and so on? [51:27] Hugo Bowne-Anderson: I’m thinking like, let me… If you go to any given model, there is some sort of serverless inference you can… [51:36] Simon Willison: you can just do to ping the apis that they’ve already got set up there oh interesting i mean so as you can see we’ve got like 20 different plugins for any scale endpoints is a very good one fireworks um open router so if it’s available via an api you can build a plugin for it the other thing is if it’s an open air compatible if it’s open ai compatible as the api you don’t have to build anything at all you can actually configure llm you can teach it about additional Yeah, you can teach about additional [52:08] Simon Willison: OpenAI compatible models by just dropping some lines into a YAML file. So if it already speaks OpenAI, without writing additional plugins, you can still talk to it. [52:20] Hugo Bowne-Anderson: Amazing. And just check out the link I shared with you. If you want to open that one, it should be in the chat. [52:27] Simon Willison: Is it the… [52:29] Hugo Bowne-Anderson: It’s the hugging face. API. [52:34] Simon Willison: No, I’ve not built something against this yet. [52:38] Hugo Bowne-Anderson: This could actually be really exciting. Yeah. Because I’ve got a lot of pretty heavy-duty models that you can just ping as part of their serverless. [52:49] Simon Willison: I don’t think anyone’s built that yet, but that would be a really good one to get going, absolutely. [52:55] Hugo Bowne-Anderson: So if anyone’s interested in that, definitely jump in there. We do have questions around using your client utility for agentic workflows. [53:07] Simon Willison: Yeah, not yet, because I haven’t done the function calling piece. Once the function calling piece is in, I think that’s going to get really interesting. And that’s also the kind of thing where I’d like to, I feel like you could explore that really by writing additional plugins, like an LLM agents plugin or something. So yeah, there is the other side of LLM which isn’t as mature is there is a Python API. So you can pip install LLM and use it from Python code. [53:35] Simon Willison: I’m not I’m not completely happy with the interface of this yet, so I don’t tend to push people towards it. Once I’ve got that stable, once I have a 1.0 release, I think this will be a very nice sort of abstraction layer over a hundred different models, because any model that’s available through a plugin to the command line tool will be available as a plugin that you can use from Python directly. So that’s going to get super fun as well, especially in Jupyter notebooks and such like. [53:58] Hugo Bowne-Anderson: Awesome. We actually have some questions around hardware. Would you mind sharing the system info of your Mac? Is it powerful? Is it with all the commands you demoed? Wonder if I need a new Mac? I can tell people that I’ve got an M1 from 2021. So my MacBook’s got a GPU, but it’s like three, four years old or whatever. And it runs this stuff wonderfully. [54:20] Simon Willison: So yeah, I’m an M2 Mac, 64 gig. I wish I’d got more RAM. At the same time, the local models, the Mistral 7Bs and such like, run flawlessly. PHY3, absolutely fantastic model, that runs flawlessly. They don’t even gobble up that much RAM as well. The largest model I’ve run so far is Lama 370B, which takes about 40 gigabytes of RAM. And it’s definitely the most GPT 3.5-like local model that I’ve ever run. [54:53] Simon Willison: I have a hunch that within a few months the Mac will be an incredible platform for running models because Apple are finally like all speed ahead on local model stuff. Their MLX library is really, really good. So it might be that in six months time, an M4 MacBook Pro with 192 gigabytes of RAM is the best machine out there. But I wouldn’t spend any money now based on future potential. [55:18] Hugo Bowne-Anderson: Right. And I’m actually very bullish on Apple and excited about what happens in the space as well. Also, we haven’t talked about this at all, but the ability to run all this cool stuff on your cell phone is people are complaining about all types of stuff at the moment and Apple hasn’t done this. But this is wild. This is absolutely like cosmic stuff. [55:40] Simon Willison: There is an app that absolutely everyone with a modern iPhone should try out called MLC Chat. Yeah. It straight up runs Mistral on the phone. It just works. And it’s worked for like six months. It’s absolutely incredible. I can run Mistral 7B Instruct Quantized on my iPhone. Yeah. And it’s good. I’ve used this on flights to look up Python commands and things. Yeah. That’s incredible. And yeah, Apple stuff. [56:09] Simon Willison: it’s interesting that none of the stuff they announced the other day was actually a chatbot you know that they’re building language model powered features that summarize and that help with copy editing and stuff they’re not giving us a chat thing which means that they don’t they’re not responsible for hallucinations and all of the other weird stuff that can happen which is a really interesting design choice i feel like apple did such a good job of avoiding most of the traps and pitfalls and weirdnesses in in the in the products that they announced yesterday [56:40] Hugo Bowne-Anderson: Totally agree. And so two more questions, we should wrap up in a second. I wonder, people have said to me, and I don’t… My answer to this, people like, hey, why do you run LLMs locally when there are so many ways to access bigger models? And one of my answers is just, like you mentioned being on a plane or in the apocalypse or that type of thing. But it’s also just for exploration to be able to do something when I’m at home to use my local stuff. [57:14] Simon Willison: The vast majority of my real work that I would do LLMs, I use Clod 3 Opus, I use GPT-4O, I occasionally use Clod 3 Haiku. But the local models as a way of exploring the space are so fascinating. And it’s also, I feel like if you want to learn how language models work, the best way to do that is to work with the really bad ones. [57:35] Simon Willison: Like the working, spending time with a crap local model that hallucinates constantly is such a good way of getting your sort of mental model of what these things are and how they work. Because when you do that, then you start saying, oh, okay, I get it. ChatGPT 3.5 is like Mistral 7b, but it’s a bit better. So it makes less mistakes and all of those kinds of things. But yeah, and I mean, there are plenty of very valid privacy concerns around this as well. I’ve kind of skipped those. [58:03] Simon Willison: Most of the stuff I say to models is me working on open source code, where if there’s a leak, it doesn’t affect me at all. But yeah, I feel like… If you’re interested in understanding the world of language models, running local models is such a great way to explore them. [58:20] Hugo Bowne-Anderson: Totally. I do have a question also around, you mentioned the eval tool that you’re slowly working on. Does it incorporate data set as well? Because I couldn’t, when, so when I want to do like at least my first round of evaluations, I’ll do it in a notebook or spin up a basic streamlet out where I can tag things as right or wrong and then filter by those. So these are the types of that could make sense in data. [58:41] Simon Willison: Where I want to go. So the idea with the evals tools, and it doesn’t do this yet. It should be recording the results to SQLite so that you can have. like a custom data interface to help you evaluate them. I want to do one of those, the LMSIS arena style interfaces where you can see two different prompted, two different responses from prompts that you’ve run evals against and click on the one that’s better and that gets recorded in the database as well. [59:05] Simon Willison: Like there’s so much that I could do with that because fundamentally SQLite is such a great sort of substrate to build tools on top of. Like it’s incredibly fast it’s free everyone’s got it you can use it as a file format for passing things around like imagine running a bunch of evals and then schlepping a like five megabytes sqlite file to your co-worker to have a look at what you’ve done that stuff all becomes possible as well But yeah, so that’s the ambition there. I don’t know when I’ll get to invest the time in it. [59:35] Hugo Bowne-Anderson: Well, once again, like if people here are interested in helping out or chatting about this stuff, please do get involved. I do. I am also interested, speaking about the SQLite database and then dataset. So one thing that’s also nice about LM Studio is that it’ll tell you, like it does have some serious product stuff. When you run something, it’ll like give you in your GUI. the latency and number of tokens and stuff like that. We log that stuff to SQLite and have that in. And then like serious, you know, benchmarking of different models. [1:00:10] Simon Willison: Yep. I’ve been meaning to file this ticket for a while. Awesome. That needs to happen. Yep. I guess it’s tokens per second and total duration. Yeah, exactly. It’s going to be interesting figuring out how to best do that for models where I don’t have a good token count from them, but I can fudge it. Just the duration on its own would be useful things to start recording. [1:00:41] Hugo Bowne-Anderson: Absolutely. And so there’s kind of a through line in some of these questions. Firstly, a lot of people are like, wow, this is amazing. Thank you. Misha has said Simon is a hero. Thank you. has said this is brilliant. I can’t believe you’ve done this. So that’s all super cool. I want to build on this question. Eyal says a little off topic, but how are you able to build so many amazing things? I just want to- [1:01:07] Simon Willison: I have a blog post about that. [1:01:09] Hugo Bowne-Anderson: Raise that as an issue on a GitHub repository? Well, [1:01:12] Simon Willison: yeah. Here we go. I have a blog- It’s not the building, [1:01:15] Hugo Bowne-Anderson: it’s the writing as well. So yeah, what structures do you put in your own life in order to- I have a great story. [1:01:24] Simon Willison: Basically, so this talk here is about my approach to personal projects and effectively, and really the argument I make here is that you need to write unit tests and documentation because then you can do more projects. Because if you haven’t done that, you’ll come across a project like my LLM evals project I haven’t touched in two months, but because I’ve got a decent set of issues and sort of notes tucked away in there, I’m going to be able to pick up on that really easily. [1:01:48] Simon Willison: And then the other trick is I only work on projects that I already know I can do quickly. I don’t have time to take on a six-month mega project, but when I look at things like LLM, I already had the expertise of working with SQLite from the dataset stuff. I knew how to write Python command line applications. I knew how to build plugin infrastructures because I’d done that for dataset. [1:02:09] Simon Willison: So I was probably the person on earth most well-equipped to build a command line tool in Python that has plugins and does language model stuff and logs to SQLite. And so really that’s my sort of main trick is I’ve got a bunch of things that I know how to do, and I’m really good at spotting opportunities to combine them in a way that lets me build something really cool, but quite quickly, because I’ve got so many other things going on. Amazing. That’s the trick. It’s being selective in your projects. [1:02:38] Hugo Bowne-Anderson: And also there are small things you do like your, well, it’s not small anymore, right? But your Today I Learn blog, and what a wonderful way to, you know, it doesn’t necessarily need to be novel stuff, right? But because it’s the Today I Learn, you just quickly write something you learn. [1:02:52] Simon Willison: I will tell you the trick for that is every single thing that I do, I do in GitHub issues. So if I’m working on anything at all, I will fire up a GitHub issue thread in a private repo or in a public repo, and I will write notes as I figure it out. And one of my favorite examples, this is when I wanted to serve an AWS Lambda function with a function URL from a custom subdomain, which took me 77 comments all from me to figure out because, oh my God, AWS is a nightmare. [1:03:21] Simon Willison: And in those comments, I will drop in links to things I found and screenshots of the horrifying web interfaces I have to use and all of that kind of thing. And then when I went to write up a TIL, I just copy and paste the markdown to the issue. So most of my TALs take like 10 minutes to put together because they’re basically just the sort of semi-structured notes I had already copied and pasted and cleaned up a little bit. But this is in that productivity presentation I gave. This works so well. [1:03:49] Simon Willison: It’s almost like a scientist’s notebook kind of approach where anything you’re doing, you write very comprehensive notes on what do I need to do next? What did I just try? What worked? What didn’t work? And you get them all in that sequence. And it means that I can. [1:04:03] Simon Willison: I don’t remember a single thing about AWS Lambda now, but next time I want to solve this problem, I can come back and I can read through this and it’ll sort of reboot my brain to the point that I can take out the project from where I got to. [1:04:16] Hugo Bowne-Anderson: Awesome. I know we’re over time. There are a couple of very more interesting questions. So if you’ve got a couple more minutes. [1:04:22] Simon Willison: Yes, absolutely. [1:04:23] Hugo Bowne-Anderson: There’s one around, have you thought about using textual or rich in order to make pretty output? [1:04:32] Simon Willison: I think. Does that work? Like what’s a LM logs? What’s this? LM logs pipe hyphen dash m rich. What’s the thing? Is it rich dot markdown you can do? [1:04:47] Hugo Bowne-Anderson: I think so, but… [1:04:50] Simon Willison: Maybe I do. Look at that! There we go, Rich just pretty printed my markdown. So yeah, so I haven’t added Rich as a dependency because I’m very, very protective of my dependencies. I try and keep them as minimal as possible. I should do a plugin. It would be really cool if there was a… I’d need a new plugin hook, but if there was a plugin where you could install LLM Rich and now LLM outputs things like that, that would be super fun. So yeah, I should do that. [1:05:18] Hugo Bowne-Anderson: That would be super cool. And just for full transparency, occasionally when I want to have fun playing around with the command line, I muck around with tools like CowSay. So piping LLM to CowSay has been fun as well to get cows to. Final question is. Misha has a few GPU rigs and they’re wondering if there’s any idea how to run LLM with multiple models on different machines but on the same LAN. [1:05:53] Simon Willison: I would solve that with more duct tape. I’d take advantage of existing tools that let you run the same command on multiple machines. Ansible, things like that. I think that would be the way to do that. And that’s the joy of Unix is so many of these problems, if you’ve got a little Unix command, you can wire it together with extra bash script and Ansible and Kubernetes and Lord only knows what else. I run LLM occasionally inside of GitHub Actions, and that works because it’s just a command line. [1:06:22] Simon Willison: So yeah, for that, I’d look at existing tools that let you run commands in parallel on multiple machines. [1:06:30] Hugo Bowne-Anderson: Amazing. So everyone, next step, pip install LLM or pipx install LLM and let us know on Discord how you go. I just want to thank everyone for joining once again. And thank you, Simon, for all of your expertise and wisdom. And it’s always fun to chat. [1:06:46] Simon Willison: Thanks for having me. And I will drop a marked down document with all of the links and demos and things in Discord at some point in the next six hours. So I’ll drop that into the Discord channel. [1:06:58] Hugo Bowne-Anderson: Fantastic. All right. See you all on Discord. And thanks once again, Simon."
  }
]